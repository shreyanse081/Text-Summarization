{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "#this code is for Amazon fine food review. Here we are trying to make a text summarizer.\n",
    "'''Cell 1:'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>cHelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I  had bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cell 2:'''\n",
    "\n",
    "reviews = pd.read_csv(\"D:/Documents/DataSets/amazon-fine-food-review/Reviews.csv\")[:35173]\n",
    "#reviews = pd.read_csv(\"E:/Shreyans/Data sets/amazon-fine-food-reviews/Reviews.csv\")\n",
    "\n",
    "\n",
    "'''Cell 3:'''\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                        0\n",
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               1\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Score                     0\n",
       "Time                      0\n",
       "Summary                   1\n",
       "Text                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35173, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Cell 5:'''\n",
    "\n",
    "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'], 1)\n",
    "reviews = reviews.dropna()\n",
    "demo_summary= reviews.drop(['Text'],1)\n",
    "reviews = reviews.reset_index(drop=True)\n",
    "\n",
    "'''Cell 6:'''\n",
    "reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary    0\n",
       "Text       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35172, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "count          35172\n",
      "unique         28194\n",
      "top       Delicious!\n",
      "freq             136\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "count                                                 35172\n",
      "unique                                                33043\n",
      "top       Diamond Almonds<br />Almonds are a good source...\n",
      "freq                                                     12\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(reviews.Summary.value_counts(\" \"))\n",
    "#print(reviews.Summary.value_counts())\n",
    "print()\n",
    "print(reviews.Summary.describe())\n",
    "print()\n",
    "print(reviews.Text.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Cell 7: A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python'''\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#Cell 8:'''\n",
    "def clean_text(text, remove_stopwords):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_nan = 0\n",
    "for summary in reviews.Summary:\n",
    "    if str(summary) != 'nan':\n",
    "        count_nan=count_nan+1\n",
    "\n",
    "print(count_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 9:'''\n",
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    clean_summaries.append(clean_text(str(summary), remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(str(text), remove_stopwords=True))\n",
    "print(\"Texts are complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summaries:  good quality dog food\n",
      "\n",
      "Text:  product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summaries:  not as advertised\n",
      "\n",
      "Text:  confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "Summaries:   delight  says it all\n",
      "\n",
      "Text:  looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal\n",
      "Summaries:  cough medicine\n",
      "\n",
      "Text:  great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "Summaries:  great taffy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Text: \", clean_texts[i])\n",
    "    print(\"Summaries: \",clean_summaries[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 37145\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 10:'''\n",
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1\n",
    "                \n",
    "'''#Cell 11: Find the number of times each word was used and the size of the vocabulary'''\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417195\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 12:'''\n",
    "# Commented out because it won't load on Kaggle, so I'll replace it an empty list\n",
    "embeddings_index = []\n",
    "\n",
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "#D:/Documents/DataSets for laptop\n",
    "#E:/Shreyans/Data sets/numberbatch-en.txt\n",
    "with open('D:/Documents/DataSets/numberbatch-en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 312\n",
      "Percent of words that are missing from vocabulary: 0.84%\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 13: Find the number of words that are missing from CN, and are used more than our threshold.'''\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 37145\n",
      "Number of words we will use: 25391\n",
      "Percent of words we will use: 68.36%\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 14: Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe'''\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25391\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 15:'''\n",
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        #embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#Cell 16:'''\n",
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 1563100\n",
      "Total number of UNKs in headlines: 23699\n",
      "Percent of words that are UNK: 1.52%\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 17:'''\n",
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#Cell 18:'''\n",
    "\n",
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  35172.000000\n",
      "mean       4.177101\n",
      "std        2.648076\n",
      "min        0.000000\n",
      "25%        2.000000\n",
      "50%        4.000000\n",
      "75%        5.000000\n",
      "max       30.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  35172.000000\n",
      "mean      41.264500\n",
      "std       39.651104\n",
      "min        1.000000\n",
      "25%       18.000000\n",
      "50%       29.000000\n",
      "75%       50.000000\n",
      "max      783.000000\n"
     ]
    }
   ],
   "source": [
    "'''#Cell 19:'''\n",
    "\n",
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Cell 20:'''\n",
    "\n",
    "#Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Cell 21:'''\n",
    "#Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 22:'''\n",
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31084\n",
      "31084\n"
     ]
    }
   ],
   "source": [
    "'''Cell 23:'''\n",
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 199 #84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 100 # use 1\n",
    "unk_summary_limit = 100 # use 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 24:'''\n",
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 25:'''\n",
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 26:'''\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Cell 27:'''\n",
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 28:'''\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Applying Attention</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 29:'''\n",
    "#output_units=12\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    #Attention Mechanism\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],\n",
    "                                                                    _zero_state_tensors(rnn_size, \n",
    "                                                                                        batch_size, \n",
    "                                                                                        tf.float32)) \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 30:'''\n",
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 31:'''\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 32:'''\n",
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Cell 33:'''\n",
    "# Set the Hyperparameters\n",
    "epochs = 50 # use 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 3\n",
    "learning_rate = 0.008\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "'''Cell 34:'''\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> This module will train the above graph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 5 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "#uncomment the above line for new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training will Strat now.\n",
      "Epoch   1/50 Batch    5/485 - Loss:  7.340, Seconds: 6.06\n",
      "Epoch   1/50 Batch   10/485 - Loss:  3.060, Seconds: 7.13\n",
      "Epoch   1/50 Batch   15/485 - Loss:  2.882, Seconds: 5.80\n",
      "Epoch   1/50 Batch   20/485 - Loss:  2.630, Seconds: 7.45\n",
      "Epoch   1/50 Batch   25/485 - Loss:  2.879, Seconds: 5.24\n",
      "Epoch   1/50 Batch   30/485 - Loss:  2.645, Seconds: 7.55\n",
      "Epoch   1/50 Batch   35/485 - Loss:  2.709, Seconds: 6.89\n",
      "Epoch   1/50 Batch   40/485 - Loss:  2.786, Seconds: 5.47\n",
      "Epoch   1/50 Batch   45/485 - Loss:  2.720, Seconds: 6.99\n",
      "Epoch   1/50 Batch   50/485 - Loss:  2.644, Seconds: 7.91\n",
      "Epoch   1/50 Batch   55/485 - Loss:  2.672, Seconds: 4.94\n",
      "Epoch   1/50 Batch   60/485 - Loss:  2.701, Seconds: 5.02\n",
      "Epoch   1/50 Batch   65/485 - Loss:  2.484, Seconds: 6.07\n",
      "Epoch   1/50 Batch   70/485 - Loss:  2.375, Seconds: 6.73\n",
      "Epoch   1/50 Batch   75/485 - Loss:  2.732, Seconds: 7.29\n",
      "Epoch   1/50 Batch   80/485 - Loss:  2.705, Seconds: 7.36\n",
      "Epoch   1/50 Batch   85/485 - Loss:  2.596, Seconds: 6.92\n",
      "Epoch   1/50 Batch   90/485 - Loss:  2.608, Seconds: 6.33\n",
      "Epoch   1/50 Batch   95/485 - Loss:  2.580, Seconds: 6.93\n",
      "Epoch   1/50 Batch  100/485 - Loss:  2.289, Seconds: 8.82\n",
      "Epoch   1/50 Batch  105/485 - Loss:  2.441, Seconds: 8.98\n",
      "Epoch   1/50 Batch  110/485 - Loss:  2.516, Seconds: 7.66\n",
      "Epoch   1/50 Batch  115/485 - Loss:  2.600, Seconds: 8.21\n",
      "Epoch   1/50 Batch  120/485 - Loss:  2.460, Seconds: 7.56\n",
      "Epoch   1/50 Batch  125/485 - Loss:  2.599, Seconds: 7.20\n",
      "Epoch   1/50 Batch  130/485 - Loss:  2.408, Seconds: 6.66\n",
      "Epoch   1/50 Batch  135/485 - Loss:  2.407, Seconds: 8.36\n",
      "Epoch   1/50 Batch  140/485 - Loss:  2.649, Seconds: 7.31\n",
      "Epoch   1/50 Batch  145/485 - Loss:  2.369, Seconds: 7.85\n",
      "Epoch   1/50 Batch  150/485 - Loss:  2.581, Seconds: 7.26\n",
      "Epoch   1/50 Batch  155/485 - Loss:  2.476, Seconds: 7.98\n",
      "Epoch   1/50 Batch  160/485 - Loss:  2.254, Seconds: 6.81\n",
      "Average loss for this update: 2.744\n",
      "New Record!\n",
      "Epoch   1/50 Batch  165/485 - Loss:  2.519, Seconds: 7.73\n",
      "Epoch   1/50 Batch  170/485 - Loss:  2.494, Seconds: 8.12\n",
      "Epoch   1/50 Batch  175/485 - Loss:  2.228, Seconds: 8.93\n",
      "Epoch   1/50 Batch  180/485 - Loss:  2.403, Seconds: 8.91\n",
      "Epoch   1/50 Batch  185/485 - Loss:  2.523, Seconds: 7.80\n",
      "Epoch   1/50 Batch  190/485 - Loss:  2.475, Seconds: 9.32\n",
      "Epoch   1/50 Batch  195/485 - Loss:  2.467, Seconds: 7.83\n",
      "Epoch   1/50 Batch  200/485 - Loss:  2.631, Seconds: 7.88\n",
      "Epoch   1/50 Batch  205/485 - Loss:  2.516, Seconds: 7.15\n",
      "Epoch   1/50 Batch  210/485 - Loss:  2.567, Seconds: 9.42\n",
      "Epoch   1/50 Batch  215/485 - Loss:  2.477, Seconds: 8.29\n",
      "Epoch   1/50 Batch  220/485 - Loss:  2.411, Seconds: 9.04\n",
      "Epoch   1/50 Batch  225/485 - Loss:  2.572, Seconds: 7.32\n",
      "Epoch   1/50 Batch  230/485 - Loss:  2.463, Seconds: 9.23\n",
      "Epoch   1/50 Batch  235/485 - Loss:  2.489, Seconds: 8.97\n",
      "Epoch   1/50 Batch  240/485 - Loss:  2.793, Seconds: 6.73\n",
      "Epoch   1/50 Batch  245/485 - Loss:  2.336, Seconds: 8.59\n",
      "Epoch   1/50 Batch  250/485 - Loss:  2.347, Seconds: 10.04\n",
      "Epoch   1/50 Batch  255/485 - Loss:  2.497, Seconds: 9.92\n",
      "Epoch   1/50 Batch  260/485 - Loss:  2.698, Seconds: 9.26\n",
      "Epoch   1/50 Batch  265/485 - Loss:  2.547, Seconds: 7.58\n",
      "Epoch   1/50 Batch  270/485 - Loss:  2.555, Seconds: 8.71\n",
      "Epoch   1/50 Batch  275/485 - Loss:  2.385, Seconds: 9.43\n",
      "Epoch   1/50 Batch  280/485 - Loss:  2.424, Seconds: 9.45\n",
      "Epoch   1/50 Batch  285/485 - Loss:  2.595, Seconds: 8.89\n",
      "Epoch   1/50 Batch  290/485 - Loss:  2.356, Seconds: 9.54\n",
      "Epoch   1/50 Batch  295/485 - Loss:  2.238, Seconds: 10.31\n",
      "Epoch   1/50 Batch  300/485 - Loss:  2.267, Seconds: 9.74\n",
      "Epoch   1/50 Batch  305/485 - Loss:  2.513, Seconds: 9.91\n",
      "Epoch   1/50 Batch  310/485 - Loss:  2.529, Seconds: 8.57\n",
      "Epoch   1/50 Batch  315/485 - Loss:  2.566, Seconds: 9.37\n",
      "Epoch   1/50 Batch  320/485 - Loss:  2.595, Seconds: 9.99\n",
      "Average loss for this update: 2.484\n",
      "New Record!\n",
      "Epoch   1/50 Batch  325/485 - Loss:  2.737, Seconds: 8.40\n",
      "Epoch   1/50 Batch  330/485 - Loss:  2.324, Seconds: 10.97\n",
      "Epoch   1/50 Batch  335/485 - Loss:  2.596, Seconds: 10.41\n",
      "Epoch   1/50 Batch  340/485 - Loss:  2.575, Seconds: 9.84\n",
      "Epoch   1/50 Batch  345/485 - Loss:  2.691, Seconds: 9.76\n",
      "Epoch   1/50 Batch  350/485 - Loss:  2.671, Seconds: 9.88\n",
      "Epoch   1/50 Batch  355/485 - Loss:  2.587, Seconds: 10.61\n",
      "Epoch   1/50 Batch  360/485 - Loss:  2.442, Seconds: 11.79\n",
      "Epoch   1/50 Batch  365/485 - Loss:  2.600, Seconds: 11.46\n",
      "Epoch   1/50 Batch  370/485 - Loss:  2.769, Seconds: 10.95\n",
      "Epoch   1/50 Batch  375/485 - Loss:  2.750, Seconds: 10.41\n",
      "Epoch   1/50 Batch  380/485 - Loss:  2.583, Seconds: 11.12\n",
      "Epoch   1/50 Batch  385/485 - Loss:  2.514, Seconds: 10.82\n",
      "Epoch   1/50 Batch  390/485 - Loss:  2.393, Seconds: 11.53\n",
      "Epoch   1/50 Batch  395/485 - Loss:  2.686, Seconds: 10.93\n",
      "Epoch   1/50 Batch  400/485 - Loss:  2.706, Seconds: 11.82\n",
      "Epoch   1/50 Batch  405/485 - Loss:  2.597, Seconds: 11.96\n",
      "Epoch   1/50 Batch  410/485 - Loss:  2.502, Seconds: 12.10\n",
      "Epoch   1/50 Batch  415/485 - Loss:  2.759, Seconds: 13.13\n",
      "Epoch   1/50 Batch  420/485 - Loss:  2.780, Seconds: 13.27\n",
      "Epoch   1/50 Batch  425/485 - Loss:  2.698, Seconds: 12.03\n",
      "Epoch   1/50 Batch  430/485 - Loss:  2.660, Seconds: 13.13\n",
      "Epoch   1/50 Batch  435/485 - Loss:  2.828, Seconds: 13.27\n",
      "Epoch   1/50 Batch  440/485 - Loss:  2.865, Seconds: 13.66\n",
      "Epoch   1/50 Batch  445/485 - Loss:  2.810, Seconds: 13.92\n",
      "Epoch   1/50 Batch  450/485 - Loss:  2.786, Seconds: 13.37\n",
      "Epoch   1/50 Batch  455/485 - Loss:  2.663, Seconds: 14.74\n",
      "Epoch   1/50 Batch  460/485 - Loss:  3.059, Seconds: 14.77\n",
      "Epoch   1/50 Batch  465/485 - Loss:  2.875, Seconds: 16.91\n",
      "Epoch   1/50 Batch  470/485 - Loss:  3.011, Seconds: 18.04\n",
      "Epoch   1/50 Batch  475/485 - Loss:  2.967, Seconds: 19.35\n",
      "Epoch   1/50 Batch  480/485 - Loss:  3.067, Seconds: 19.33\n",
      "Average loss for this update: 2.705\n",
      "No Improvement.\n",
      "Epoch   2/50 Batch    5/485 - Loss:  2.536, Seconds: 6.26\n",
      "Epoch   2/50 Batch   10/485 - Loss:  1.970, Seconds: 6.87\n",
      "Epoch   2/50 Batch   15/485 - Loss:  1.933, Seconds: 5.71\n",
      "Epoch   2/50 Batch   20/485 - Loss:  1.839, Seconds: 7.52\n",
      "Epoch   2/50 Batch   25/485 - Loss:  2.058, Seconds: 5.23\n",
      "Epoch   2/50 Batch   30/485 - Loss:  1.913, Seconds: 7.40\n",
      "Epoch   2/50 Batch   35/485 - Loss:  2.029, Seconds: 6.90\n",
      "Epoch   2/50 Batch   40/485 - Loss:  2.072, Seconds: 5.28\n",
      "Epoch   2/50 Batch   45/485 - Loss:  2.002, Seconds: 6.99\n",
      "Epoch   2/50 Batch   50/485 - Loss:  2.004, Seconds: 7.51\n",
      "Epoch   2/50 Batch   55/485 - Loss:  2.012, Seconds: 4.82\n",
      "Epoch   2/50 Batch   60/485 - Loss:  2.032, Seconds: 4.81\n",
      "Epoch   2/50 Batch   65/485 - Loss:  1.949, Seconds: 5.93\n",
      "Epoch   2/50 Batch   70/485 - Loss:  1.831, Seconds: 6.53\n",
      "Epoch   2/50 Batch   75/485 - Loss:  2.099, Seconds: 7.03\n",
      "Epoch   2/50 Batch   80/485 - Loss:  2.044, Seconds: 7.09\n",
      "Epoch   2/50 Batch   85/485 - Loss:  2.029, Seconds: 6.90\n",
      "Epoch   2/50 Batch   90/485 - Loss:  2.010, Seconds: 6.26\n",
      "Epoch   2/50 Batch   95/485 - Loss:  1.964, Seconds: 6.81\n",
      "Epoch   2/50 Batch  100/485 - Loss:  1.801, Seconds: 8.60\n",
      "Epoch   2/50 Batch  105/485 - Loss:  1.908, Seconds: 8.79\n",
      "Epoch   2/50 Batch  110/485 - Loss:  1.927, Seconds: 7.55\n",
      "Epoch   2/50 Batch  115/485 - Loss:  2.075, Seconds: 8.13\n",
      "Epoch   2/50 Batch  120/485 - Loss:  1.897, Seconds: 7.59\n",
      "Epoch   2/50 Batch  125/485 - Loss:  2.042, Seconds: 7.01\n",
      "Epoch   2/50 Batch  130/485 - Loss:  1.909, Seconds: 6.57\n",
      "Epoch   2/50 Batch  135/485 - Loss:  1.931, Seconds: 8.15\n",
      "Epoch   2/50 Batch  140/485 - Loss:  2.122, Seconds: 7.19\n",
      "Epoch   2/50 Batch  145/485 - Loss:  1.893, Seconds: 7.70\n",
      "Epoch   2/50 Batch  150/485 - Loss:  2.042, Seconds: 7.18\n",
      "Epoch   2/50 Batch  155/485 - Loss:  1.999, Seconds: 7.75\n",
      "Epoch   2/50 Batch  160/485 - Loss:  1.816, Seconds: 6.45\n",
      "Average loss for this update: 1.99\n",
      "New Record!\n",
      "Epoch   2/50 Batch  165/485 - Loss:  2.029, Seconds: 7.44\n",
      "Epoch   2/50 Batch  170/485 - Loss:  2.027, Seconds: 7.98\n",
      "Epoch   2/50 Batch  175/485 - Loss:  1.788, Seconds: 8.55\n",
      "Epoch   2/50 Batch  180/485 - Loss:  1.929, Seconds: 8.65\n",
      "Epoch   2/50 Batch  185/485 - Loss:  2.045, Seconds: 7.48\n",
      "Epoch   2/50 Batch  190/485 - Loss:  1.999, Seconds: 9.16\n",
      "Epoch   2/50 Batch  195/485 - Loss:  1.985, Seconds: 7.43\n",
      "Epoch   2/50 Batch  200/485 - Loss:  2.127, Seconds: 7.38\n",
      "Epoch   2/50 Batch  205/485 - Loss:  2.083, Seconds: 6.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/50 Batch  210/485 - Loss:  2.086, Seconds: 9.25\n",
      "Epoch   2/50 Batch  215/485 - Loss:  2.022, Seconds: 8.16\n",
      "Epoch   2/50 Batch  220/485 - Loss:  2.002, Seconds: 8.77\n",
      "Epoch   2/50 Batch  225/485 - Loss:  2.119, Seconds: 6.98\n",
      "Epoch   2/50 Batch  230/485 - Loss:  2.017, Seconds: 8.91\n",
      "Epoch   2/50 Batch  235/485 - Loss:  2.030, Seconds: 8.94\n",
      "Epoch   2/50 Batch  240/485 - Loss:  2.266, Seconds: 6.59\n",
      "Epoch   2/50 Batch  245/485 - Loss:  1.960, Seconds: 8.49\n",
      "Epoch   2/50 Batch  250/485 - Loss:  1.963, Seconds: 9.78\n",
      "Epoch   2/50 Batch  255/485 - Loss:  2.103, Seconds: 9.86\n",
      "Epoch   2/50 Batch  260/485 - Loss:  2.245, Seconds: 9.13\n",
      "Epoch   2/50 Batch  265/485 - Loss:  2.115, Seconds: 7.49\n",
      "Epoch   2/50 Batch  270/485 - Loss:  2.160, Seconds: 8.61\n",
      "Epoch   2/50 Batch  275/485 - Loss:  2.005, Seconds: 9.39\n",
      "Epoch   2/50 Batch  280/485 - Loss:  2.015, Seconds: 9.42\n",
      "Epoch   2/50 Batch  285/485 - Loss:  2.181, Seconds: 8.81\n",
      "Epoch   2/50 Batch  290/485 - Loss:  2.010, Seconds: 9.60\n",
      "Epoch   2/50 Batch  295/485 - Loss:  1.891, Seconds: 10.41\n",
      "Epoch   2/50 Batch  300/485 - Loss:  1.906, Seconds: 9.79\n",
      "Epoch   2/50 Batch  305/485 - Loss:  2.134, Seconds: 9.72\n",
      "Epoch   2/50 Batch  310/485 - Loss:  2.154, Seconds: 8.53\n",
      "Epoch   2/50 Batch  315/485 - Loss:  2.203, Seconds: 9.24\n",
      "Epoch   2/50 Batch  320/485 - Loss:  2.206, Seconds: 10.24\n",
      "Average loss for this update: 2.056\n",
      "No Improvement.\n",
      "Epoch   2/50 Batch  325/485 - Loss:  2.346, Seconds: 8.20\n",
      "Epoch   2/50 Batch  330/485 - Loss:  1.970, Seconds: 10.80\n",
      "Epoch   2/50 Batch  335/485 - Loss:  2.215, Seconds: 10.17\n",
      "Epoch   2/50 Batch  340/485 - Loss:  2.191, Seconds: 9.81\n",
      "Epoch   2/50 Batch  345/485 - Loss:  2.278, Seconds: 9.67\n",
      "Epoch   2/50 Batch  350/485 - Loss:  2.305, Seconds: 9.79\n",
      "Epoch   2/50 Batch  355/485 - Loss:  2.204, Seconds: 10.53\n",
      "Epoch   2/50 Batch  360/485 - Loss:  2.119, Seconds: 11.46\n",
      "Epoch   2/50 Batch  365/485 - Loss:  2.209, Seconds: 11.39\n",
      "Epoch   2/50 Batch  370/485 - Loss:  2.340, Seconds: 10.87\n",
      "Epoch   2/50 Batch  375/485 - Loss:  2.370, Seconds: 10.59\n",
      "Epoch   2/50 Batch  380/485 - Loss:  2.261, Seconds: 11.20\n",
      "Epoch   2/50 Batch  385/485 - Loss:  2.199, Seconds: 10.72\n",
      "Epoch   2/50 Batch  390/485 - Loss:  2.073, Seconds: 11.37\n",
      "Epoch   2/50 Batch  395/485 - Loss:  2.341, Seconds: 10.98\n",
      "Epoch   2/50 Batch  400/485 - Loss:  2.344, Seconds: 11.82\n",
      "Epoch   2/50 Batch  405/485 - Loss:  2.247, Seconds: 11.88\n",
      "Epoch   2/50 Batch  410/485 - Loss:  2.168, Seconds: 11.99\n",
      "Epoch   2/50 Batch  415/485 - Loss:  2.386, Seconds: 13.21\n",
      "Epoch   2/50 Batch  420/485 - Loss:  2.417, Seconds: 13.25\n",
      "Epoch   2/50 Batch  425/485 - Loss:  2.336, Seconds: 12.18\n",
      "Epoch   2/50 Batch  430/485 - Loss:  2.341, Seconds: 13.24\n",
      "Epoch   2/50 Batch  435/485 - Loss:  2.450, Seconds: 13.22\n",
      "Epoch   2/50 Batch  440/485 - Loss:  2.488, Seconds: 13.74\n",
      "Epoch   2/50 Batch  445/485 - Loss:  2.460, Seconds: 13.92\n",
      "Epoch   2/50 Batch  450/485 - Loss:  2.455, Seconds: 13.56\n",
      "Epoch   2/50 Batch  455/485 - Loss:  2.332, Seconds: 14.80\n",
      "Epoch   2/50 Batch  460/485 - Loss:  2.683, Seconds: 14.62\n",
      "Epoch   2/50 Batch  465/485 - Loss:  2.558, Seconds: 16.89\n",
      "Epoch   2/50 Batch  470/485 - Loss:  2.655, Seconds: 18.29\n",
      "Epoch   2/50 Batch  475/485 - Loss:  2.628, Seconds: 19.48\n",
      "Epoch   2/50 Batch  480/485 - Loss:  2.723, Seconds: 19.45\n",
      "Average loss for this update: 2.347\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch    5/485 - Loss:  2.310, Seconds: 6.32\n",
      "Epoch   3/50 Batch   10/485 - Loss:  1.773, Seconds: 6.84\n",
      "Epoch   3/50 Batch   15/485 - Loss:  1.700, Seconds: 5.72\n",
      "Epoch   3/50 Batch   20/485 - Loss:  1.637, Seconds: 7.52\n",
      "Epoch   3/50 Batch   25/485 - Loss:  1.847, Seconds: 5.31\n",
      "Epoch   3/50 Batch   30/485 - Loss:  1.693, Seconds: 7.50\n",
      "Epoch   3/50 Batch   35/485 - Loss:  1.795, Seconds: 6.93\n",
      "Epoch   3/50 Batch   40/485 - Loss:  1.855, Seconds: 5.26\n",
      "Epoch   3/50 Batch   45/485 - Loss:  1.817, Seconds: 7.07\n",
      "Epoch   3/50 Batch   50/485 - Loss:  1.783, Seconds: 7.50\n",
      "Epoch   3/50 Batch   55/485 - Loss:  1.789, Seconds: 4.88\n",
      "Epoch   3/50 Batch   60/485 - Loss:  1.814, Seconds: 4.85\n",
      "Epoch   3/50 Batch   65/485 - Loss:  1.746, Seconds: 5.92\n",
      "Epoch   3/50 Batch   70/485 - Loss:  1.646, Seconds: 6.67\n",
      "Epoch   3/50 Batch   75/485 - Loss:  1.901, Seconds: 7.05\n",
      "Epoch   3/50 Batch   80/485 - Loss:  1.802, Seconds: 7.11\n",
      "Epoch   3/50 Batch   85/485 - Loss:  1.796, Seconds: 6.69\n",
      "Epoch   3/50 Batch   90/485 - Loss:  1.776, Seconds: 6.21\n",
      "Epoch   3/50 Batch   95/485 - Loss:  1.757, Seconds: 6.80\n",
      "Epoch   3/50 Batch  100/485 - Loss:  1.622, Seconds: 8.57\n",
      "Epoch   3/50 Batch  105/485 - Loss:  1.700, Seconds: 8.59\n",
      "Epoch   3/50 Batch  110/485 - Loss:  1.729, Seconds: 7.50\n",
      "Epoch   3/50 Batch  115/485 - Loss:  1.837, Seconds: 8.07\n",
      "Epoch   3/50 Batch  120/485 - Loss:  1.706, Seconds: 7.46\n",
      "Epoch   3/50 Batch  125/485 - Loss:  1.830, Seconds: 7.12\n",
      "Epoch   3/50 Batch  130/485 - Loss:  1.707, Seconds: 6.44\n",
      "Epoch   3/50 Batch  135/485 - Loss:  1.751, Seconds: 8.07\n",
      "Epoch   3/50 Batch  140/485 - Loss:  1.924, Seconds: 7.15\n",
      "Epoch   3/50 Batch  145/485 - Loss:  1.705, Seconds: 7.53\n",
      "Epoch   3/50 Batch  150/485 - Loss:  1.777, Seconds: 7.14\n",
      "Epoch   3/50 Batch  155/485 - Loss:  1.786, Seconds: 7.74\n",
      "Epoch   3/50 Batch  160/485 - Loss:  1.669, Seconds: 6.57\n",
      "Average loss for this update: 1.781\n",
      "New Record!\n",
      "Epoch   3/50 Batch  165/485 - Loss:  1.832, Seconds: 7.31\n",
      "Epoch   3/50 Batch  170/485 - Loss:  1.824, Seconds: 7.85\n",
      "Epoch   3/50 Batch  175/485 - Loss:  1.628, Seconds: 8.73\n",
      "Epoch   3/50 Batch  180/485 - Loss:  1.748, Seconds: 8.62\n",
      "Epoch   3/50 Batch  185/485 - Loss:  1.846, Seconds: 7.55\n",
      "Epoch   3/50 Batch  190/485 - Loss:  1.825, Seconds: 9.07\n",
      "Epoch   3/50 Batch  195/485 - Loss:  1.782, Seconds: 7.38\n",
      "Epoch   3/50 Batch  200/485 - Loss:  1.893, Seconds: 7.52\n",
      "Epoch   3/50 Batch  205/485 - Loss:  1.862, Seconds: 7.32\n",
      "Epoch   3/50 Batch  210/485 - Loss:  1.866, Seconds: 9.25\n",
      "Epoch   3/50 Batch  215/485 - Loss:  1.827, Seconds: 8.15\n",
      "Epoch   3/50 Batch  220/485 - Loss:  1.798, Seconds: 8.69\n",
      "Epoch   3/50 Batch  225/485 - Loss:  1.870, Seconds: 7.12\n",
      "Epoch   3/50 Batch  230/485 - Loss:  1.816, Seconds: 9.40\n",
      "Epoch   3/50 Batch  235/485 - Loss:  1.816, Seconds: 8.98\n",
      "Epoch   3/50 Batch  240/485 - Loss:  2.028, Seconds: 6.68\n",
      "Epoch   3/50 Batch  245/485 - Loss:  1.776, Seconds: 8.52\n",
      "Epoch   3/50 Batch  250/485 - Loss:  1.809, Seconds: 9.71\n",
      "Epoch   3/50 Batch  255/485 - Loss:  1.905, Seconds: 9.74\n",
      "Epoch   3/50 Batch  260/485 - Loss:  2.013, Seconds: 9.31\n",
      "Epoch   3/50 Batch  265/485 - Loss:  1.881, Seconds: 7.51\n",
      "Epoch   3/50 Batch  270/485 - Loss:  1.958, Seconds: 8.62\n",
      "Epoch   3/50 Batch  275/485 - Loss:  1.846, Seconds: 9.96\n",
      "Epoch   3/50 Batch  280/485 - Loss:  1.837, Seconds: 9.51\n",
      "Epoch   3/50 Batch  285/485 - Loss:  1.971, Seconds: 8.93\n",
      "Epoch   3/50 Batch  290/485 - Loss:  1.844, Seconds: 9.66\n",
      "Epoch   3/50 Batch  295/485 - Loss:  1.726, Seconds: 10.39\n",
      "Epoch   3/50 Batch  300/485 - Loss:  1.754, Seconds: 9.78\n",
      "Epoch   3/50 Batch  305/485 - Loss:  1.922, Seconds: 9.89\n",
      "Epoch   3/50 Batch  310/485 - Loss:  1.969, Seconds: 8.67\n",
      "Epoch   3/50 Batch  315/485 - Loss:  2.006, Seconds: 9.34\n",
      "Epoch   3/50 Batch  320/485 - Loss:  2.022, Seconds: 9.90\n",
      "Average loss for this update: 1.859\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  325/485 - Loss:  2.135, Seconds: 8.25\n",
      "Epoch   3/50 Batch  330/485 - Loss:  1.778, Seconds: 10.75\n",
      "Epoch   3/50 Batch  335/485 - Loss:  2.003, Seconds: 10.23\n",
      "Epoch   3/50 Batch  340/485 - Loss:  1.985, Seconds: 9.69\n",
      "Epoch   3/50 Batch  345/485 - Loss:  2.062, Seconds: 9.80\n",
      "Epoch   3/50 Batch  350/485 - Loss:  2.115, Seconds: 9.91\n",
      "Epoch   3/50 Batch  355/485 - Loss:  1.991, Seconds: 10.62\n",
      "Epoch   3/50 Batch  360/485 - Loss:  1.932, Seconds: 11.28\n",
      "Epoch   3/50 Batch  365/485 - Loss:  2.008, Seconds: 11.47\n",
      "Epoch   3/50 Batch  370/485 - Loss:  2.105, Seconds: 10.98\n",
      "Epoch   3/50 Batch  375/485 - Loss:  2.130, Seconds: 10.36\n",
      "Epoch   3/50 Batch  380/485 - Loss:  2.050, Seconds: 11.47\n",
      "Epoch   3/50 Batch  385/485 - Loss:  2.039, Seconds: 10.59\n",
      "Epoch   3/50 Batch  390/485 - Loss:  1.893, Seconds: 11.48\n",
      "Epoch   3/50 Batch  395/485 - Loss:  2.160, Seconds: 11.03\n",
      "Epoch   3/50 Batch  400/485 - Loss:  2.139, Seconds: 11.75\n",
      "Epoch   3/50 Batch  405/485 - Loss:  2.059, Seconds: 11.86\n",
      "Epoch   3/50 Batch  410/485 - Loss:  1.999, Seconds: 12.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/50 Batch  415/485 - Loss:  2.186, Seconds: 13.18\n",
      "Epoch   3/50 Batch  420/485 - Loss:  2.207, Seconds: 13.33\n",
      "Epoch   3/50 Batch  425/485 - Loss:  2.109, Seconds: 12.21\n",
      "Epoch   3/50 Batch  430/485 - Loss:  2.152, Seconds: 13.03\n",
      "Epoch   3/50 Batch  435/485 - Loss:  2.234, Seconds: 13.41\n",
      "Epoch   3/50 Batch  440/485 - Loss:  2.284, Seconds: 13.59\n",
      "Epoch   3/50 Batch  445/485 - Loss:  2.235, Seconds: 13.86\n",
      "Epoch   3/50 Batch  450/485 - Loss:  2.258, Seconds: 13.51\n",
      "Epoch   3/50 Batch  455/485 - Loss:  2.156, Seconds: 14.79\n",
      "Epoch   3/50 Batch  460/485 - Loss:  2.460, Seconds: 14.65\n",
      "Epoch   3/50 Batch  465/485 - Loss:  2.353, Seconds: 16.76\n",
      "Epoch   3/50 Batch  470/485 - Loss:  2.432, Seconds: 17.83\n",
      "Epoch   3/50 Batch  475/485 - Loss:  2.419, Seconds: 19.32\n",
      "Epoch   3/50 Batch  480/485 - Loss:  2.494, Seconds: 19.81\n",
      "Average loss for this update: 2.143\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch    5/485 - Loss:  2.109, Seconds: 6.30\n",
      "Epoch   4/50 Batch   10/485 - Loss:  1.634, Seconds: 6.90\n",
      "Epoch   4/50 Batch   15/485 - Loss:  1.601, Seconds: 5.69\n",
      "Epoch   4/50 Batch   20/485 - Loss:  1.523, Seconds: 7.40\n",
      "Epoch   4/50 Batch   25/485 - Loss:  1.674, Seconds: 5.17\n",
      "Epoch   4/50 Batch   30/485 - Loss:  1.560, Seconds: 7.57\n",
      "Epoch   4/50 Batch   35/485 - Loss:  1.673, Seconds: 6.94\n",
      "Epoch   4/50 Batch   40/485 - Loss:  1.724, Seconds: 5.33\n",
      "Epoch   4/50 Batch   45/485 - Loss:  1.668, Seconds: 6.98\n",
      "Epoch   4/50 Batch   50/485 - Loss:  1.666, Seconds: 7.59\n",
      "Epoch   4/50 Batch   55/485 - Loss:  1.662, Seconds: 4.86\n",
      "Epoch   4/50 Batch   60/485 - Loss:  1.673, Seconds: 4.85\n",
      "Epoch   4/50 Batch   65/485 - Loss:  1.603, Seconds: 5.94\n",
      "Epoch   4/50 Batch   70/485 - Loss:  1.539, Seconds: 6.65\n",
      "Epoch   4/50 Batch   75/485 - Loss:  1.763, Seconds: 7.15\n",
      "Epoch   4/50 Batch   80/485 - Loss:  1.653, Seconds: 7.21\n",
      "Epoch   4/50 Batch   85/485 - Loss:  1.662, Seconds: 6.72\n",
      "Epoch   4/50 Batch   90/485 - Loss:  1.669, Seconds: 6.19\n",
      "Epoch   4/50 Batch   95/485 - Loss:  1.632, Seconds: 6.83\n",
      "Epoch   4/50 Batch  100/485 - Loss:  1.524, Seconds: 8.48\n",
      "Epoch   4/50 Batch  105/485 - Loss:  1.607, Seconds: 8.54\n",
      "Epoch   4/50 Batch  110/485 - Loss:  1.621, Seconds: 7.56\n",
      "Epoch   4/50 Batch  115/485 - Loss:  1.715, Seconds: 7.98\n",
      "Epoch   4/50 Batch  120/485 - Loss:  1.562, Seconds: 7.50\n",
      "Epoch   4/50 Batch  125/485 - Loss:  1.725, Seconds: 6.91\n",
      "Epoch   4/50 Batch  130/485 - Loss:  1.587, Seconds: 6.37\n",
      "Epoch   4/50 Batch  135/485 - Loss:  1.613, Seconds: 8.14\n",
      "Epoch   4/50 Batch  140/485 - Loss:  1.781, Seconds: 7.03\n",
      "Epoch   4/50 Batch  145/485 - Loss:  1.594, Seconds: 7.63\n",
      "Epoch   4/50 Batch  150/485 - Loss:  1.672, Seconds: 7.09\n",
      "Epoch   4/50 Batch  155/485 - Loss:  1.667, Seconds: 7.95\n",
      "Epoch   4/50 Batch  160/485 - Loss:  1.571, Seconds: 6.54\n",
      "Average loss for this update: 1.654\n",
      "New Record!\n",
      "Epoch   4/50 Batch  165/485 - Loss:  1.748, Seconds: 7.37\n",
      "Epoch   4/50 Batch  170/485 - Loss:  1.714, Seconds: 7.85\n",
      "Epoch   4/50 Batch  175/485 - Loss:  1.538, Seconds: 8.61\n",
      "Epoch   4/50 Batch  180/485 - Loss:  1.628, Seconds: 8.80\n",
      "Epoch   4/50 Batch  185/485 - Loss:  1.715, Seconds: 7.44\n",
      "Epoch   4/50 Batch  190/485 - Loss:  1.661, Seconds: 9.21\n",
      "Epoch   4/50 Batch  195/485 - Loss:  1.675, Seconds: 7.49\n",
      "Epoch   4/50 Batch  200/485 - Loss:  1.764, Seconds: 7.49\n",
      "Epoch   4/50 Batch  205/485 - Loss:  1.734, Seconds: 7.00\n",
      "Epoch   4/50 Batch  210/485 - Loss:  1.758, Seconds: 9.34\n",
      "Epoch   4/50 Batch  215/485 - Loss:  1.707, Seconds: 8.25\n",
      "Epoch   4/50 Batch  220/485 - Loss:  1.689, Seconds: 8.79\n",
      "Epoch   4/50 Batch  225/485 - Loss:  1.766, Seconds: 7.17\n",
      "Epoch   4/50 Batch  230/485 - Loss:  1.717, Seconds: 8.91\n",
      "Epoch   4/50 Batch  235/485 - Loss:  1.720, Seconds: 9.05\n",
      "Epoch   4/50 Batch  240/485 - Loss:  1.884, Seconds: 6.73\n",
      "Epoch   4/50 Batch  245/485 - Loss:  1.690, Seconds: 8.48\n",
      "Epoch   4/50 Batch  250/485 - Loss:  1.677, Seconds: 9.76\n",
      "Epoch   4/50 Batch  255/485 - Loss:  1.793, Seconds: 9.84\n",
      "Epoch   4/50 Batch  260/485 - Loss:  1.907, Seconds: 9.27\n",
      "Epoch   4/50 Batch  265/485 - Loss:  1.789, Seconds: 7.63\n",
      "Epoch   4/50 Batch  270/485 - Loss:  1.808, Seconds: 8.79\n",
      "Epoch   4/50 Batch  275/485 - Loss:  1.744, Seconds: 9.53\n",
      "Epoch   4/50 Batch  280/485 - Loss:  1.754, Seconds: 9.52\n",
      "Epoch   4/50 Batch  285/485 - Loss:  1.863, Seconds: 8.95\n",
      "Epoch   4/50 Batch  290/485 - Loss:  1.696, Seconds: 9.68\n",
      "Epoch   4/50 Batch  295/485 - Loss:  1.616, Seconds: 10.66\n",
      "Epoch   4/50 Batch  300/485 - Loss:  1.649, Seconds: 10.06\n",
      "Epoch   4/50 Batch  305/485 - Loss:  1.809, Seconds: 10.02\n",
      "Epoch   4/50 Batch  310/485 - Loss:  1.842, Seconds: 8.66\n",
      "Epoch   4/50 Batch  315/485 - Loss:  1.863, Seconds: 9.35\n",
      "Epoch   4/50 Batch  320/485 - Loss:  1.891, Seconds: 10.03\n",
      "Average loss for this update: 1.744\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  325/485 - Loss:  1.990, Seconds: 8.30\n",
      "Epoch   4/50 Batch  330/485 - Loss:  1.693, Seconds: 10.80\n",
      "Epoch   4/50 Batch  335/485 - Loss:  1.906, Seconds: 10.27\n",
      "Epoch   4/50 Batch  340/485 - Loss:  1.844, Seconds: 9.71\n",
      "Epoch   4/50 Batch  345/485 - Loss:  1.935, Seconds: 9.75\n",
      "Epoch   4/50 Batch  350/485 - Loss:  1.995, Seconds: 9.94\n",
      "Epoch   4/50 Batch  355/485 - Loss:  1.856, Seconds: 11.02\n",
      "Epoch   4/50 Batch  360/485 - Loss:  1.820, Seconds: 11.38\n",
      "Epoch   4/50 Batch  365/485 - Loss:  1.876, Seconds: 11.52\n",
      "Epoch   4/50 Batch  370/485 - Loss:  1.979, Seconds: 11.02\n",
      "Epoch   4/50 Batch  375/485 - Loss:  2.028, Seconds: 10.49\n",
      "Epoch   4/50 Batch  380/485 - Loss:  1.935, Seconds: 11.15\n",
      "Epoch   4/50 Batch  385/485 - Loss:  1.945, Seconds: 10.68\n",
      "Epoch   4/50 Batch  390/485 - Loss:  1.773, Seconds: 11.53\n",
      "Epoch   4/50 Batch  395/485 - Loss:  2.060, Seconds: 10.98\n",
      "Epoch   4/50 Batch  400/485 - Loss:  2.021, Seconds: 11.80\n",
      "Epoch   4/50 Batch  405/485 - Loss:  1.942, Seconds: 12.03\n",
      "Epoch   4/50 Batch  410/485 - Loss:  1.899, Seconds: 12.27\n",
      "Epoch   4/50 Batch  415/485 - Loss:  2.072, Seconds: 13.10\n",
      "Epoch   4/50 Batch  420/485 - Loss:  2.094, Seconds: 13.26\n",
      "Epoch   4/50 Batch  425/485 - Loss:  1.993, Seconds: 12.14\n",
      "Epoch   4/50 Batch  430/485 - Loss:  2.026, Seconds: 13.02\n",
      "Epoch   4/50 Batch  435/485 - Loss:  2.129, Seconds: 13.23\n",
      "Epoch   4/50 Batch  440/485 - Loss:  2.169, Seconds: 13.53\n",
      "Epoch   4/50 Batch  445/485 - Loss:  2.101, Seconds: 13.95\n",
      "Epoch   4/50 Batch  450/485 - Loss:  2.133, Seconds: 13.54\n",
      "Epoch   4/50 Batch  455/485 - Loss:  2.053, Seconds: 15.27\n",
      "Epoch   4/50 Batch  460/485 - Loss:  2.324, Seconds: 14.80\n",
      "Epoch   4/50 Batch  465/485 - Loss:  2.217, Seconds: 16.84\n",
      "Epoch   4/50 Batch  470/485 - Loss:  2.302, Seconds: 17.99\n",
      "Epoch   4/50 Batch  475/485 - Loss:  2.283, Seconds: 19.44\n",
      "Epoch   4/50 Batch  480/485 - Loss:  2.357, Seconds: 19.60\n",
      "Average loss for this update: 2.023\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch    5/485 - Loss:  1.983, Seconds: 6.57\n",
      "Epoch   5/50 Batch   10/485 - Loss:  1.527, Seconds: 6.89\n",
      "Epoch   5/50 Batch   15/485 - Loss:  1.529, Seconds: 5.78\n",
      "Epoch   5/50 Batch   20/485 - Loss:  1.449, Seconds: 7.63\n",
      "Epoch   5/50 Batch   25/485 - Loss:  1.601, Seconds: 5.35\n",
      "Epoch   5/50 Batch   30/485 - Loss:  1.445, Seconds: 7.67\n",
      "Epoch   5/50 Batch   35/485 - Loss:  1.554, Seconds: 7.02\n",
      "Epoch   5/50 Batch   40/485 - Loss:  1.643, Seconds: 5.42\n",
      "Epoch   5/50 Batch   45/485 - Loss:  1.573, Seconds: 7.00\n",
      "Epoch   5/50 Batch   50/485 - Loss:  1.577, Seconds: 7.56\n",
      "Epoch   5/50 Batch   55/485 - Loss:  1.548, Seconds: 5.04\n",
      "Epoch   5/50 Batch   60/485 - Loss:  1.574, Seconds: 4.91\n",
      "Epoch   5/50 Batch   65/485 - Loss:  1.515, Seconds: 5.97\n",
      "Epoch   5/50 Batch   70/485 - Loss:  1.432, Seconds: 6.66\n",
      "Epoch   5/50 Batch   75/485 - Loss:  1.662, Seconds: 7.15\n",
      "Epoch   5/50 Batch   80/485 - Loss:  1.556, Seconds: 7.17\n",
      "Epoch   5/50 Batch   85/485 - Loss:  1.552, Seconds: 6.85\n",
      "Epoch   5/50 Batch   90/485 - Loss:  1.569, Seconds: 6.28\n",
      "Epoch   5/50 Batch   95/485 - Loss:  1.534, Seconds: 6.95\n",
      "Epoch   5/50 Batch  100/485 - Loss:  1.453, Seconds: 8.58\n",
      "Epoch   5/50 Batch  105/485 - Loss:  1.507, Seconds: 8.60\n",
      "Epoch   5/50 Batch  110/485 - Loss:  1.528, Seconds: 7.58\n",
      "Epoch   5/50 Batch  115/485 - Loss:  1.627, Seconds: 8.17\n",
      "Epoch   5/50 Batch  120/485 - Loss:  1.475, Seconds: 7.48\n",
      "Epoch   5/50 Batch  125/485 - Loss:  1.589, Seconds: 6.94\n",
      "Epoch   5/50 Batch  130/485 - Loss:  1.515, Seconds: 6.39\n",
      "Epoch   5/50 Batch  135/485 - Loss:  1.543, Seconds: 8.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 Batch  140/485 - Loss:  1.684, Seconds: 7.13\n",
      "Epoch   5/50 Batch  145/485 - Loss:  1.496, Seconds: 7.83\n",
      "Epoch   5/50 Batch  150/485 - Loss:  1.598, Seconds: 7.15\n",
      "Epoch   5/50 Batch  155/485 - Loss:  1.597, Seconds: 7.83\n",
      "Epoch   5/50 Batch  160/485 - Loss:  1.467, Seconds: 6.58\n",
      "Average loss for this update: 1.559\n",
      "New Record!\n",
      "Epoch   5/50 Batch  165/485 - Loss:  1.618, Seconds: 7.33\n",
      "Epoch   5/50 Batch  170/485 - Loss:  1.654, Seconds: 8.24\n",
      "Epoch   5/50 Batch  175/485 - Loss:  1.456, Seconds: 8.56\n",
      "Epoch   5/50 Batch  180/485 - Loss:  1.523, Seconds: 8.73\n",
      "Epoch   5/50 Batch  185/485 - Loss:  1.603, Seconds: 7.50\n",
      "Epoch   5/50 Batch  190/485 - Loss:  1.581, Seconds: 9.21\n",
      "Epoch   5/50 Batch  195/485 - Loss:  1.573, Seconds: 7.46\n",
      "Epoch   5/50 Batch  200/485 - Loss:  1.694, Seconds: 7.42\n",
      "Epoch   5/50 Batch  205/485 - Loss:  1.641, Seconds: 7.00\n",
      "Epoch   5/50 Batch  210/485 - Loss:  1.657, Seconds: 9.28\n",
      "Epoch   5/50 Batch  215/485 - Loss:  1.616, Seconds: 8.31\n",
      "Epoch   5/50 Batch  220/485 - Loss:  1.633, Seconds: 8.84\n",
      "Epoch   5/50 Batch  225/485 - Loss:  1.691, Seconds: 7.14\n",
      "Epoch   5/50 Batch  230/485 - Loss:  1.632, Seconds: 8.92\n",
      "Epoch   5/50 Batch  235/485 - Loss:  1.607, Seconds: 8.98\n",
      "Epoch   5/50 Batch  240/485 - Loss:  1.777, Seconds: 6.71\n",
      "Epoch   5/50 Batch  245/485 - Loss:  1.597, Seconds: 8.49\n",
      "Epoch   5/50 Batch  250/485 - Loss:  1.598, Seconds: 9.81\n",
      "Epoch   5/50 Batch  255/485 - Loss:  1.688, Seconds: 9.87\n",
      "Epoch   5/50 Batch  260/485 - Loss:  1.790, Seconds: 9.22\n",
      "Epoch   5/50 Batch  265/485 - Loss:  1.693, Seconds: 7.64\n",
      "Epoch   5/50 Batch  270/485 - Loss:  1.747, Seconds: 8.77\n",
      "Epoch   5/50 Batch  275/485 - Loss:  1.634, Seconds: 9.46\n",
      "Epoch   5/50 Batch  280/485 - Loss:  1.631, Seconds: 9.64\n",
      "Epoch   5/50 Batch  285/485 - Loss:  1.792, Seconds: 9.25\n",
      "Epoch   5/50 Batch  290/485 - Loss:  1.651, Seconds: 9.66\n",
      "Epoch   5/50 Batch  295/485 - Loss:  1.530, Seconds: 10.55\n",
      "Epoch   5/50 Batch  300/485 - Loss:  1.565, Seconds: 9.85\n",
      "Epoch   5/50 Batch  305/485 - Loss:  1.725, Seconds: 9.98\n",
      "Epoch   5/50 Batch  310/485 - Loss:  1.767, Seconds: 8.94\n",
      "Epoch   5/50 Batch  315/485 - Loss:  1.756, Seconds: 9.37\n",
      "Epoch   5/50 Batch  320/485 - Loss:  1.793, Seconds: 10.17\n",
      "Average loss for this update: 1.654\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  325/485 - Loss:  1.912, Seconds: 8.33\n",
      "Epoch   5/50 Batch  330/485 - Loss:  1.601, Seconds: 10.88\n",
      "Epoch   5/50 Batch  335/485 - Loss:  1.818, Seconds: 10.37\n",
      "Epoch   5/50 Batch  340/485 - Loss:  1.770, Seconds: 9.85\n",
      "Epoch   5/50 Batch  345/485 - Loss:  1.828, Seconds: 9.95\n",
      "Epoch   5/50 Batch  350/485 - Loss:  1.893, Seconds: 10.15\n",
      "Epoch   5/50 Batch  355/485 - Loss:  1.774, Seconds: 10.84\n",
      "Epoch   5/50 Batch  360/485 - Loss:  1.765, Seconds: 11.50\n",
      "Epoch   5/50 Batch  365/485 - Loss:  1.804, Seconds: 11.64\n",
      "Epoch   5/50 Batch  370/485 - Loss:  1.884, Seconds: 11.15\n",
      "Epoch   5/50 Batch  375/485 - Loss:  1.905, Seconds: 10.50\n",
      "Epoch   5/50 Batch  380/485 - Loss:  1.908, Seconds: 11.44\n",
      "Epoch   5/50 Batch  385/485 - Loss:  1.870, Seconds: 10.79\n",
      "Epoch   5/50 Batch  390/485 - Loss:  1.715, Seconds: 11.66\n",
      "Epoch   5/50 Batch  395/485 - Loss:  1.961, Seconds: 11.04\n",
      "Epoch   5/50 Batch  400/485 - Loss:  1.934, Seconds: 11.74\n",
      "Epoch   5/50 Batch  405/485 - Loss:  1.864, Seconds: 11.97\n",
      "Epoch   5/50 Batch  410/485 - Loss:  1.836, Seconds: 12.26\n",
      "Epoch   5/50 Batch  415/485 - Loss:  1.980, Seconds: 13.25\n",
      "Epoch   5/50 Batch  420/485 - Loss:  1.984, Seconds: 13.53\n",
      "Epoch   5/50 Batch  425/485 - Loss:  1.891, Seconds: 12.38\n",
      "Epoch   5/50 Batch  430/485 - Loss:  1.960, Seconds: 13.12\n",
      "Epoch   5/50 Batch  435/485 - Loss:  2.046, Seconds: 13.40\n",
      "Epoch   5/50 Batch  440/485 - Loss:  2.091, Seconds: 13.74\n",
      "Epoch   5/50 Batch  445/485 - Loss:  2.050, Seconds: 13.97\n",
      "Epoch   5/50 Batch  450/485 - Loss:  2.056, Seconds: 13.71\n",
      "Epoch   5/50 Batch  455/485 - Loss:  1.940, Seconds: 15.09\n",
      "Epoch   5/50 Batch  460/485 - Loss:  2.230, Seconds: 14.83\n",
      "Epoch   5/50 Batch  465/485 - Loss:  2.151, Seconds: 16.89\n",
      "Epoch   5/50 Batch  470/485 - Loss:  2.210, Seconds: 17.93\n",
      "Epoch   5/50 Batch  475/485 - Loss:  2.178, Seconds: 19.54\n",
      "Epoch   5/50 Batch  480/485 - Loss:  2.295, Seconds: 19.87\n",
      "Average loss for this update: 1.941\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch    5/485 - Loss:  1.934, Seconds: 6.25\n",
      "Epoch   6/50 Batch   10/485 - Loss:  1.448, Seconds: 6.91\n",
      "Epoch   6/50 Batch   15/485 - Loss:  1.462, Seconds: 5.89\n",
      "Epoch   6/50 Batch   20/485 - Loss:  1.376, Seconds: 7.58\n",
      "Epoch   6/50 Batch   25/485 - Loss:  1.501, Seconds: 5.38\n",
      "Epoch   6/50 Batch   30/485 - Loss:  1.397, Seconds: 7.69\n",
      "Epoch   6/50 Batch   35/485 - Loss:  1.483, Seconds: 7.04\n",
      "Epoch   6/50 Batch   40/485 - Loss:  1.570, Seconds: 5.47\n",
      "Epoch   6/50 Batch   45/485 - Loss:  1.497, Seconds: 7.14\n",
      "Epoch   6/50 Batch   50/485 - Loss:  1.485, Seconds: 7.66\n",
      "Epoch   6/50 Batch   55/485 - Loss:  1.485, Seconds: 4.91\n",
      "Epoch   6/50 Batch   60/485 - Loss:  1.505, Seconds: 4.95\n",
      "Epoch   6/50 Batch   65/485 - Loss:  1.447, Seconds: 6.04\n",
      "Epoch   6/50 Batch   70/485 - Loss:  1.375, Seconds: 6.62\n",
      "Epoch   6/50 Batch   75/485 - Loss:  1.594, Seconds: 7.12\n",
      "Epoch   6/50 Batch   80/485 - Loss:  1.473, Seconds: 7.09\n",
      "Epoch   6/50 Batch   85/485 - Loss:  1.452, Seconds: 6.88\n",
      "Epoch   6/50 Batch   90/485 - Loss:  1.485, Seconds: 6.29\n",
      "Epoch   6/50 Batch   95/485 - Loss:  1.478, Seconds: 6.73\n",
      "Epoch   6/50 Batch  100/485 - Loss:  1.399, Seconds: 8.50\n",
      "Epoch   6/50 Batch  105/485 - Loss:  1.430, Seconds: 8.53\n",
      "Epoch   6/50 Batch  110/485 - Loss:  1.433, Seconds: 7.55\n",
      "Epoch   6/50 Batch  115/485 - Loss:  1.538, Seconds: 8.19\n",
      "Epoch   6/50 Batch  120/485 - Loss:  1.412, Seconds: 7.49\n",
      "Epoch   6/50 Batch  125/485 - Loss:  1.510, Seconds: 6.96\n",
      "Epoch   6/50 Batch  130/485 - Loss:  1.440, Seconds: 6.44\n",
      "Epoch   6/50 Batch  135/485 - Loss:  1.460, Seconds: 8.11\n",
      "Epoch   6/50 Batch  140/485 - Loss:  1.602, Seconds: 7.12\n",
      "Epoch   6/50 Batch  145/485 - Loss:  1.436, Seconds: 7.71\n",
      "Epoch   6/50 Batch  150/485 - Loss:  1.491, Seconds: 7.11\n",
      "Epoch   6/50 Batch  155/485 - Loss:  1.512, Seconds: 7.75\n",
      "Epoch   6/50 Batch  160/485 - Loss:  1.432, Seconds: 6.71\n",
      "Average loss for this update: 1.486\n",
      "New Record!\n",
      "Epoch   6/50 Batch  165/485 - Loss:  1.570, Seconds: 7.40\n",
      "Epoch   6/50 Batch  170/485 - Loss:  1.558, Seconds: 8.00\n",
      "Epoch   6/50 Batch  175/485 - Loss:  1.421, Seconds: 8.81\n",
      "Epoch   6/50 Batch  180/485 - Loss:  1.484, Seconds: 8.68\n",
      "Epoch   6/50 Batch  185/485 - Loss:  1.528, Seconds: 7.61\n",
      "Epoch   6/50 Batch  190/485 - Loss:  1.485, Seconds: 9.36\n",
      "Epoch   6/50 Batch  195/485 - Loss:  1.487, Seconds: 7.51\n",
      "Epoch   6/50 Batch  200/485 - Loss:  1.636, Seconds: 7.49\n",
      "Epoch   6/50 Batch  205/485 - Loss:  1.571, Seconds: 6.96\n",
      "Epoch   6/50 Batch  210/485 - Loss:  1.591, Seconds: 9.33\n",
      "Epoch   6/50 Batch  215/485 - Loss:  1.538, Seconds: 8.27\n",
      "Epoch   6/50 Batch  220/485 - Loss:  1.555, Seconds: 8.84\n",
      "Epoch   6/50 Batch  225/485 - Loss:  1.573, Seconds: 7.15\n",
      "Epoch   6/50 Batch  230/485 - Loss:  1.560, Seconds: 8.97\n",
      "Epoch   6/50 Batch  235/485 - Loss:  1.537, Seconds: 9.14\n",
      "Epoch   6/50 Batch  240/485 - Loss:  1.685, Seconds: 6.75\n",
      "Epoch   6/50 Batch  245/485 - Loss:  1.524, Seconds: 8.53\n",
      "Epoch   6/50 Batch  250/485 - Loss:  1.541, Seconds: 9.78\n",
      "Epoch   6/50 Batch  255/485 - Loss:  1.626, Seconds: 9.85\n",
      "Epoch   6/50 Batch  260/485 - Loss:  1.698, Seconds: 9.29\n",
      "Epoch   6/50 Batch  265/485 - Loss:  1.622, Seconds: 7.56\n",
      "Epoch   6/50 Batch  270/485 - Loss:  1.674, Seconds: 8.82\n",
      "Epoch   6/50 Batch  275/485 - Loss:  1.594, Seconds: 9.47\n",
      "Epoch   6/50 Batch  280/485 - Loss:  1.573, Seconds: 9.53\n",
      "Epoch   6/50 Batch  285/485 - Loss:  1.733, Seconds: 9.03\n",
      "Epoch   6/50 Batch  290/485 - Loss:  1.589, Seconds: 9.69\n",
      "Epoch   6/50 Batch  295/485 - Loss:  1.486, Seconds: 10.29\n",
      "Epoch   6/50 Batch  300/485 - Loss:  1.501, Seconds: 9.86\n",
      "Epoch   6/50 Batch  305/485 - Loss:  1.634, Seconds: 9.94\n",
      "Epoch   6/50 Batch  310/485 - Loss:  1.705, Seconds: 8.67\n",
      "Epoch   6/50 Batch  315/485 - Loss:  1.699, Seconds: 9.32\n",
      "Epoch   6/50 Batch  320/485 - Loss:  1.717, Seconds: 9.98\n",
      "Average loss for this update: 1.584\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  325/485 - Loss:  1.815, Seconds: 8.19\n",
      "Epoch   6/50 Batch  330/485 - Loss:  1.532, Seconds: 10.76\n",
      "Epoch   6/50 Batch  335/485 - Loss:  1.760, Seconds: 10.34\n",
      "Epoch   6/50 Batch  340/485 - Loss:  1.718, Seconds: 9.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/50 Batch  345/485 - Loss:  1.752, Seconds: 10.11\n",
      "Epoch   6/50 Batch  350/485 - Loss:  1.838, Seconds: 9.84\n",
      "Epoch   6/50 Batch  355/485 - Loss:  1.694, Seconds: 10.66\n",
      "Epoch   6/50 Batch  360/485 - Loss:  1.716, Seconds: 11.37\n",
      "Epoch   6/50 Batch  365/485 - Loss:  1.752, Seconds: 11.52\n",
      "Epoch   6/50 Batch  370/485 - Loss:  1.873, Seconds: 10.95\n",
      "Epoch   6/50 Batch  375/485 - Loss:  1.835, Seconds: 10.53\n",
      "Epoch   6/50 Batch  380/485 - Loss:  1.773, Seconds: 11.70\n",
      "Epoch   6/50 Batch  385/485 - Loss:  1.786, Seconds: 10.70\n",
      "Epoch   6/50 Batch  390/485 - Loss:  1.658, Seconds: 11.54\n",
      "Epoch   6/50 Batch  395/485 - Loss:  1.903, Seconds: 11.02\n",
      "Epoch   6/50 Batch  400/485 - Loss:  1.859, Seconds: 11.79\n",
      "Epoch   6/50 Batch  405/485 - Loss:  1.799, Seconds: 11.86\n",
      "Epoch   6/50 Batch  410/485 - Loss:  1.773, Seconds: 12.22\n",
      "Epoch   6/50 Batch  415/485 - Loss:  1.897, Seconds: 13.04\n",
      "Epoch   6/50 Batch  420/485 - Loss:  1.903, Seconds: 13.16\n",
      "Epoch   6/50 Batch  425/485 - Loss:  1.805, Seconds: 12.22\n",
      "Epoch   6/50 Batch  430/485 - Loss:  1.882, Seconds: 13.39\n",
      "Epoch   6/50 Batch  435/485 - Loss:  1.971, Seconds: 13.29\n",
      "Epoch   6/50 Batch  440/485 - Loss:  1.997, Seconds: 13.68\n",
      "Epoch   6/50 Batch  445/485 - Loss:  1.982, Seconds: 14.03\n",
      "Epoch   6/50 Batch  450/485 - Loss:  1.997, Seconds: 13.83\n",
      "Epoch   6/50 Batch  455/485 - Loss:  1.898, Seconds: 14.97\n",
      "Epoch   6/50 Batch  460/485 - Loss:  2.178, Seconds: 14.90\n",
      "Epoch   6/50 Batch  465/485 - Loss:  2.102, Seconds: 16.91\n",
      "Epoch   6/50 Batch  470/485 - Loss:  2.135, Seconds: 18.22\n",
      "Epoch   6/50 Batch  475/485 - Loss:  2.128, Seconds: 19.41\n",
      "Epoch   6/50 Batch  480/485 - Loss:  2.189, Seconds: 19.60\n",
      "Average loss for this update: 1.872\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch    5/485 - Loss:  1.833, Seconds: 6.23\n",
      "Epoch   7/50 Batch   10/485 - Loss:  1.387, Seconds: 6.83\n",
      "Epoch   7/50 Batch   15/485 - Loss:  1.388, Seconds: 5.70\n",
      "Epoch   7/50 Batch   20/485 - Loss:  1.294, Seconds: 7.57\n",
      "Epoch   7/50 Batch   25/485 - Loss:  1.420, Seconds: 5.36\n",
      "Epoch   7/50 Batch   30/485 - Loss:  1.352, Seconds: 7.59\n",
      "Epoch   7/50 Batch   35/485 - Loss:  1.416, Seconds: 7.11\n",
      "Epoch   7/50 Batch   40/485 - Loss:  1.491, Seconds: 5.36\n",
      "Epoch   7/50 Batch   45/485 - Loss:  1.430, Seconds: 7.25\n",
      "Epoch   7/50 Batch   50/485 - Loss:  1.437, Seconds: 7.63\n",
      "Epoch   7/50 Batch   55/485 - Loss:  1.422, Seconds: 4.87\n",
      "Epoch   7/50 Batch   60/485 - Loss:  1.444, Seconds: 4.86\n",
      "Epoch   7/50 Batch   65/485 - Loss:  1.368, Seconds: 6.02\n",
      "Epoch   7/50 Batch   70/485 - Loss:  1.310, Seconds: 6.61\n",
      "Epoch   7/50 Batch   75/485 - Loss:  1.533, Seconds: 7.12\n",
      "Epoch   7/50 Batch   80/485 - Loss:  1.408, Seconds: 7.15\n",
      "Epoch   7/50 Batch   85/485 - Loss:  1.393, Seconds: 6.88\n",
      "Epoch   7/50 Batch   90/485 - Loss:  1.413, Seconds: 6.18\n",
      "Epoch   7/50 Batch   95/485 - Loss:  1.389, Seconds: 6.78\n",
      "Epoch   7/50 Batch  100/485 - Loss:  1.332, Seconds: 8.56\n",
      "Epoch   7/50 Batch  105/485 - Loss:  1.377, Seconds: 8.61\n",
      "Epoch   7/50 Batch  110/485 - Loss:  1.390, Seconds: 7.53\n",
      "Epoch   7/50 Batch  115/485 - Loss:  1.503, Seconds: 8.05\n",
      "Epoch   7/50 Batch  120/485 - Loss:  1.338, Seconds: 7.48\n",
      "Epoch   7/50 Batch  125/485 - Loss:  1.464, Seconds: 7.00\n",
      "Epoch   7/50 Batch  130/485 - Loss:  1.361, Seconds: 6.39\n",
      "Epoch   7/50 Batch  135/485 - Loss:  1.397, Seconds: 8.22\n",
      "Epoch   7/50 Batch  140/485 - Loss:  1.539, Seconds: 7.04\n",
      "Epoch   7/50 Batch  145/485 - Loss:  1.391, Seconds: 7.62\n",
      "Epoch   7/50 Batch  150/485 - Loss:  1.440, Seconds: 7.15\n",
      "Epoch   7/50 Batch  155/485 - Loss:  1.447, Seconds: 7.83\n",
      "Epoch   7/50 Batch  160/485 - Loss:  1.369, Seconds: 6.61\n",
      "Average loss for this update: 1.421\n",
      "New Record!\n",
      "Epoch   7/50 Batch  165/485 - Loss:  1.519, Seconds: 7.41\n",
      "Epoch   7/50 Batch  170/485 - Loss:  1.503, Seconds: 7.96\n",
      "Epoch   7/50 Batch  175/485 - Loss:  1.354, Seconds: 8.71\n",
      "Epoch   7/50 Batch  180/485 - Loss:  1.411, Seconds: 8.70\n",
      "Epoch   7/50 Batch  185/485 - Loss:  1.504, Seconds: 7.56\n",
      "Epoch   7/50 Batch  190/485 - Loss:  1.437, Seconds: 9.20\n",
      "Epoch   7/50 Batch  195/485 - Loss:  1.423, Seconds: 7.58\n",
      "Epoch   7/50 Batch  200/485 - Loss:  1.558, Seconds: 7.56\n",
      "Epoch   7/50 Batch  205/485 - Loss:  1.533, Seconds: 7.09\n",
      "Epoch   7/50 Batch  210/485 - Loss:  1.543, Seconds: 9.31\n",
      "Epoch   7/50 Batch  215/485 - Loss:  1.480, Seconds: 8.31\n",
      "Epoch   7/50 Batch  220/485 - Loss:  1.482, Seconds: 8.98\n",
      "Epoch   7/50 Batch  225/485 - Loss:  1.515, Seconds: 7.32\n",
      "Epoch   7/50 Batch  230/485 - Loss:  1.529, Seconds: 8.99\n",
      "Epoch   7/50 Batch  235/485 - Loss:  1.475, Seconds: 9.06\n",
      "Epoch   7/50 Batch  240/485 - Loss:  1.596, Seconds: 6.71\n",
      "Epoch   7/50 Batch  245/485 - Loss:  1.482, Seconds: 8.56\n",
      "Epoch   7/50 Batch  250/485 - Loss:  1.485, Seconds: 9.83\n",
      "Epoch   7/50 Batch  255/485 - Loss:  1.558, Seconds: 9.83\n",
      "Epoch   7/50 Batch  260/485 - Loss:  1.635, Seconds: 9.38\n",
      "Epoch   7/50 Batch  265/485 - Loss:  1.543, Seconds: 7.56\n",
      "Epoch   7/50 Batch  270/485 - Loss:  1.593, Seconds: 8.76\n",
      "Epoch   7/50 Batch  275/485 - Loss:  1.523, Seconds: 9.76\n",
      "Epoch   7/50 Batch  280/485 - Loss:  1.500, Seconds: 9.61\n",
      "Epoch   7/50 Batch  285/485 - Loss:  1.686, Seconds: 8.95\n",
      "Epoch   7/50 Batch  290/485 - Loss:  1.519, Seconds: 9.69\n",
      "Epoch   7/50 Batch  295/485 - Loss:  1.418, Seconds: 10.41\n",
      "Epoch   7/50 Batch  300/485 - Loss:  1.441, Seconds: 9.85\n",
      "Epoch   7/50 Batch  305/485 - Loss:  1.566, Seconds: 9.96\n",
      "Epoch   7/50 Batch  310/485 - Loss:  1.616, Seconds: 8.66\n",
      "Epoch   7/50 Batch  315/485 - Loss:  1.643, Seconds: 9.50\n",
      "Epoch   7/50 Batch  320/485 - Loss:  1.673, Seconds: 10.02\n",
      "Average loss for this update: 1.523\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  325/485 - Loss:  1.734, Seconds: 8.28\n",
      "Epoch   7/50 Batch  330/485 - Loss:  1.476, Seconds: 10.86\n",
      "Epoch   7/50 Batch  335/485 - Loss:  1.687, Seconds: 10.23\n",
      "Epoch   7/50 Batch  340/485 - Loss:  1.628, Seconds: 9.81\n",
      "Epoch   7/50 Batch  345/485 - Loss:  1.700, Seconds: 9.78\n",
      "Epoch   7/50 Batch  350/485 - Loss:  1.782, Seconds: 9.88\n",
      "Epoch   7/50 Batch  355/485 - Loss:  1.642, Seconds: 10.78\n",
      "Epoch   7/50 Batch  360/485 - Loss:  1.621, Seconds: 11.46\n",
      "Epoch   7/50 Batch  365/485 - Loss:  1.657, Seconds: 11.54\n",
      "Epoch   7/50 Batch  370/485 - Loss:  1.791, Seconds: 10.95\n",
      "Epoch   7/50 Batch  375/485 - Loss:  1.781, Seconds: 10.43\n",
      "Epoch   7/50 Batch  380/485 - Loss:  1.751, Seconds: 11.25\n",
      "Epoch   7/50 Batch  385/485 - Loss:  1.718, Seconds: 10.79\n",
      "Epoch   7/50 Batch  390/485 - Loss:  1.573, Seconds: 11.60\n",
      "Epoch   7/50 Batch  395/485 - Loss:  1.803, Seconds: 10.94\n",
      "Epoch   7/50 Batch  400/485 - Loss:  1.791, Seconds: 11.75\n",
      "Epoch   7/50 Batch  405/485 - Loss:  1.747, Seconds: 11.98\n",
      "Epoch   7/50 Batch  410/485 - Loss:  1.713, Seconds: 12.08\n",
      "Epoch   7/50 Batch  415/485 - Loss:  1.835, Seconds: 13.03\n",
      "Epoch   7/50 Batch  420/485 - Loss:  1.835, Seconds: 13.32\n",
      "Epoch   7/50 Batch  425/485 - Loss:  1.736, Seconds: 12.12\n",
      "Epoch   7/50 Batch  430/485 - Loss:  1.822, Seconds: 13.30\n",
      "Epoch   7/50 Batch  435/485 - Loss:  1.910, Seconds: 13.55\n",
      "Epoch   7/50 Batch  440/485 - Loss:  1.920, Seconds: 13.73\n",
      "Epoch   7/50 Batch  445/485 - Loss:  1.910, Seconds: 13.98\n",
      "Epoch   7/50 Batch  450/485 - Loss:  1.947, Seconds: 13.76\n",
      "Epoch   7/50 Batch  455/485 - Loss:  1.829, Seconds: 15.07\n",
      "Epoch   7/50 Batch  460/485 - Loss:  2.113, Seconds: 14.72\n",
      "Epoch   7/50 Batch  465/485 - Loss:  2.038, Seconds: 16.92\n",
      "Epoch   7/50 Batch  470/485 - Loss:  2.050, Seconds: 18.22\n",
      "Epoch   7/50 Batch  475/485 - Loss:  2.028, Seconds: 19.39\n",
      "Epoch   7/50 Batch  480/485 - Loss:  2.128, Seconds: 19.68\n",
      "Average loss for this update: 1.803\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch    5/485 - Loss:  1.776, Seconds: 6.33\n",
      "Epoch   8/50 Batch   10/485 - Loss:  1.424, Seconds: 6.87\n",
      "Epoch   8/50 Batch   15/485 - Loss:  1.451, Seconds: 5.74\n",
      "Epoch   8/50 Batch   20/485 - Loss:  1.347, Seconds: 7.73\n",
      "Epoch   8/50 Batch   25/485 - Loss:  1.448, Seconds: 5.35\n",
      "Epoch   8/50 Batch   30/485 - Loss:  1.399, Seconds: 7.65\n",
      "Epoch   8/50 Batch   35/485 - Loss:  1.419, Seconds: 7.13\n",
      "Epoch   8/50 Batch   40/485 - Loss:  1.477, Seconds: 5.42\n",
      "Epoch   8/50 Batch   45/485 - Loss:  1.450, Seconds: 7.04\n",
      "Epoch   8/50 Batch   50/485 - Loss:  1.426, Seconds: 7.67\n",
      "Epoch   8/50 Batch   55/485 - Loss:  1.412, Seconds: 4.95\n",
      "Epoch   8/50 Batch   60/485 - Loss:  1.435, Seconds: 4.90\n",
      "Epoch   8/50 Batch   65/485 - Loss:  1.383, Seconds: 6.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/50 Batch   70/485 - Loss:  1.293, Seconds: 6.64\n",
      "Epoch   8/50 Batch   75/485 - Loss:  1.517, Seconds: 7.19\n",
      "Epoch   8/50 Batch   80/485 - Loss:  1.394, Seconds: 7.20\n",
      "Epoch   8/50 Batch   85/485 - Loss:  1.389, Seconds: 6.88\n",
      "Epoch   8/50 Batch   90/485 - Loss:  1.419, Seconds: 6.28\n",
      "Epoch   8/50 Batch   95/485 - Loss:  1.386, Seconds: 6.77\n",
      "Epoch   8/50 Batch  100/485 - Loss:  1.311, Seconds: 8.61\n",
      "Epoch   8/50 Batch  105/485 - Loss:  1.347, Seconds: 8.62\n",
      "Epoch   8/50 Batch  110/485 - Loss:  1.333, Seconds: 7.53\n",
      "Epoch   8/50 Batch  115/485 - Loss:  1.470, Seconds: 8.12\n",
      "Epoch   8/50 Batch  120/485 - Loss:  1.331, Seconds: 7.48\n",
      "Epoch   8/50 Batch  125/485 - Loss:  1.442, Seconds: 7.15\n",
      "Epoch   8/50 Batch  130/485 - Loss:  1.373, Seconds: 6.47\n",
      "Epoch   8/50 Batch  135/485 - Loss:  1.386, Seconds: 8.15\n",
      "Epoch   8/50 Batch  140/485 - Loss:  1.509, Seconds: 7.11\n",
      "Epoch   8/50 Batch  145/485 - Loss:  1.379, Seconds: 7.76\n",
      "Epoch   8/50 Batch  150/485 - Loss:  1.438, Seconds: 7.26\n",
      "Epoch   8/50 Batch  155/485 - Loss:  1.427, Seconds: 7.94\n",
      "Epoch   8/50 Batch  160/485 - Loss:  1.334, Seconds: 6.68\n",
      "Average loss for this update: 1.416\n",
      "New Record!\n",
      "Epoch   8/50 Batch  165/485 - Loss:  1.455, Seconds: 7.42\n",
      "Epoch   8/50 Batch  170/485 - Loss:  1.473, Seconds: 8.01\n",
      "Epoch   8/50 Batch  175/485 - Loss:  1.337, Seconds: 8.71\n",
      "Epoch   8/50 Batch  180/485 - Loss:  1.393, Seconds: 8.72\n",
      "Epoch   8/50 Batch  185/485 - Loss:  1.486, Seconds: 7.65\n",
      "Epoch   8/50 Batch  190/485 - Loss:  1.406, Seconds: 9.23\n",
      "Epoch   8/50 Batch  195/485 - Loss:  1.411, Seconds: 7.63\n",
      "Epoch   8/50 Batch  200/485 - Loss:  1.520, Seconds: 7.71\n",
      "Epoch   8/50 Batch  205/485 - Loss:  1.496, Seconds: 7.03\n",
      "Epoch   8/50 Batch  210/485 - Loss:  1.493, Seconds: 9.46\n",
      "Epoch   8/50 Batch  215/485 - Loss:  1.469, Seconds: 8.29\n",
      "Epoch   8/50 Batch  220/485 - Loss:  1.465, Seconds: 9.01\n",
      "Epoch   8/50 Batch  225/485 - Loss:  1.483, Seconds: 7.21\n",
      "Epoch   8/50 Batch  230/485 - Loss:  1.479, Seconds: 8.95\n",
      "Epoch   8/50 Batch  235/485 - Loss:  1.452, Seconds: 9.05\n",
      "Epoch   8/50 Batch  240/485 - Loss:  1.570, Seconds: 6.78\n",
      "Epoch   8/50 Batch  245/485 - Loss:  1.461, Seconds: 8.47\n",
      "Epoch   8/50 Batch  250/485 - Loss:  1.456, Seconds: 9.89\n",
      "Epoch   8/50 Batch  255/485 - Loss:  1.551, Seconds: 9.80\n",
      "Epoch   8/50 Batch  260/485 - Loss:  1.587, Seconds: 9.47\n",
      "Epoch   8/50 Batch  265/485 - Loss:  1.511, Seconds: 7.71\n",
      "Epoch   8/50 Batch  270/485 - Loss:  1.581, Seconds: 8.83\n",
      "Epoch   8/50 Batch  275/485 - Loss:  1.500, Seconds: 9.59\n",
      "Epoch   8/50 Batch  280/485 - Loss:  1.453, Seconds: 9.60\n",
      "Epoch   8/50 Batch  285/485 - Loss:  1.638, Seconds: 8.93\n",
      "Epoch   8/50 Batch  290/485 - Loss:  1.489, Seconds: 9.86\n",
      "Epoch   8/50 Batch  295/485 - Loss:  1.397, Seconds: 10.34\n",
      "Epoch   8/50 Batch  300/485 - Loss:  1.415, Seconds: 9.79\n",
      "Epoch   8/50 Batch  305/485 - Loss:  1.548, Seconds: 9.85\n",
      "Epoch   8/50 Batch  310/485 - Loss:  1.605, Seconds: 8.64\n",
      "Epoch   8/50 Batch  315/485 - Loss:  1.605, Seconds: 9.39\n",
      "Epoch   8/50 Batch  320/485 - Loss:  1.632, Seconds: 10.11\n",
      "Average loss for this update: 1.494\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  325/485 - Loss:  1.721, Seconds: 8.32\n",
      "Epoch   8/50 Batch  330/485 - Loss:  1.485, Seconds: 10.89\n",
      "Epoch   8/50 Batch  335/485 - Loss:  1.657, Seconds: 10.30\n",
      "Epoch   8/50 Batch  340/485 - Loss:  1.575, Seconds: 9.82\n",
      "Epoch   8/50 Batch  345/485 - Loss:  1.668, Seconds: 9.86\n",
      "Epoch   8/50 Batch  350/485 - Loss:  1.743, Seconds: 10.26\n",
      "Epoch   8/50 Batch  355/485 - Loss:  1.604, Seconds: 10.70\n",
      "Epoch   8/50 Batch  360/485 - Loss:  1.611, Seconds: 11.47\n",
      "Epoch   8/50 Batch  365/485 - Loss:  1.632, Seconds: 11.51\n",
      "Epoch   8/50 Batch  370/485 - Loss:  1.713, Seconds: 11.01\n",
      "Epoch   8/50 Batch  375/485 - Loss:  1.737, Seconds: 10.52\n",
      "Epoch   8/50 Batch  380/485 - Loss:  1.709, Seconds: 11.24\n",
      "Epoch   8/50 Batch  385/485 - Loss:  1.688, Seconds: 10.76\n",
      "Epoch   8/50 Batch  390/485 - Loss:  1.564, Seconds: 11.58\n",
      "Epoch   8/50 Batch  395/485 - Loss:  1.783, Seconds: 10.95\n",
      "Epoch   8/50 Batch  400/485 - Loss:  1.733, Seconds: 11.95\n",
      "Epoch   8/50 Batch  405/485 - Loss:  1.693, Seconds: 12.40\n",
      "Epoch   8/50 Batch  410/485 - Loss:  1.671, Seconds: 12.18\n",
      "Epoch   8/50 Batch  415/485 - Loss:  1.804, Seconds: 13.22\n",
      "Epoch   8/50 Batch  420/485 - Loss:  1.783, Seconds: 13.34\n",
      "Epoch   8/50 Batch  425/485 - Loss:  1.703, Seconds: 12.29\n",
      "Epoch   8/50 Batch  430/485 - Loss:  1.781, Seconds: 13.24\n",
      "Epoch   8/50 Batch  435/485 - Loss:  1.903, Seconds: 13.35\n",
      "Epoch   8/50 Batch  440/485 - Loss:  1.869, Seconds: 13.97\n",
      "Epoch   8/50 Batch  445/485 - Loss:  1.875, Seconds: 14.07\n",
      "Epoch   8/50 Batch  450/485 - Loss:  1.908, Seconds: 13.96\n",
      "Epoch   8/50 Batch  455/485 - Loss:  1.797, Seconds: 14.89\n",
      "Epoch   8/50 Batch  460/485 - Loss:  2.079, Seconds: 14.76\n",
      "Epoch   8/50 Batch  465/485 - Loss:  2.005, Seconds: 17.03\n",
      "Epoch   8/50 Batch  470/485 - Loss:  2.035, Seconds: 18.12\n",
      "Epoch   8/50 Batch  475/485 - Loss:  1.991, Seconds: 19.65\n",
      "Epoch   8/50 Batch  480/485 - Loss:  2.072, Seconds: 19.65\n",
      "Average loss for this update: 1.769\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch    5/485 - Loss:  1.769, Seconds: 6.82\n",
      "Epoch   9/50 Batch   10/485 - Loss:  1.318, Seconds: 6.96\n",
      "Epoch   9/50 Batch   15/485 - Loss:  1.323, Seconds: 5.93\n",
      "Epoch   9/50 Batch   20/485 - Loss:  1.216, Seconds: 7.73\n",
      "Epoch   9/50 Batch   25/485 - Loss:  1.332, Seconds: 5.48\n",
      "Epoch   9/50 Batch   30/485 - Loss:  1.275, Seconds: 8.03\n",
      "Epoch   9/50 Batch   35/485 - Loss:  1.352, Seconds: 7.40\n",
      "Epoch   9/50 Batch   40/485 - Loss:  1.402, Seconds: 5.63\n",
      "Epoch   9/50 Batch   45/485 - Loss:  1.338, Seconds: 7.14\n",
      "Epoch   9/50 Batch   50/485 - Loss:  1.343, Seconds: 7.71\n",
      "Epoch   9/50 Batch   55/485 - Loss:  1.294, Seconds: 5.16\n",
      "Epoch   9/50 Batch   60/485 - Loss:  1.342, Seconds: 4.98\n",
      "Epoch   9/50 Batch   65/485 - Loss:  1.295, Seconds: 6.04\n",
      "Epoch   9/50 Batch   70/485 - Loss:  1.218, Seconds: 6.70\n",
      "Epoch   9/50 Batch   75/485 - Loss:  1.450, Seconds: 7.29\n",
      "Epoch   9/50 Batch   80/485 - Loss:  1.264, Seconds: 7.19\n",
      "Epoch   9/50 Batch   85/485 - Loss:  1.276, Seconds: 6.90\n",
      "Epoch   9/50 Batch   90/485 - Loss:  1.332, Seconds: 6.32\n",
      "Epoch   9/50 Batch   95/485 - Loss:  1.285, Seconds: 6.83\n",
      "Epoch   9/50 Batch  100/485 - Loss:  1.214, Seconds: 8.54\n",
      "Epoch   9/50 Batch  105/485 - Loss:  1.277, Seconds: 8.63\n",
      "Epoch   9/50 Batch  110/485 - Loss:  1.281, Seconds: 7.56\n",
      "Epoch   9/50 Batch  115/485 - Loss:  1.393, Seconds: 8.14\n",
      "Epoch   9/50 Batch  120/485 - Loss:  1.256, Seconds: 7.53\n",
      "Epoch   9/50 Batch  125/485 - Loss:  1.351, Seconds: 7.11\n",
      "Epoch   9/50 Batch  130/485 - Loss:  1.283, Seconds: 6.40\n",
      "Epoch   9/50 Batch  135/485 - Loss:  1.298, Seconds: 8.28\n",
      "Epoch   9/50 Batch  140/485 - Loss:  1.406, Seconds: 7.13\n",
      "Epoch   9/50 Batch  145/485 - Loss:  1.277, Seconds: 7.77\n",
      "Epoch   9/50 Batch  150/485 - Loss:  1.321, Seconds: 7.19\n",
      "Epoch   9/50 Batch  155/485 - Loss:  1.369, Seconds: 7.89\n",
      "Epoch   9/50 Batch  160/485 - Loss:  1.248, Seconds: 6.77\n",
      "Average loss for this update: 1.325\n",
      "New Record!\n",
      "Epoch   9/50 Batch  165/485 - Loss:  1.377, Seconds: 7.54\n",
      "Epoch   9/50 Batch  170/485 - Loss:  1.392, Seconds: 7.90\n",
      "Epoch   9/50 Batch  175/485 - Loss:  1.265, Seconds: 8.86\n",
      "Epoch   9/50 Batch  180/485 - Loss:  1.295, Seconds: 8.85\n",
      "Epoch   9/50 Batch  185/485 - Loss:  1.387, Seconds: 7.55\n",
      "Epoch   9/50 Batch  190/485 - Loss:  1.321, Seconds: 9.37\n",
      "Epoch   9/50 Batch  195/485 - Loss:  1.334, Seconds: 7.71\n",
      "Epoch   9/50 Batch  200/485 - Loss:  1.446, Seconds: 7.56\n",
      "Epoch   9/50 Batch  205/485 - Loss:  1.449, Seconds: 7.04\n",
      "Epoch   9/50 Batch  210/485 - Loss:  1.437, Seconds: 9.41\n",
      "Epoch   9/50 Batch  215/485 - Loss:  1.418, Seconds: 8.28\n",
      "Epoch   9/50 Batch  220/485 - Loss:  1.409, Seconds: 8.87\n",
      "Epoch   9/50 Batch  225/485 - Loss:  1.412, Seconds: 7.33\n",
      "Epoch   9/50 Batch  230/485 - Loss:  1.399, Seconds: 9.04\n",
      "Epoch   9/50 Batch  235/485 - Loss:  1.407, Seconds: 9.05\n",
      "Epoch   9/50 Batch  240/485 - Loss:  1.476, Seconds: 6.72\n",
      "Epoch   9/50 Batch  245/485 - Loss:  1.382, Seconds: 8.51\n",
      "Epoch   9/50 Batch  250/485 - Loss:  1.387, Seconds: 9.79\n",
      "Epoch   9/50 Batch  255/485 - Loss:  1.447, Seconds: 9.79\n",
      "Epoch   9/50 Batch  260/485 - Loss:  1.507, Seconds: 9.32\n",
      "Epoch   9/50 Batch  265/485 - Loss:  1.444, Seconds: 7.57\n",
      "Epoch   9/50 Batch  270/485 - Loss:  1.539, Seconds: 8.75\n",
      "Epoch   9/50 Batch  275/485 - Loss:  1.426, Seconds: 9.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/50 Batch  280/485 - Loss:  1.403, Seconds: 9.60\n",
      "Epoch   9/50 Batch  285/485 - Loss:  1.564, Seconds: 9.11\n",
      "Epoch   9/50 Batch  290/485 - Loss:  1.435, Seconds: 9.88\n",
      "Epoch   9/50 Batch  295/485 - Loss:  1.331, Seconds: 10.56\n",
      "Epoch   9/50 Batch  300/485 - Loss:  1.347, Seconds: 10.48\n",
      "Epoch   9/50 Batch  305/485 - Loss:  1.482, Seconds: 10.10\n",
      "Epoch   9/50 Batch  310/485 - Loss:  1.527, Seconds: 8.80\n",
      "Epoch   9/50 Batch  315/485 - Loss:  1.540, Seconds: 10.04\n",
      "Epoch   9/50 Batch  320/485 - Loss:  1.532, Seconds: 10.40\n",
      "Average loss for this update: 1.422\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch  325/485 - Loss:  1.636, Seconds: 8.75\n",
      "Epoch   9/50 Batch  330/485 - Loss:  1.374, Seconds: 11.38\n",
      "Epoch   9/50 Batch  335/485 - Loss:  1.599, Seconds: 10.28\n",
      "Epoch   9/50 Batch  340/485 - Loss:  1.536, Seconds: 9.81\n",
      "Epoch   9/50 Batch  345/485 - Loss:  1.579, Seconds: 10.62\n",
      "Epoch   9/50 Batch  350/485 - Loss:  1.642, Seconds: 10.43\n",
      "Epoch   9/50 Batch  355/485 - Loss:  1.531, Seconds: 10.72\n",
      "Epoch   9/50 Batch  360/485 - Loss:  1.529, Seconds: 11.53\n",
      "Epoch   9/50 Batch  365/485 - Loss:  1.582, Seconds: 11.73\n",
      "Epoch   9/50 Batch  370/485 - Loss:  1.667, Seconds: 11.69\n",
      "Epoch   9/50 Batch  375/485 - Loss:  1.661, Seconds: 11.11\n",
      "Epoch   9/50 Batch  380/485 - Loss:  1.648, Seconds: 11.44\n",
      "Epoch   9/50 Batch  385/485 - Loss:  1.630, Seconds: 10.83\n",
      "Epoch   9/50 Batch  390/485 - Loss:  1.505, Seconds: 11.74\n",
      "Epoch   9/50 Batch  395/485 - Loss:  1.716, Seconds: 11.21\n",
      "Epoch   9/50 Batch  400/485 - Loss:  1.642, Seconds: 12.30\n",
      "Epoch   9/50 Batch  405/485 - Loss:  1.634, Seconds: 12.82\n",
      "Epoch   9/50 Batch  410/485 - Loss:  1.594, Seconds: 12.86\n",
      "Epoch   9/50 Batch  415/485 - Loss:  1.711, Seconds: 13.38\n",
      "Epoch   9/50 Batch  420/485 - Loss:  1.707, Seconds: 13.49\n",
      "Epoch   9/50 Batch  425/485 - Loss:  1.638, Seconds: 12.20\n",
      "Epoch   9/50 Batch  430/485 - Loss:  1.720, Seconds: 13.37\n",
      "Epoch   9/50 Batch  435/485 - Loss:  1.817, Seconds: 14.31\n",
      "Epoch   9/50 Batch  440/485 - Loss:  1.783, Seconds: 14.32\n",
      "Epoch   9/50 Batch  445/485 - Loss:  1.793, Seconds: 14.61\n",
      "Epoch   9/50 Batch  450/485 - Loss:  1.831, Seconds: 13.90\n",
      "Epoch   9/50 Batch  455/485 - Loss:  1.734, Seconds: 15.71\n",
      "Epoch   9/50 Batch  460/485 - Loss:  2.001, Seconds: 14.79\n",
      "Epoch   9/50 Batch  465/485 - Loss:  1.954, Seconds: 17.90\n",
      "Epoch   9/50 Batch  470/485 - Loss:  1.985, Seconds: 18.60\n",
      "Epoch   9/50 Batch  475/485 - Loss:  1.949, Seconds: 20.12\n",
      "Epoch   9/50 Batch  480/485 - Loss:  2.018, Seconds: 20.10\n",
      "Average loss for this update: 1.698\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch    5/485 - Loss:  1.710, Seconds: 6.37\n",
      "Epoch  10/50 Batch   10/485 - Loss:  1.257, Seconds: 7.31\n",
      "Epoch  10/50 Batch   15/485 - Loss:  1.240, Seconds: 5.98\n",
      "Epoch  10/50 Batch   20/485 - Loss:  1.172, Seconds: 7.83\n",
      "Epoch  10/50 Batch   25/485 - Loss:  1.293, Seconds: 5.42\n",
      "Epoch  10/50 Batch   30/485 - Loss:  1.202, Seconds: 7.73\n",
      "Epoch  10/50 Batch   35/485 - Loss:  1.294, Seconds: 7.20\n",
      "Epoch  10/50 Batch   40/485 - Loss:  1.331, Seconds: 5.81\n",
      "Epoch  10/50 Batch   45/485 - Loss:  1.275, Seconds: 7.59\n",
      "Epoch  10/50 Batch   50/485 - Loss:  1.254, Seconds: 7.78\n",
      "Epoch  10/50 Batch   55/485 - Loss:  1.225, Seconds: 5.04\n",
      "Epoch  10/50 Batch   60/485 - Loss:  1.252, Seconds: 5.30\n",
      "Epoch  10/50 Batch   65/485 - Loss:  1.233, Seconds: 6.17\n",
      "Epoch  10/50 Batch   70/485 - Loss:  1.152, Seconds: 7.10\n",
      "Epoch  10/50 Batch   75/485 - Loss:  1.382, Seconds: 7.44\n",
      "Epoch  10/50 Batch   80/485 - Loss:  1.290, Seconds: 7.42\n",
      "Epoch  10/50 Batch   85/485 - Loss:  1.234, Seconds: 7.29\n",
      "Epoch  10/50 Batch   90/485 - Loss:  1.252, Seconds: 6.76\n",
      "Epoch  10/50 Batch   95/485 - Loss:  1.245, Seconds: 7.23\n",
      "Epoch  10/50 Batch  100/485 - Loss:  1.176, Seconds: 9.09\n",
      "Epoch  10/50 Batch  105/485 - Loss:  1.215, Seconds: 9.18\n",
      "Epoch  10/50 Batch  110/485 - Loss:  1.224, Seconds: 8.01\n",
      "Epoch  10/50 Batch  115/485 - Loss:  1.348, Seconds: 8.46\n",
      "Epoch  10/50 Batch  120/485 - Loss:  1.217, Seconds: 8.29\n",
      "Epoch  10/50 Batch  125/485 - Loss:  1.312, Seconds: 7.39\n",
      "Epoch  10/50 Batch  130/485 - Loss:  1.227, Seconds: 6.71\n",
      "Epoch  10/50 Batch  135/485 - Loss:  1.275, Seconds: 8.64\n",
      "Epoch  10/50 Batch  140/485 - Loss:  1.350, Seconds: 7.63\n",
      "Epoch  10/50 Batch  145/485 - Loss:  1.252, Seconds: 8.23\n",
      "Epoch  10/50 Batch  150/485 - Loss:  1.260, Seconds: 7.68\n",
      "Epoch  10/50 Batch  155/485 - Loss:  1.314, Seconds: 8.30\n",
      "Epoch  10/50 Batch  160/485 - Loss:  1.238, Seconds: 6.93\n",
      "Average loss for this update: 1.272\n",
      "New Record!\n",
      "Epoch  10/50 Batch  165/485 - Loss:  1.314, Seconds: 7.51\n",
      "Epoch  10/50 Batch  170/485 - Loss:  1.341, Seconds: 8.18\n",
      "Epoch  10/50 Batch  175/485 - Loss:  1.206, Seconds: 8.81\n",
      "Epoch  10/50 Batch  180/485 - Loss:  1.273, Seconds: 8.65\n",
      "Epoch  10/50 Batch  185/485 - Loss:  1.341, Seconds: 7.91\n",
      "Epoch  10/50 Batch  190/485 - Loss:  1.268, Seconds: 9.28\n",
      "Epoch  10/50 Batch  195/485 - Loss:  1.292, Seconds: 7.75\n",
      "Epoch  10/50 Batch  200/485 - Loss:  1.423, Seconds: 7.72\n",
      "Epoch  10/50 Batch  205/485 - Loss:  1.376, Seconds: 7.23\n",
      "Epoch  10/50 Batch  210/485 - Loss:  1.367, Seconds: 9.60\n",
      "Epoch  10/50 Batch  215/485 - Loss:  1.348, Seconds: 8.62\n",
      "Epoch  10/50 Batch  220/485 - Loss:  1.350, Seconds: 10.22\n",
      "Epoch  10/50 Batch  225/485 - Loss:  1.367, Seconds: 7.70\n",
      "Epoch  10/50 Batch  230/485 - Loss:  1.350, Seconds: 9.29\n",
      "Epoch  10/50 Batch  235/485 - Loss:  1.341, Seconds: 9.38\n",
      "Epoch  10/50 Batch  240/485 - Loss:  1.451, Seconds: 7.08\n",
      "Epoch  10/50 Batch  245/485 - Loss:  1.352, Seconds: 8.73\n",
      "Epoch  10/50 Batch  250/485 - Loss:  1.341, Seconds: 10.28\n",
      "Epoch  10/50 Batch  255/485 - Loss:  1.411, Seconds: 10.14\n",
      "Epoch  10/50 Batch  260/485 - Loss:  1.440, Seconds: 9.50\n",
      "Epoch  10/50 Batch  265/485 - Loss:  1.396, Seconds: 9.01\n",
      "Epoch  10/50 Batch  270/485 - Loss:  1.447, Seconds: 9.98\n",
      "Epoch  10/50 Batch  275/485 - Loss:  1.374, Seconds: 9.92\n",
      "Epoch  10/50 Batch  280/485 - Loss:  1.355, Seconds: 10.06\n",
      "Epoch  10/50 Batch  285/485 - Loss:  1.527, Seconds: 9.00\n",
      "Epoch  10/50 Batch  290/485 - Loss:  1.389, Seconds: 10.19\n",
      "Epoch  10/50 Batch  295/485 - Loss:  1.272, Seconds: 10.58\n",
      "Epoch  10/50 Batch  300/485 - Loss:  1.327, Seconds: 10.04\n",
      "Epoch  10/50 Batch  305/485 - Loss:  1.454, Seconds: 10.01\n",
      "Epoch  10/50 Batch  310/485 - Loss:  1.477, Seconds: 8.66\n",
      "Epoch  10/50 Batch  315/485 - Loss:  1.488, Seconds: 9.49\n",
      "Epoch  10/50 Batch  320/485 - Loss:  1.491, Seconds: 10.53\n",
      "Average loss for this update: 1.373\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch  325/485 - Loss:  1.583, Seconds: 8.99\n",
      "Epoch  10/50 Batch  330/485 - Loss:  1.346, Seconds: 11.35\n",
      "Epoch  10/50 Batch  335/485 - Loss:  1.542, Seconds: 10.82\n",
      "Epoch  10/50 Batch  340/485 - Loss:  1.486, Seconds: 9.86\n",
      "Epoch  10/50 Batch  345/485 - Loss:  1.509, Seconds: 10.01\n",
      "Epoch  10/50 Batch  350/485 - Loss:  1.625, Seconds: 10.13\n",
      "Epoch  10/50 Batch  355/485 - Loss:  1.479, Seconds: 10.92\n",
      "Epoch  10/50 Batch  360/485 - Loss:  1.466, Seconds: 11.76\n",
      "Epoch  10/50 Batch  365/485 - Loss:  1.530, Seconds: 12.11\n",
      "Epoch  10/50 Batch  370/485 - Loss:  1.589, Seconds: 11.51\n",
      "Epoch  10/50 Batch  375/485 - Loss:  1.621, Seconds: 11.08\n",
      "Epoch  10/50 Batch  380/485 - Loss:  1.576, Seconds: 11.96\n",
      "Epoch  10/50 Batch  385/485 - Loss:  1.581, Seconds: 10.86\n",
      "Epoch  10/50 Batch  390/485 - Loss:  1.464, Seconds: 12.30\n",
      "Epoch  10/50 Batch  395/485 - Loss:  1.647, Seconds: 11.09\n",
      "Epoch  10/50 Batch  400/485 - Loss:  1.583, Seconds: 12.20\n",
      "Epoch  10/50 Batch  405/485 - Loss:  1.570, Seconds: 12.07\n",
      "Epoch  10/50 Batch  410/485 - Loss:  1.575, Seconds: 12.91\n",
      "Epoch  10/50 Batch  415/485 - Loss:  1.684, Seconds: 13.71\n",
      "Epoch  10/50 Batch  420/485 - Loss:  1.654, Seconds: 14.06\n",
      "Epoch  10/50 Batch  425/485 - Loss:  1.575, Seconds: 12.88\n",
      "Epoch  10/50 Batch  430/485 - Loss:  1.662, Seconds: 13.23\n",
      "Epoch  10/50 Batch  435/485 - Loss:  1.752, Seconds: 13.58\n",
      "Epoch  10/50 Batch  440/485 - Loss:  1.726, Seconds: 14.09\n",
      "Epoch  10/50 Batch  445/485 - Loss:  1.728, Seconds: 14.51\n",
      "Epoch  10/50 Batch  450/485 - Loss:  1.756, Seconds: 13.89\n",
      "Epoch  10/50 Batch  455/485 - Loss:  1.675, Seconds: 15.42\n",
      "Epoch  10/50 Batch  460/485 - Loss:  1.924, Seconds: 14.76\n",
      "Epoch  10/50 Batch  465/485 - Loss:  1.865, Seconds: 17.06\n",
      "Epoch  10/50 Batch  470/485 - Loss:  1.879, Seconds: 18.34\n",
      "Epoch  10/50 Batch  475/485 - Loss:  1.880, Seconds: 19.97\n",
      "Epoch  10/50 Batch  480/485 - Loss:  1.935, Seconds: 19.70\n",
      "Average loss for this update: 1.64\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/50 Batch    5/485 - Loss:  1.694, Seconds: 6.57\n",
      "Epoch  11/50 Batch   10/485 - Loss:  1.227, Seconds: 7.26\n",
      "Epoch  11/50 Batch   15/485 - Loss:  1.189, Seconds: 6.00\n",
      "Epoch  11/50 Batch   20/485 - Loss:  1.144, Seconds: 8.23\n",
      "Epoch  11/50 Batch   25/485 - Loss:  1.261, Seconds: 5.59\n",
      "Epoch  11/50 Batch   30/485 - Loss:  1.155, Seconds: 7.98\n",
      "Epoch  11/50 Batch   35/485 - Loss:  1.230, Seconds: 7.33\n",
      "Epoch  11/50 Batch   40/485 - Loss:  1.267, Seconds: 5.65\n",
      "Epoch  11/50 Batch   45/485 - Loss:  1.221, Seconds: 7.18\n",
      "Epoch  11/50 Batch   50/485 - Loss:  1.204, Seconds: 7.89\n",
      "Epoch  11/50 Batch   55/485 - Loss:  1.181, Seconds: 4.95\n",
      "Epoch  11/50 Batch   60/485 - Loss:  1.198, Seconds: 5.10\n",
      "Epoch  11/50 Batch   65/485 - Loss:  1.168, Seconds: 6.22\n",
      "Epoch  11/50 Batch   70/485 - Loss:  1.118, Seconds: 7.02\n",
      "Epoch  11/50 Batch   75/485 - Loss:  1.275, Seconds: 7.48\n",
      "Epoch  11/50 Batch   80/485 - Loss:  1.196, Seconds: 7.37\n",
      "Epoch  11/50 Batch   85/485 - Loss:  1.158, Seconds: 6.98\n",
      "Epoch  11/50 Batch   90/485 - Loss:  1.222, Seconds: 6.36\n",
      "Epoch  11/50 Batch   95/485 - Loss:  1.163, Seconds: 6.96\n",
      "Epoch  11/50 Batch  100/485 - Loss:  1.125, Seconds: 8.64\n",
      "Epoch  11/50 Batch  105/485 - Loss:  1.160, Seconds: 8.74\n",
      "Epoch  11/50 Batch  110/485 - Loss:  1.166, Seconds: 7.66\n",
      "Epoch  11/50 Batch  115/485 - Loss:  1.270, Seconds: 8.20\n",
      "Epoch  11/50 Batch  120/485 - Loss:  1.128, Seconds: 7.83\n",
      "Epoch  11/50 Batch  125/485 - Loss:  1.231, Seconds: 7.19\n",
      "Epoch  11/50 Batch  130/485 - Loss:  1.176, Seconds: 6.55\n",
      "Epoch  11/50 Batch  135/485 - Loss:  1.215, Seconds: 8.43\n",
      "Epoch  11/50 Batch  140/485 - Loss:  1.325, Seconds: 7.40\n",
      "Epoch  11/50 Batch  145/485 - Loss:  1.190, Seconds: 7.83\n",
      "Epoch  11/50 Batch  150/485 - Loss:  1.208, Seconds: 7.35\n",
      "Epoch  11/50 Batch  155/485 - Loss:  1.271, Seconds: 8.00\n",
      "Epoch  11/50 Batch  160/485 - Loss:  1.194, Seconds: 6.66\n",
      "Average loss for this update: 1.217\n",
      "New Record!\n",
      "Epoch  11/50 Batch  165/485 - Loss:  1.270, Seconds: 7.49\n",
      "Epoch  11/50 Batch  170/485 - Loss:  1.284, Seconds: 8.12\n",
      "Epoch  11/50 Batch  175/485 - Loss:  1.172, Seconds: 8.77\n",
      "Epoch  11/50 Batch  180/485 - Loss:  1.218, Seconds: 8.78\n",
      "Epoch  11/50 Batch  185/485 - Loss:  1.308, Seconds: 7.76\n",
      "Epoch  11/50 Batch  190/485 - Loss:  1.230, Seconds: 9.69\n",
      "Epoch  11/50 Batch  195/485 - Loss:  1.241, Seconds: 7.79\n",
      "Epoch  11/50 Batch  200/485 - Loss:  1.353, Seconds: 7.73\n",
      "Epoch  11/50 Batch  205/485 - Loss:  1.309, Seconds: 7.24\n",
      "Epoch  11/50 Batch  210/485 - Loss:  1.303, Seconds: 9.58\n",
      "Epoch  11/50 Batch  215/485 - Loss:  1.294, Seconds: 8.50\n",
      "Epoch  11/50 Batch  220/485 - Loss:  1.322, Seconds: 9.46\n",
      "Epoch  11/50 Batch  225/485 - Loss:  1.318, Seconds: 7.51\n",
      "Epoch  11/50 Batch  230/485 - Loss:  1.313, Seconds: 9.21\n",
      "Epoch  11/50 Batch  235/485 - Loss:  1.316, Seconds: 9.16\n",
      "Epoch  11/50 Batch  240/485 - Loss:  1.384, Seconds: 7.13\n",
      "Epoch  11/50 Batch  245/485 - Loss:  1.293, Seconds: 8.60\n",
      "Epoch  11/50 Batch  250/485 - Loss:  1.292, Seconds: 10.42\n",
      "Epoch  11/50 Batch  255/485 - Loss:  1.380, Seconds: 10.08\n",
      "Epoch  11/50 Batch  260/485 - Loss:  1.381, Seconds: 9.80\n",
      "Epoch  11/50 Batch  265/485 - Loss:  1.322, Seconds: 7.74\n",
      "Epoch  11/50 Batch  270/485 - Loss:  1.378, Seconds: 8.97\n",
      "Epoch  11/50 Batch  275/485 - Loss:  1.348, Seconds: 9.66\n",
      "Epoch  11/50 Batch  280/485 - Loss:  1.310, Seconds: 9.77\n",
      "Epoch  11/50 Batch  285/485 - Loss:  1.452, Seconds: 9.20\n",
      "Epoch  11/50 Batch  290/485 - Loss:  1.334, Seconds: 10.34\n",
      "Epoch  11/50 Batch  295/485 - Loss:  1.248, Seconds: 10.63\n",
      "Epoch  11/50 Batch  300/485 - Loss:  1.289, Seconds: 10.55\n",
      "Epoch  11/50 Batch  305/485 - Loss:  1.403, Seconds: 9.98\n",
      "Epoch  11/50 Batch  310/485 - Loss:  1.439, Seconds: 8.80\n",
      "Epoch  11/50 Batch  315/485 - Loss:  1.411, Seconds: 9.51\n",
      "Epoch  11/50 Batch  320/485 - Loss:  1.434, Seconds: 10.11\n",
      "Average loss for this update: 1.323\n",
      "No Improvement.\n",
      "Epoch  11/50 Batch  325/485 - Loss:  1.524, Seconds: 8.31\n",
      "Epoch  11/50 Batch  330/485 - Loss:  1.293, Seconds: 11.02\n",
      "Epoch  11/50 Batch  335/485 - Loss:  1.491, Seconds: 10.52\n",
      "Epoch  11/50 Batch  340/485 - Loss:  1.446, Seconds: 10.19\n",
      "Epoch  11/50 Batch  345/485 - Loss:  1.494, Seconds: 10.14\n",
      "Epoch  11/50 Batch  350/485 - Loss:  1.545, Seconds: 10.36\n",
      "Epoch  11/50 Batch  355/485 - Loss:  1.391, Seconds: 11.05\n",
      "Epoch  11/50 Batch  360/485 - Loss:  1.437, Seconds: 12.07\n",
      "Epoch  11/50 Batch  365/485 - Loss:  1.454, Seconds: 11.84\n",
      "Epoch  11/50 Batch  370/485 - Loss:  1.525, Seconds: 11.73\n",
      "Epoch  11/50 Batch  375/485 - Loss:  1.560, Seconds: 10.64\n",
      "Epoch  11/50 Batch  380/485 - Loss:  1.539, Seconds: 11.50\n",
      "Epoch  11/50 Batch  385/485 - Loss:  1.536, Seconds: 10.98\n",
      "Epoch  11/50 Batch  390/485 - Loss:  1.427, Seconds: 11.70\n",
      "Epoch  11/50 Batch  395/485 - Loss:  1.611, Seconds: 11.45\n",
      "Epoch  11/50 Batch  400/485 - Loss:  1.550, Seconds: 12.51\n",
      "Epoch  11/50 Batch  405/485 - Loss:  1.524, Seconds: 12.10\n",
      "Epoch  11/50 Batch  410/485 - Loss:  1.530, Seconds: 12.43\n",
      "Epoch  11/50 Batch  415/485 - Loss:  1.634, Seconds: 13.38\n",
      "Epoch  11/50 Batch  420/485 - Loss:  1.610, Seconds: 13.43\n",
      "Epoch  11/50 Batch  425/485 - Loss:  1.533, Seconds: 12.57\n",
      "Epoch  11/50 Batch  430/485 - Loss:  1.636, Seconds: 13.90\n",
      "Epoch  11/50 Batch  435/485 - Loss:  1.691, Seconds: 13.73\n",
      "Epoch  11/50 Batch  440/485 - Loss:  1.685, Seconds: 14.97\n",
      "Epoch  11/50 Batch  445/485 - Loss:  1.679, Seconds: 14.67\n",
      "Epoch  11/50 Batch  450/485 - Loss:  1.706, Seconds: 14.46\n",
      "Epoch  11/50 Batch  455/485 - Loss:  1.614, Seconds: 15.52\n",
      "Epoch  11/50 Batch  460/485 - Loss:  1.869, Seconds: 15.03\n",
      "Epoch  11/50 Batch  465/485 - Loss:  1.817, Seconds: 17.25\n",
      "Epoch  11/50 Batch  470/485 - Loss:  1.833, Seconds: 18.37\n",
      "Epoch  11/50 Batch  475/485 - Loss:  1.833, Seconds: 19.76\n",
      "Epoch  11/50 Batch  480/485 - Loss:  1.881, Seconds: 19.87\n",
      "Average loss for this update: 1.591\n",
      "No Improvement.\n",
      "Epoch  12/50 Batch    5/485 - Loss:  1.660, Seconds: 6.48\n",
      "Epoch  12/50 Batch   10/485 - Loss:  1.182, Seconds: 7.23\n",
      "Epoch  12/50 Batch   15/485 - Loss:  1.161, Seconds: 5.95\n",
      "Epoch  12/50 Batch   20/485 - Loss:  1.120, Seconds: 7.84\n",
      "Epoch  12/50 Batch   25/485 - Loss:  1.211, Seconds: 5.50\n",
      "Epoch  12/50 Batch   30/485 - Loss:  1.170, Seconds: 7.98\n",
      "Epoch  12/50 Batch   35/485 - Loss:  1.198, Seconds: 7.23\n",
      "Epoch  12/50 Batch   40/485 - Loss:  1.236, Seconds: 5.43\n",
      "Epoch  12/50 Batch   45/485 - Loss:  1.209, Seconds: 7.16\n",
      "Epoch  12/50 Batch   50/485 - Loss:  1.173, Seconds: 7.74\n",
      "Epoch  12/50 Batch   55/485 - Loss:  1.142, Seconds: 4.97\n",
      "Epoch  12/50 Batch   60/485 - Loss:  1.177, Seconds: 4.92\n",
      "Epoch  12/50 Batch   65/485 - Loss:  1.171, Seconds: 6.12\n",
      "Epoch  12/50 Batch   70/485 - Loss:  1.113, Seconds: 7.07\n",
      "Epoch  12/50 Batch   75/485 - Loss:  1.266, Seconds: 7.52\n",
      "Epoch  12/50 Batch   80/485 - Loss:  1.131, Seconds: 7.39\n",
      "Epoch  12/50 Batch   85/485 - Loss:  1.143, Seconds: 7.09\n",
      "Epoch  12/50 Batch   90/485 - Loss:  1.197, Seconds: 6.50\n",
      "Epoch  12/50 Batch   95/485 - Loss:  1.180, Seconds: 7.14\n",
      "Epoch  12/50 Batch  100/485 - Loss:  1.096, Seconds: 9.55\n",
      "Epoch  12/50 Batch  105/485 - Loss:  1.127, Seconds: 9.14\n",
      "Epoch  12/50 Batch  110/485 - Loss:  1.139, Seconds: 8.26\n",
      "Epoch  12/50 Batch  115/485 - Loss:  1.225, Seconds: 8.50\n",
      "Epoch  12/50 Batch  120/485 - Loss:  1.121, Seconds: 7.83\n",
      "Epoch  12/50 Batch  125/485 - Loss:  1.198, Seconds: 7.36\n",
      "Epoch  12/50 Batch  130/485 - Loss:  1.153, Seconds: 6.78\n",
      "Epoch  12/50 Batch  135/485 - Loss:  1.186, Seconds: 8.44\n",
      "Epoch  12/50 Batch  140/485 - Loss:  1.252, Seconds: 7.33\n",
      "Epoch  12/50 Batch  145/485 - Loss:  1.177, Seconds: 8.14\n",
      "Epoch  12/50 Batch  150/485 - Loss:  1.141, Seconds: 7.50\n",
      "Epoch  12/50 Batch  155/485 - Loss:  1.215, Seconds: 8.11\n",
      "Epoch  12/50 Batch  160/485 - Loss:  1.150, Seconds: 6.84\n",
      "Average loss for this update: 1.188\n",
      "New Record!\n",
      "Epoch  12/50 Batch  165/485 - Loss:  1.233, Seconds: 7.69\n",
      "Epoch  12/50 Batch  170/485 - Loss:  1.234, Seconds: 8.20\n",
      "Epoch  12/50 Batch  175/485 - Loss:  1.114, Seconds: 9.06\n",
      "Epoch  12/50 Batch  180/485 - Loss:  1.168, Seconds: 9.29\n",
      "Epoch  12/50 Batch  185/485 - Loss:  1.239, Seconds: 8.01\n",
      "Epoch  12/50 Batch  190/485 - Loss:  1.176, Seconds: 9.47\n",
      "Epoch  12/50 Batch  195/485 - Loss:  1.206, Seconds: 7.79\n",
      "Epoch  12/50 Batch  200/485 - Loss:  1.326, Seconds: 7.72\n",
      "Epoch  12/50 Batch  205/485 - Loss:  1.278, Seconds: 7.25\n",
      "Epoch  12/50 Batch  210/485 - Loss:  1.269, Seconds: 9.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/50 Batch  215/485 - Loss:  1.271, Seconds: 8.54\n",
      "Epoch  12/50 Batch  220/485 - Loss:  1.254, Seconds: 9.22\n",
      "Epoch  12/50 Batch  225/485 - Loss:  1.265, Seconds: 7.32\n",
      "Epoch  12/50 Batch  230/485 - Loss:  1.262, Seconds: 9.22\n",
      "Epoch  12/50 Batch  235/485 - Loss:  1.270, Seconds: 9.21\n",
      "Epoch  12/50 Batch  240/485 - Loss:  1.349, Seconds: 6.84\n",
      "Epoch  12/50 Batch  245/485 - Loss:  1.261, Seconds: 8.77\n",
      "Epoch  12/50 Batch  250/485 - Loss:  1.263, Seconds: 10.08\n",
      "Epoch  12/50 Batch  255/485 - Loss:  1.336, Seconds: 10.02\n",
      "Epoch  12/50 Batch  260/485 - Loss:  1.321, Seconds: 9.54\n",
      "Epoch  12/50 Batch  265/485 - Loss:  1.294, Seconds: 7.68\n",
      "Epoch  12/50 Batch  270/485 - Loss:  1.349, Seconds: 8.96\n",
      "Epoch  12/50 Batch  275/485 - Loss:  1.286, Seconds: 9.67\n",
      "Epoch  12/50 Batch  280/485 - Loss:  1.281, Seconds: 9.71\n",
      "Epoch  12/50 Batch  285/485 - Loss:  1.418, Seconds: 9.07\n",
      "Epoch  12/50 Batch  290/485 - Loss:  1.309, Seconds: 9.84\n",
      "Epoch  12/50 Batch  295/485 - Loss:  1.190, Seconds: 10.47\n",
      "Epoch  12/50 Batch  300/485 - Loss:  1.247, Seconds: 9.96\n",
      "Epoch  12/50 Batch  305/485 - Loss:  1.350, Seconds: 10.23\n",
      "Epoch  12/50 Batch  310/485 - Loss:  1.390, Seconds: 9.06\n",
      "Epoch  12/50 Batch  315/485 - Loss:  1.400, Seconds: 9.56\n",
      "Epoch  12/50 Batch  320/485 - Loss:  1.404, Seconds: 10.66\n",
      "Average loss for this update: 1.282\n",
      "No Improvement.\n",
      "Epoch  12/50 Batch  325/485 - Loss:  1.471, Seconds: 8.55\n",
      "Epoch  12/50 Batch  330/485 - Loss:  1.269, Seconds: 11.14\n",
      "Epoch  12/50 Batch  335/485 - Loss:  1.462, Seconds: 10.79\n",
      "Epoch  12/50 Batch  340/485 - Loss:  1.403, Seconds: 10.47\n",
      "Epoch  12/50 Batch  345/485 - Loss:  1.437, Seconds: 10.22\n",
      "Epoch  12/50 Batch  350/485 - Loss:  1.527, Seconds: 10.19\n",
      "Epoch  12/50 Batch  355/485 - Loss:  1.372, Seconds: 10.87\n",
      "Epoch  12/50 Batch  360/485 - Loss:  1.390, Seconds: 12.03\n",
      "Epoch  12/50 Batch  365/485 - Loss:  1.411, Seconds: 12.32\n",
      "Epoch  12/50 Batch  370/485 - Loss:  1.487, Seconds: 11.16\n",
      "Epoch  12/50 Batch  375/485 - Loss:  1.511, Seconds: 11.23\n",
      "Epoch  12/50 Batch  380/485 - Loss:  1.489, Seconds: 11.83\n",
      "Epoch  12/50 Batch  385/485 - Loss:  1.505, Seconds: 11.23\n",
      "Epoch  12/50 Batch  390/485 - Loss:  1.388, Seconds: 12.24\n",
      "Epoch  12/50 Batch  395/485 - Loss:  1.553, Seconds: 11.78\n",
      "Epoch  12/50 Batch  400/485 - Loss:  1.512, Seconds: 12.43\n",
      "Epoch  12/50 Batch  405/485 - Loss:  1.490, Seconds: 12.75\n",
      "Epoch  12/50 Batch  410/485 - Loss:  1.517, Seconds: 12.69\n",
      "Epoch  12/50 Batch  415/485 - Loss:  1.620, Seconds: 13.70\n",
      "Epoch  12/50 Batch  420/485 - Loss:  1.567, Seconds: 13.61\n",
      "Epoch  12/50 Batch  425/485 - Loss:  1.491, Seconds: 12.85\n",
      "Epoch  12/50 Batch  430/485 - Loss:  1.590, Seconds: 13.81\n",
      "Epoch  12/50 Batch  435/485 - Loss:  1.667, Seconds: 13.80\n",
      "Epoch  12/50 Batch  440/485 - Loss:  1.623, Seconds: 14.67\n",
      "Epoch  12/50 Batch  445/485 - Loss:  1.611, Seconds: 15.33\n",
      "Epoch  12/50 Batch  450/485 - Loss:  1.682, Seconds: 14.04\n",
      "Epoch  12/50 Batch  455/485 - Loss:  1.578, Seconds: 15.23\n",
      "Epoch  12/50 Batch  460/485 - Loss:  1.848, Seconds: 15.37\n",
      "Epoch  12/50 Batch  465/485 - Loss:  1.749, Seconds: 17.54\n",
      "Epoch  12/50 Batch  470/485 - Loss:  1.789, Seconds: 18.50\n",
      "Epoch  12/50 Batch  475/485 - Loss:  1.774, Seconds: 19.61\n",
      "Epoch  12/50 Batch  480/485 - Loss:  1.839, Seconds: 19.87\n",
      "Average loss for this update: 1.551\n",
      "No Improvement.\n",
      "Epoch  13/50 Batch    5/485 - Loss:  1.619, Seconds: 6.53\n",
      "Epoch  13/50 Batch   10/485 - Loss:  1.143, Seconds: 7.12\n",
      "Epoch  13/50 Batch   15/485 - Loss:  1.133, Seconds: 6.03\n",
      "Epoch  13/50 Batch   20/485 - Loss:  1.091, Seconds: 8.11\n",
      "Epoch  13/50 Batch   25/485 - Loss:  1.182, Seconds: 5.54\n",
      "Epoch  13/50 Batch   30/485 - Loss:  1.138, Seconds: 8.05\n",
      "Epoch  13/50 Batch   35/485 - Loss:  1.168, Seconds: 7.36\n",
      "Epoch  13/50 Batch   40/485 - Loss:  1.187, Seconds: 5.51\n",
      "Epoch  13/50 Batch   45/485 - Loss:  1.185, Seconds: 7.20\n",
      "Epoch  13/50 Batch   50/485 - Loss:  1.153, Seconds: 7.81\n",
      "Epoch  13/50 Batch   55/485 - Loss:  1.119, Seconds: 4.94\n",
      "Epoch  13/50 Batch   60/485 - Loss:  1.140, Seconds: 4.96\n",
      "Epoch  13/50 Batch   65/485 - Loss:  1.133, Seconds: 6.28\n",
      "Epoch  13/50 Batch   70/485 - Loss:  1.028, Seconds: 6.74\n",
      "Epoch  13/50 Batch   75/485 - Loss:  1.248, Seconds: 7.26\n",
      "Epoch  13/50 Batch   80/485 - Loss:  1.130, Seconds: 7.47\n",
      "Epoch  13/50 Batch   85/485 - Loss:  1.098, Seconds: 7.00\n",
      "Epoch  13/50 Batch   90/485 - Loss:  1.137, Seconds: 6.33\n",
      "Epoch  13/50 Batch   95/485 - Loss:  1.113, Seconds: 6.95\n",
      "Epoch  13/50 Batch  100/485 - Loss:  1.085, Seconds: 8.81\n",
      "Epoch  13/50 Batch  105/485 - Loss:  1.129, Seconds: 8.87\n",
      "Epoch  13/50 Batch  110/485 - Loss:  1.095, Seconds: 7.63\n",
      "Epoch  13/50 Batch  115/485 - Loss:  1.171, Seconds: 8.21\n",
      "Epoch  13/50 Batch  120/485 - Loss:  1.070, Seconds: 7.75\n",
      "Epoch  13/50 Batch  125/485 - Loss:  1.179, Seconds: 7.15\n",
      "Epoch  13/50 Batch  130/485 - Loss:  1.098, Seconds: 6.75\n",
      "Epoch  13/50 Batch  135/485 - Loss:  1.180, Seconds: 8.64\n",
      "Epoch  13/50 Batch  140/485 - Loss:  1.215, Seconds: 7.25\n",
      "Epoch  13/50 Batch  145/485 - Loss:  1.147, Seconds: 7.89\n",
      "Epoch  13/50 Batch  150/485 - Loss:  1.132, Seconds: 7.31\n",
      "Epoch  13/50 Batch  155/485 - Loss:  1.202, Seconds: 8.01\n",
      "Epoch  13/50 Batch  160/485 - Loss:  1.133, Seconds: 6.74\n",
      "Average loss for this update: 1.156\n",
      "New Record!\n",
      "Epoch  13/50 Batch  165/485 - Loss:  1.189, Seconds: 7.60\n",
      "Epoch  13/50 Batch  170/485 - Loss:  1.211, Seconds: 8.06\n",
      "Epoch  13/50 Batch  175/485 - Loss:  1.087, Seconds: 8.90\n",
      "Epoch  13/50 Batch  180/485 - Loss:  1.144, Seconds: 8.93\n",
      "Epoch  13/50 Batch  185/485 - Loss:  1.212, Seconds: 7.72\n",
      "Epoch  13/50 Batch  190/485 - Loss:  1.129, Seconds: 9.41\n",
      "Epoch  13/50 Batch  195/485 - Loss:  1.186, Seconds: 7.63\n",
      "Epoch  13/50 Batch  200/485 - Loss:  1.300, Seconds: 7.60\n",
      "Epoch  13/50 Batch  205/485 - Loss:  1.269, Seconds: 7.16\n",
      "Epoch  13/50 Batch  210/485 - Loss:  1.214, Seconds: 9.59\n",
      "Epoch  13/50 Batch  215/485 - Loss:  1.230, Seconds: 8.48\n",
      "Epoch  13/50 Batch  220/485 - Loss:  1.241, Seconds: 8.91\n",
      "Epoch  13/50 Batch  225/485 - Loss:  1.223, Seconds: 7.24\n",
      "Epoch  13/50 Batch  230/485 - Loss:  1.225, Seconds: 9.05\n",
      "Epoch  13/50 Batch  235/485 - Loss:  1.220, Seconds: 9.12\n",
      "Epoch  13/50 Batch  240/485 - Loss:  1.278, Seconds: 6.87\n",
      "Epoch  13/50 Batch  245/485 - Loss:  1.214, Seconds: 8.60\n",
      "Epoch  13/50 Batch  250/485 - Loss:  1.243, Seconds: 9.83\n",
      "Epoch  13/50 Batch  255/485 - Loss:  1.287, Seconds: 9.83\n",
      "Epoch  13/50 Batch  260/485 - Loss:  1.285, Seconds: 9.41\n",
      "Epoch  13/50 Batch  265/485 - Loss:  1.241, Seconds: 7.64\n",
      "Epoch  13/50 Batch  270/485 - Loss:  1.338, Seconds: 8.77\n",
      "Epoch  13/50 Batch  275/485 - Loss:  1.282, Seconds: 9.55\n",
      "Epoch  13/50 Batch  280/485 - Loss:  1.236, Seconds: 9.76\n",
      "Epoch  13/50 Batch  285/485 - Loss:  1.398, Seconds: 9.38\n",
      "Epoch  13/50 Batch  290/485 - Loss:  1.279, Seconds: 9.99\n",
      "Epoch  13/50 Batch  295/485 - Loss:  1.187, Seconds: 10.56\n",
      "Epoch  13/50 Batch  300/485 - Loss:  1.226, Seconds: 10.23\n",
      "Epoch  13/50 Batch  305/485 - Loss:  1.354, Seconds: 10.25\n",
      "Epoch  13/50 Batch  310/485 - Loss:  1.371, Seconds: 9.33\n",
      "Epoch  13/50 Batch  315/485 - Loss:  1.345, Seconds: 9.70\n",
      "Epoch  13/50 Batch  320/485 - Loss:  1.340, Seconds: 10.81\n",
      "Average loss for this update: 1.25\n",
      "No Improvement.\n",
      "Epoch  13/50 Batch  325/485 - Loss:  1.437, Seconds: 8.50\n",
      "Epoch  13/50 Batch  330/485 - Loss:  1.238, Seconds: 11.58\n",
      "Epoch  13/50 Batch  335/485 - Loss:  1.423, Seconds: 10.66\n",
      "Epoch  13/50 Batch  340/485 - Loss:  1.374, Seconds: 10.09\n",
      "Epoch  13/50 Batch  345/485 - Loss:  1.403, Seconds: 10.52\n",
      "Epoch  13/50 Batch  350/485 - Loss:  1.452, Seconds: 10.63\n",
      "Epoch  13/50 Batch  355/485 - Loss:  1.347, Seconds: 11.30\n",
      "Epoch  13/50 Batch  360/485 - Loss:  1.362, Seconds: 12.17\n",
      "Epoch  13/50 Batch  365/485 - Loss:  1.372, Seconds: 11.98\n",
      "Epoch  13/50 Batch  370/485 - Loss:  1.413, Seconds: 11.72\n",
      "Epoch  13/50 Batch  375/485 - Loss:  1.458, Seconds: 11.49\n",
      "Epoch  13/50 Batch  380/485 - Loss:  1.464, Seconds: 11.68\n",
      "Epoch  13/50 Batch  385/485 - Loss:  1.470, Seconds: 11.31\n",
      "Epoch  13/50 Batch  390/485 - Loss:  1.316, Seconds: 11.92\n",
      "Epoch  13/50 Batch  395/485 - Loss:  1.505, Seconds: 11.41\n",
      "Epoch  13/50 Batch  400/485 - Loss:  1.474, Seconds: 12.23\n",
      "Epoch  13/50 Batch  405/485 - Loss:  1.445, Seconds: 12.67\n",
      "Epoch  13/50 Batch  410/485 - Loss:  1.451, Seconds: 12.59\n",
      "Epoch  13/50 Batch  415/485 - Loss:  1.558, Seconds: 13.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13/50 Batch  420/485 - Loss:  1.519, Seconds: 14.22\n",
      "Epoch  13/50 Batch  425/485 - Loss:  1.430, Seconds: 12.57\n",
      "Epoch  13/50 Batch  430/485 - Loss:  1.526, Seconds: 13.58\n",
      "Epoch  13/50 Batch  435/485 - Loss:  1.608, Seconds: 13.58\n",
      "Epoch  13/50 Batch  440/485 - Loss:  1.610, Seconds: 14.06\n",
      "Epoch  13/50 Batch  445/485 - Loss:  1.601, Seconds: 14.31\n",
      "Epoch  13/50 Batch  450/485 - Loss:  1.623, Seconds: 14.36\n",
      "Epoch  13/50 Batch  455/485 - Loss:  1.518, Seconds: 16.09\n",
      "Epoch  13/50 Batch  460/485 - Loss:  1.795, Seconds: 16.13\n",
      "Epoch  13/50 Batch  465/485 - Loss:  1.690, Seconds: 18.09\n",
      "Epoch  13/50 Batch  470/485 - Loss:  1.738, Seconds: 18.90\n",
      "Epoch  13/50 Batch  475/485 - Loss:  1.734, Seconds: 20.59\n",
      "Epoch  13/50 Batch  480/485 - Loss:  1.798, Seconds: 20.02\n",
      "Average loss for this update: 1.505\n",
      "No Improvement.\n",
      "Epoch  14/50 Batch    5/485 - Loss:  1.591, Seconds: 6.66\n",
      "Epoch  14/50 Batch   10/485 - Loss:  1.094, Seconds: 7.28\n",
      "Epoch  14/50 Batch   15/485 - Loss:  1.090, Seconds: 6.21\n",
      "Epoch  14/50 Batch   20/485 - Loss:  1.069, Seconds: 8.18\n",
      "Epoch  14/50 Batch   25/485 - Loss:  1.179, Seconds: 5.68\n",
      "Epoch  14/50 Batch   30/485 - Loss:  1.093, Seconds: 8.30\n",
      "Epoch  14/50 Batch   35/485 - Loss:  1.165, Seconds: 7.59\n",
      "Epoch  14/50 Batch   40/485 - Loss:  1.156, Seconds: 5.72\n",
      "Epoch  14/50 Batch   45/485 - Loss:  1.150, Seconds: 7.45\n",
      "Epoch  14/50 Batch   50/485 - Loss:  1.125, Seconds: 8.00\n",
      "Epoch  14/50 Batch   55/485 - Loss:  1.075, Seconds: 5.15\n",
      "Epoch  14/50 Batch   60/485 - Loss:  1.094, Seconds: 5.09\n",
      "Epoch  14/50 Batch   65/485 - Loss:  1.115, Seconds: 6.26\n",
      "Epoch  14/50 Batch   70/485 - Loss:  1.024, Seconds: 7.06\n",
      "Epoch  14/50 Batch   75/485 - Loss:  1.178, Seconds: 7.31\n",
      "Epoch  14/50 Batch   80/485 - Loss:  1.079, Seconds: 7.52\n",
      "Epoch  14/50 Batch   85/485 - Loss:  1.082, Seconds: 7.01\n",
      "Epoch  14/50 Batch   90/485 - Loss:  1.070, Seconds: 6.44\n",
      "Epoch  14/50 Batch   95/485 - Loss:  1.070, Seconds: 7.06\n",
      "Epoch  14/50 Batch  100/485 - Loss:  1.049, Seconds: 9.14\n",
      "Epoch  14/50 Batch  105/485 - Loss:  1.072, Seconds: 8.92\n",
      "Epoch  14/50 Batch  110/485 - Loss:  1.071, Seconds: 7.78\n",
      "Epoch  14/50 Batch  115/485 - Loss:  1.144, Seconds: 8.47\n",
      "Epoch  14/50 Batch  120/485 - Loss:  1.058, Seconds: 7.72\n",
      "Epoch  14/50 Batch  125/485 - Loss:  1.112, Seconds: 7.20\n",
      "Epoch  14/50 Batch  130/485 - Loss:  1.069, Seconds: 6.82\n",
      "Epoch  14/50 Batch  135/485 - Loss:  1.120, Seconds: 8.50\n",
      "Epoch  14/50 Batch  140/485 - Loss:  1.153, Seconds: 7.34\n",
      "Epoch  14/50 Batch  145/485 - Loss:  1.098, Seconds: 8.03\n",
      "Epoch  14/50 Batch  150/485 - Loss:  1.097, Seconds: 7.60\n",
      "Epoch  14/50 Batch  155/485 - Loss:  1.124, Seconds: 8.05\n",
      "Epoch  14/50 Batch  160/485 - Loss:  1.075, Seconds: 6.91\n",
      "Average loss for this update: 1.117\n",
      "New Record!\n",
      "Epoch  14/50 Batch  165/485 - Loss:  1.147, Seconds: 7.79\n",
      "Epoch  14/50 Batch  170/485 - Loss:  1.184, Seconds: 8.54\n",
      "Epoch  14/50 Batch  175/485 - Loss:  1.089, Seconds: 8.99\n",
      "Epoch  14/50 Batch  180/485 - Loss:  1.109, Seconds: 9.03\n",
      "Epoch  14/50 Batch  185/485 - Loss:  1.186, Seconds: 7.83\n",
      "Epoch  14/50 Batch  190/485 - Loss:  1.096, Seconds: 9.75\n",
      "Epoch  14/50 Batch  195/485 - Loss:  1.131, Seconds: 7.82\n",
      "Epoch  14/50 Batch  200/485 - Loss:  1.256, Seconds: 7.79\n",
      "Epoch  14/50 Batch  205/485 - Loss:  1.215, Seconds: 7.28\n",
      "Epoch  14/50 Batch  210/485 - Loss:  1.208, Seconds: 10.13\n",
      "Epoch  14/50 Batch  215/485 - Loss:  1.204, Seconds: 8.51\n",
      "Epoch  14/50 Batch  220/485 - Loss:  1.194, Seconds: 9.37\n",
      "Epoch  14/50 Batch  225/485 - Loss:  1.186, Seconds: 7.39\n",
      "Epoch  14/50 Batch  230/485 - Loss:  1.176, Seconds: 9.22\n",
      "Epoch  14/50 Batch  235/485 - Loss:  1.177, Seconds: 9.35\n",
      "Epoch  14/50 Batch  240/485 - Loss:  1.256, Seconds: 6.94\n",
      "Epoch  14/50 Batch  245/485 - Loss:  1.187, Seconds: 8.75\n",
      "Epoch  14/50 Batch  250/485 - Loss:  1.216, Seconds: 10.05\n",
      "Epoch  14/50 Batch  255/485 - Loss:  1.262, Seconds: 10.01\n",
      "Epoch  14/50 Batch  260/485 - Loss:  1.253, Seconds: 9.58\n",
      "Epoch  14/50 Batch  265/485 - Loss:  1.209, Seconds: 7.81\n",
      "Epoch  14/50 Batch  270/485 - Loss:  1.258, Seconds: 9.15\n",
      "Epoch  14/50 Batch  275/485 - Loss:  1.246, Seconds: 9.62\n",
      "Epoch  14/50 Batch  280/485 - Loss:  1.188, Seconds: 9.98\n",
      "Epoch  14/50 Batch  285/485 - Loss:  1.320, Seconds: 9.13\n",
      "Epoch  14/50 Batch  290/485 - Loss:  1.226, Seconds: 9.89\n",
      "Epoch  14/50 Batch  295/485 - Loss:  1.138, Seconds: 10.65\n",
      "Epoch  14/50 Batch  300/485 - Loss:  1.170, Seconds: 10.18\n",
      "Epoch  14/50 Batch  305/485 - Loss:  1.284, Seconds: 10.59\n",
      "Epoch  14/50 Batch  310/485 - Loss:  1.333, Seconds: 9.51\n",
      "Epoch  14/50 Batch  315/485 - Loss:  1.336, Seconds: 9.66\n",
      "Epoch  14/50 Batch  320/485 - Loss:  1.322, Seconds: 10.43\n",
      "Average loss for this update: 1.211\n",
      "No Improvement.\n",
      "Epoch  14/50 Batch  325/485 - Loss:  1.403, Seconds: 8.52\n",
      "Epoch  14/50 Batch  330/485 - Loss:  1.190, Seconds: 11.29\n",
      "Epoch  14/50 Batch  335/485 - Loss:  1.388, Seconds: 10.76\n",
      "Epoch  14/50 Batch  340/485 - Loss:  1.305, Seconds: 10.15\n",
      "Epoch  14/50 Batch  345/485 - Loss:  1.373, Seconds: 10.30\n",
      "Epoch  14/50 Batch  350/485 - Loss:  1.414, Seconds: 10.16\n",
      "Epoch  14/50 Batch  355/485 - Loss:  1.295, Seconds: 10.90\n",
      "Epoch  14/50 Batch  360/485 - Loss:  1.312, Seconds: 11.83\n",
      "Epoch  14/50 Batch  365/485 - Loss:  1.326, Seconds: 11.80\n",
      "Epoch  14/50 Batch  370/485 - Loss:  1.402, Seconds: 11.29\n",
      "Epoch  14/50 Batch  375/485 - Loss:  1.421, Seconds: 10.62\n",
      "Epoch  14/50 Batch  380/485 - Loss:  1.395, Seconds: 11.74\n",
      "Epoch  14/50 Batch  385/485 - Loss:  1.410, Seconds: 11.18\n",
      "Epoch  14/50 Batch  390/485 - Loss:  1.312, Seconds: 11.96\n",
      "Epoch  14/50 Batch  395/485 - Loss:  1.452, Seconds: 11.35\n",
      "Epoch  14/50 Batch  400/485 - Loss:  1.439, Seconds: 12.45\n",
      "Epoch  14/50 Batch  405/485 - Loss:  1.383, Seconds: 12.38\n",
      "Epoch  14/50 Batch  410/485 - Loss:  1.432, Seconds: 12.54\n",
      "Epoch  14/50 Batch  415/485 - Loss:  1.534, Seconds: 13.57\n",
      "Epoch  14/50 Batch  420/485 - Loss:  1.479, Seconds: 13.66\n",
      "Epoch  14/50 Batch  425/485 - Loss:  1.392, Seconds: 12.45\n",
      "Epoch  14/50 Batch  430/485 - Loss:  1.480, Seconds: 13.74\n",
      "Epoch  14/50 Batch  435/485 - Loss:  1.565, Seconds: 13.72\n",
      "Epoch  14/50 Batch  440/485 - Loss:  1.562, Seconds: 14.18\n",
      "Epoch  14/50 Batch  445/485 - Loss:  1.561, Seconds: 14.46\n",
      "Epoch  14/50 Batch  450/485 - Loss:  1.595, Seconds: 13.95\n",
      "Epoch  14/50 Batch  455/485 - Loss:  1.485, Seconds: 15.37\n",
      "Epoch  14/50 Batch  460/485 - Loss:  1.733, Seconds: 15.10\n",
      "Epoch  14/50 Batch  465/485 - Loss:  1.652, Seconds: 17.64\n",
      "Epoch  14/50 Batch  470/485 - Loss:  1.695, Seconds: 18.33\n",
      "Epoch  14/50 Batch  475/485 - Loss:  1.661, Seconds: 20.12\n",
      "Epoch  14/50 Batch  480/485 - Loss:  1.763, Seconds: 20.06\n",
      "Average loss for this update: 1.463\n",
      "No Improvement.\n",
      "Epoch  15/50 Batch    5/485 - Loss:  1.552, Seconds: 6.75\n",
      "Epoch  15/50 Batch   10/485 - Loss:  1.047, Seconds: 7.43\n",
      "Epoch  15/50 Batch   15/485 - Loss:  1.073, Seconds: 6.12\n",
      "Epoch  15/50 Batch   20/485 - Loss:  1.018, Seconds: 8.11\n",
      "Epoch  15/50 Batch   25/485 - Loss:  1.123, Seconds: 5.75\n",
      "Epoch  15/50 Batch   30/485 - Loss:  1.043, Seconds: 8.29\n",
      "Epoch  15/50 Batch   35/485 - Loss:  1.119, Seconds: 7.45\n",
      "Epoch  15/50 Batch   40/485 - Loss:  1.126, Seconds: 5.54\n",
      "Epoch  15/50 Batch   45/485 - Loss:  1.095, Seconds: 7.50\n",
      "Epoch  15/50 Batch   50/485 - Loss:  1.068, Seconds: 7.92\n",
      "Epoch  15/50 Batch   55/485 - Loss:  1.062, Seconds: 4.99\n",
      "Epoch  15/50 Batch   60/485 - Loss:  1.055, Seconds: 5.19\n",
      "Epoch  15/50 Batch   65/485 - Loss:  1.042, Seconds: 6.27\n",
      "Epoch  15/50 Batch   70/485 - Loss:  1.007, Seconds: 6.91\n",
      "Epoch  15/50 Batch   75/485 - Loss:  1.129, Seconds: 7.41\n",
      "Epoch  15/50 Batch   80/485 - Loss:  1.053, Seconds: 7.43\n",
      "Epoch  15/50 Batch   85/485 - Loss:  1.024, Seconds: 7.08\n",
      "Epoch  15/50 Batch   90/485 - Loss:  1.061, Seconds: 6.53\n",
      "Epoch  15/50 Batch   95/485 - Loss:  1.047, Seconds: 7.12\n",
      "Epoch  15/50 Batch  100/485 - Loss:  1.002, Seconds: 9.10\n",
      "Epoch  15/50 Batch  105/485 - Loss:  1.028, Seconds: 9.02\n",
      "Epoch  15/50 Batch  110/485 - Loss:  1.008, Seconds: 7.97\n",
      "Epoch  15/50 Batch  115/485 - Loss:  1.073, Seconds: 8.57\n",
      "Epoch  15/50 Batch  120/485 - Loss:  0.994, Seconds: 7.80\n",
      "Epoch  15/50 Batch  125/485 - Loss:  1.101, Seconds: 7.42\n",
      "Epoch  15/50 Batch  130/485 - Loss:  1.006, Seconds: 6.72\n",
      "Epoch  15/50 Batch  135/485 - Loss:  1.070, Seconds: 9.06\n",
      "Epoch  15/50 Batch  140/485 - Loss:  1.135, Seconds: 7.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 Batch  145/485 - Loss:  1.099, Seconds: 8.01\n",
      "Epoch  15/50 Batch  150/485 - Loss:  1.055, Seconds: 7.40\n",
      "Epoch  15/50 Batch  155/485 - Loss:  1.102, Seconds: 8.14\n",
      "Epoch  15/50 Batch  160/485 - Loss:  1.061, Seconds: 6.89\n",
      "Average loss for this update: 1.077\n",
      "New Record!\n",
      "Epoch  15/50 Batch  165/485 - Loss:  1.112, Seconds: 7.78\n",
      "Epoch  15/50 Batch  170/485 - Loss:  1.166, Seconds: 8.30\n",
      "Epoch  15/50 Batch  175/485 - Loss:  1.070, Seconds: 9.10\n",
      "Epoch  15/50 Batch  180/485 - Loss:  1.061, Seconds: 9.07\n",
      "Epoch  15/50 Batch  185/485 - Loss:  1.118, Seconds: 7.88\n",
      "Epoch  15/50 Batch  190/485 - Loss:  1.075, Seconds: 9.80\n",
      "Epoch  15/50 Batch  195/485 - Loss:  1.119, Seconds: 7.98\n",
      "Epoch  15/50 Batch  200/485 - Loss:  1.214, Seconds: 7.91\n",
      "Epoch  15/50 Batch  205/485 - Loss:  1.194, Seconds: 7.25\n",
      "Epoch  15/50 Batch  210/485 - Loss:  1.191, Seconds: 9.83\n",
      "Epoch  15/50 Batch  215/485 - Loss:  1.174, Seconds: 8.77\n",
      "Epoch  15/50 Batch  220/485 - Loss:  1.148, Seconds: 9.25\n",
      "Epoch  15/50 Batch  225/485 - Loss:  1.136, Seconds: 7.41\n",
      "Epoch  15/50 Batch  230/485 - Loss:  1.146, Seconds: 9.70\n",
      "Epoch  15/50 Batch  235/485 - Loss:  1.143, Seconds: 9.54\n",
      "Epoch  15/50 Batch  240/485 - Loss:  1.205, Seconds: 6.90\n",
      "Epoch  15/50 Batch  245/485 - Loss:  1.136, Seconds: 8.97\n",
      "Epoch  15/50 Batch  250/485 - Loss:  1.164, Seconds: 10.23\n",
      "Epoch  15/50 Batch  255/485 - Loss:  1.242, Seconds: 10.34\n",
      "Epoch  15/50 Batch  260/485 - Loss:  1.229, Seconds: 9.78\n",
      "Epoch  15/50 Batch  265/485 - Loss:  1.174, Seconds: 7.74\n",
      "Epoch  15/50 Batch  270/485 - Loss:  1.255, Seconds: 9.51\n",
      "Epoch  15/50 Batch  275/485 - Loss:  1.174, Seconds: 9.78\n",
      "Epoch  15/50 Batch  280/485 - Loss:  1.182, Seconds: 9.77\n",
      "Epoch  15/50 Batch  285/485 - Loss:  1.292, Seconds: 9.23\n",
      "Epoch  15/50 Batch  290/485 - Loss:  1.216, Seconds: 10.15\n",
      "Epoch  15/50 Batch  295/485 - Loss:  1.105, Seconds: 10.81\n",
      "Epoch  15/50 Batch  300/485 - Loss:  1.162, Seconds: 10.21\n",
      "Epoch  15/50 Batch  305/485 - Loss:  1.249, Seconds: 10.31\n",
      "Epoch  15/50 Batch  310/485 - Loss:  1.298, Seconds: 8.88\n",
      "Epoch  15/50 Batch  315/485 - Loss:  1.320, Seconds: 9.46\n",
      "Epoch  15/50 Batch  320/485 - Loss:  1.293, Seconds: 10.30\n",
      "Average loss for this update: 1.18\n",
      "No Improvement.\n",
      "Epoch  15/50 Batch  325/485 - Loss:  1.380, Seconds: 8.51\n",
      "Epoch  15/50 Batch  330/485 - Loss:  1.162, Seconds: 11.45\n",
      "Epoch  15/50 Batch  335/485 - Loss:  1.338, Seconds: 10.60\n",
      "Epoch  15/50 Batch  340/485 - Loss:  1.282, Seconds: 9.98\n",
      "Epoch  15/50 Batch  345/485 - Loss:  1.353, Seconds: 10.37\n",
      "Epoch  15/50 Batch  350/485 - Loss:  1.389, Seconds: 10.24\n",
      "Epoch  15/50 Batch  355/485 - Loss:  1.270, Seconds: 11.10\n",
      "Epoch  15/50 Batch  360/485 - Loss:  1.311, Seconds: 12.17\n",
      "Epoch  15/50 Batch  365/485 - Loss:  1.309, Seconds: 11.85\n",
      "Epoch  15/50 Batch  370/485 - Loss:  1.356, Seconds: 11.30\n",
      "Epoch  15/50 Batch  375/485 - Loss:  1.402, Seconds: 10.96\n",
      "Epoch  15/50 Batch  380/485 - Loss:  1.410, Seconds: 11.56\n",
      "Epoch  15/50 Batch  385/485 - Loss:  1.381, Seconds: 11.68\n",
      "Epoch  15/50 Batch  390/485 - Loss:  1.254, Seconds: 12.01\n",
      "Epoch  15/50 Batch  395/485 - Loss:  1.444, Seconds: 11.19\n",
      "Epoch  15/50 Batch  400/485 - Loss:  1.343, Seconds: 12.30\n",
      "Epoch  15/50 Batch  405/485 - Loss:  1.373, Seconds: 12.72\n",
      "Epoch  15/50 Batch  410/485 - Loss:  1.383, Seconds: 12.67\n",
      "Epoch  15/50 Batch  415/485 - Loss:  1.504, Seconds: 13.79\n",
      "Epoch  15/50 Batch  420/485 - Loss:  1.444, Seconds: 13.61\n",
      "Epoch  15/50 Batch  425/485 - Loss:  1.370, Seconds: 12.77\n",
      "Epoch  15/50 Batch  430/485 - Loss:  1.441, Seconds: 13.48\n",
      "Epoch  15/50 Batch  435/485 - Loss:  1.550, Seconds: 13.91\n",
      "Epoch  15/50 Batch  440/485 - Loss:  1.513, Seconds: 13.80\n",
      "Epoch  15/50 Batch  445/485 - Loss:  1.516, Seconds: 14.34\n",
      "Epoch  15/50 Batch  450/485 - Loss:  1.589, Seconds: 14.24\n",
      "Epoch  15/50 Batch  455/485 - Loss:  1.440, Seconds: 15.24\n",
      "Epoch  15/50 Batch  460/485 - Loss:  1.720, Seconds: 16.22\n",
      "Epoch  15/50 Batch  465/485 - Loss:  1.607, Seconds: 18.35\n",
      "Epoch  15/50 Batch  470/485 - Loss:  1.663, Seconds: 19.19\n",
      "Epoch  15/50 Batch  475/485 - Loss:  1.676, Seconds: 20.16\n",
      "Epoch  15/50 Batch  480/485 - Loss:  1.705, Seconds: 20.34\n",
      "Average loss for this update: 1.434\n",
      "No Improvement.\n",
      "Epoch  16/50 Batch    5/485 - Loss:  1.490, Seconds: 6.61\n",
      "Epoch  16/50 Batch   10/485 - Loss:  0.991, Seconds: 7.27\n",
      "Epoch  16/50 Batch   15/485 - Loss:  1.019, Seconds: 6.10\n",
      "Epoch  16/50 Batch   20/485 - Loss:  0.993, Seconds: 7.98\n",
      "Epoch  16/50 Batch   25/485 - Loss:  1.129, Seconds: 5.46\n",
      "Epoch  16/50 Batch   30/485 - Loss:  1.011, Seconds: 8.03\n",
      "Epoch  16/50 Batch   35/485 - Loss:  1.070, Seconds: 7.79\n",
      "Epoch  16/50 Batch   40/485 - Loss:  1.068, Seconds: 5.83\n",
      "Epoch  16/50 Batch   45/485 - Loss:  1.063, Seconds: 7.49\n",
      "Epoch  16/50 Batch   50/485 - Loss:  1.048, Seconds: 8.29\n",
      "Epoch  16/50 Batch   55/485 - Loss:  1.028, Seconds: 5.36\n",
      "Epoch  16/50 Batch   60/485 - Loss:  1.038, Seconds: 5.10\n",
      "Epoch  16/50 Batch   65/485 - Loss:  1.043, Seconds: 6.23\n",
      "Epoch  16/50 Batch   70/485 - Loss:  0.946, Seconds: 6.99\n",
      "Epoch  16/50 Batch   75/485 - Loss:  1.127, Seconds: 7.70\n",
      "Epoch  16/50 Batch   80/485 - Loss:  1.044, Seconds: 7.59\n",
      "Epoch  16/50 Batch   85/485 - Loss:  0.984, Seconds: 7.25\n",
      "Epoch  16/50 Batch   90/485 - Loss:  1.003, Seconds: 6.68\n",
      "Epoch  16/50 Batch   95/485 - Loss:  1.017, Seconds: 7.26\n",
      "Epoch  16/50 Batch  100/485 - Loss:  0.973, Seconds: 9.02\n",
      "Epoch  16/50 Batch  105/485 - Loss:  1.009, Seconds: 9.22\n",
      "Epoch  16/50 Batch  110/485 - Loss:  0.970, Seconds: 8.04\n",
      "Epoch  16/50 Batch  115/485 - Loss:  1.091, Seconds: 8.56\n",
      "Epoch  16/50 Batch  120/485 - Loss:  0.964, Seconds: 7.98\n",
      "Epoch  16/50 Batch  125/485 - Loss:  1.070, Seconds: 7.70\n",
      "Epoch  16/50 Batch  130/485 - Loss:  1.009, Seconds: 6.77\n",
      "Epoch  16/50 Batch  135/485 - Loss:  1.054, Seconds: 8.69\n",
      "Epoch  16/50 Batch  140/485 - Loss:  1.102, Seconds: 7.54\n",
      "Epoch  16/50 Batch  145/485 - Loss:  1.043, Seconds: 8.09\n",
      "Epoch  16/50 Batch  150/485 - Loss:  1.014, Seconds: 7.84\n",
      "Epoch  16/50 Batch  155/485 - Loss:  1.085, Seconds: 8.19\n",
      "Epoch  16/50 Batch  160/485 - Loss:  1.014, Seconds: 7.02\n",
      "Average loss for this update: 1.047\n",
      "New Record!\n",
      "Epoch  16/50 Batch  165/485 - Loss:  1.101, Seconds: 7.72\n",
      "Epoch  16/50 Batch  170/485 - Loss:  1.104, Seconds: 8.34\n",
      "Epoch  16/50 Batch  175/485 - Loss:  1.019, Seconds: 9.13\n",
      "Epoch  16/50 Batch  180/485 - Loss:  1.056, Seconds: 9.10\n",
      "Epoch  16/50 Batch  185/485 - Loss:  1.105, Seconds: 7.92\n",
      "Epoch  16/50 Batch  190/485 - Loss:  1.064, Seconds: 9.63\n",
      "Epoch  16/50 Batch  195/485 - Loss:  1.090, Seconds: 7.75\n",
      "Epoch  16/50 Batch  200/485 - Loss:  1.159, Seconds: 7.93\n",
      "Epoch  16/50 Batch  205/485 - Loss:  1.181, Seconds: 7.36\n",
      "Epoch  16/50 Batch  210/485 - Loss:  1.144, Seconds: 9.84\n",
      "Epoch  16/50 Batch  215/485 - Loss:  1.144, Seconds: 8.66\n",
      "Epoch  16/50 Batch  220/485 - Loss:  1.121, Seconds: 9.15\n",
      "Epoch  16/50 Batch  225/485 - Loss:  1.118, Seconds: 7.30\n",
      "Epoch  16/50 Batch  230/485 - Loss:  1.170, Seconds: 9.48\n",
      "Epoch  16/50 Batch  235/485 - Loss:  1.122, Seconds: 9.35\n",
      "Epoch  16/50 Batch  240/485 - Loss:  1.171, Seconds: 7.40\n",
      "Epoch  16/50 Batch  245/485 - Loss:  1.128, Seconds: 8.87\n",
      "Epoch  16/50 Batch  250/485 - Loss:  1.125, Seconds: 10.35\n",
      "Epoch  16/50 Batch  255/485 - Loss:  1.180, Seconds: 10.50\n",
      "Epoch  16/50 Batch  260/485 - Loss:  1.179, Seconds: 10.08\n",
      "Epoch  16/50 Batch  265/485 - Loss:  1.157, Seconds: 7.83\n",
      "Epoch  16/50 Batch  270/485 - Loss:  1.200, Seconds: 9.11\n",
      "Epoch  16/50 Batch  275/485 - Loss:  1.170, Seconds: 10.01\n",
      "Epoch  16/50 Batch  280/485 - Loss:  1.126, Seconds: 10.10\n",
      "Epoch  16/50 Batch  285/485 - Loss:  1.270, Seconds: 9.34\n",
      "Epoch  16/50 Batch  290/485 - Loss:  1.167, Seconds: 9.94\n",
      "Epoch  16/50 Batch  295/485 - Loss:  1.077, Seconds: 11.17\n",
      "Epoch  16/50 Batch  300/485 - Loss:  1.130, Seconds: 10.69\n",
      "Epoch  16/50 Batch  305/485 - Loss:  1.243, Seconds: 10.69\n",
      "Epoch  16/50 Batch  310/485 - Loss:  1.250, Seconds: 8.87\n",
      "Epoch  16/50 Batch  315/485 - Loss:  1.244, Seconds: 9.90\n",
      "Epoch  16/50 Batch  320/485 - Loss:  1.274, Seconds: 11.09\n",
      "Average loss for this update: 1.15\n",
      "No Improvement.\n",
      "Epoch  16/50 Batch  325/485 - Loss:  1.356, Seconds: 8.90\n",
      "Epoch  16/50 Batch  330/485 - Loss:  1.150, Seconds: 11.49\n",
      "Epoch  16/50 Batch  335/485 - Loss:  1.305, Seconds: 10.92\n",
      "Epoch  16/50 Batch  340/485 - Loss:  1.262, Seconds: 10.47\n",
      "Epoch  16/50 Batch  345/485 - Loss:  1.306, Seconds: 10.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16/50 Batch  350/485 - Loss:  1.363, Seconds: 10.63\n",
      "Epoch  16/50 Batch  355/485 - Loss:  1.237, Seconds: 11.48\n",
      "Epoch  16/50 Batch  360/485 - Loss:  1.254, Seconds: 11.79\n",
      "Epoch  16/50 Batch  365/485 - Loss:  1.305, Seconds: 12.58\n",
      "Epoch  16/50 Batch  370/485 - Loss:  1.317, Seconds: 11.45\n",
      "Epoch  16/50 Batch  375/485 - Loss:  1.339, Seconds: 11.12\n",
      "Epoch  16/50 Batch  380/485 - Loss:  1.359, Seconds: 11.93\n",
      "Epoch  16/50 Batch  385/485 - Loss:  1.380, Seconds: 12.12\n",
      "Epoch  16/50 Batch  390/485 - Loss:  1.245, Seconds: 11.80\n",
      "Epoch  16/50 Batch  395/485 - Loss:  1.410, Seconds: 11.60\n",
      "Epoch  16/50 Batch  400/485 - Loss:  1.345, Seconds: 12.30\n",
      "Epoch  16/50 Batch  405/485 - Loss:  1.335, Seconds: 13.11\n",
      "Epoch  16/50 Batch  410/485 - Loss:  1.362, Seconds: 12.89\n",
      "Epoch  16/50 Batch  415/485 - Loss:  1.451, Seconds: 13.88\n",
      "Epoch  16/50 Batch  420/485 - Loss:  1.401, Seconds: 13.77\n",
      "Epoch  16/50 Batch  425/485 - Loss:  1.324, Seconds: 12.75\n",
      "Epoch  16/50 Batch  430/485 - Loss:  1.420, Seconds: 13.90\n",
      "Epoch  16/50 Batch  435/485 - Loss:  1.495, Seconds: 13.64\n",
      "Epoch  16/50 Batch  440/485 - Loss:  1.492, Seconds: 14.00\n",
      "Epoch  16/50 Batch  445/485 - Loss:  1.483, Seconds: 14.65\n",
      "Epoch  16/50 Batch  450/485 - Loss:  1.522, Seconds: 14.06\n",
      "Epoch  16/50 Batch  455/485 - Loss:  1.441, Seconds: 15.69\n",
      "Epoch  16/50 Batch  460/485 - Loss:  1.672, Seconds: 15.23\n",
      "Epoch  16/50 Batch  465/485 - Loss:  1.577, Seconds: 17.60\n",
      "Epoch  16/50 Batch  470/485 - Loss:  1.656, Seconds: 18.85\n",
      "Epoch  16/50 Batch  475/485 - Loss:  1.623, Seconds: 20.41\n",
      "Epoch  16/50 Batch  480/485 - Loss:  1.664, Seconds: 20.54\n",
      "Average loss for this update: 1.402\n",
      "No Improvement.\n",
      "Epoch  17/50 Batch    5/485 - Loss:  1.529, Seconds: 6.53\n",
      "Epoch  17/50 Batch   10/485 - Loss:  0.983, Seconds: 7.22\n",
      "Epoch  17/50 Batch   15/485 - Loss:  1.031, Seconds: 6.24\n",
      "Epoch  17/50 Batch   20/485 - Loss:  0.990, Seconds: 8.07\n",
      "Epoch  17/50 Batch   25/485 - Loss:  1.062, Seconds: 5.72\n",
      "Epoch  17/50 Batch   30/485 - Loss:  1.012, Seconds: 8.16\n",
      "Epoch  17/50 Batch   35/485 - Loss:  1.025, Seconds: 7.94\n",
      "Epoch  17/50 Batch   40/485 - Loss:  1.077, Seconds: 5.95\n",
      "Epoch  17/50 Batch   45/485 - Loss:  1.048, Seconds: 7.57\n",
      "Epoch  17/50 Batch   50/485 - Loss:  1.050, Seconds: 7.92\n",
      "Epoch  17/50 Batch   55/485 - Loss:  0.977, Seconds: 5.00\n",
      "Epoch  17/50 Batch   60/485 - Loss:  0.986, Seconds: 5.04\n",
      "Epoch  17/50 Batch   65/485 - Loss:  1.016, Seconds: 6.21\n",
      "Epoch  17/50 Batch   70/485 - Loss:  0.949, Seconds: 6.96\n",
      "Epoch  17/50 Batch   75/485 - Loss:  1.115, Seconds: 7.80\n",
      "Epoch  17/50 Batch   80/485 - Loss:  1.009, Seconds: 7.90\n",
      "Epoch  17/50 Batch   85/485 - Loss:  0.966, Seconds: 7.62\n",
      "Epoch  17/50 Batch   90/485 - Loss:  0.975, Seconds: 6.61\n",
      "Epoch  17/50 Batch   95/485 - Loss:  0.971, Seconds: 7.79\n",
      "Epoch  17/50 Batch  100/485 - Loss:  0.963, Seconds: 9.34\n",
      "Epoch  17/50 Batch  105/485 - Loss:  0.998, Seconds: 9.33\n",
      "Epoch  17/50 Batch  110/485 - Loss:  0.958, Seconds: 8.08\n",
      "Epoch  17/50 Batch  115/485 - Loss:  1.074, Seconds: 8.60\n",
      "Epoch  17/50 Batch  120/485 - Loss:  0.959, Seconds: 8.11\n",
      "Epoch  17/50 Batch  125/485 - Loss:  1.066, Seconds: 7.78\n",
      "Epoch  17/50 Batch  130/485 - Loss:  0.968, Seconds: 6.81\n",
      "Epoch  17/50 Batch  135/485 - Loss:  1.015, Seconds: 8.51\n",
      "Epoch  17/50 Batch  140/485 - Loss:  1.052, Seconds: 7.31\n",
      "Epoch  17/50 Batch  145/485 - Loss:  1.036, Seconds: 7.86\n",
      "Epoch  17/50 Batch  150/485 - Loss:  0.970, Seconds: 7.48\n",
      "Epoch  17/50 Batch  155/485 - Loss:  1.047, Seconds: 8.06\n",
      "Epoch  17/50 Batch  160/485 - Loss:  1.016, Seconds: 6.81\n",
      "Average loss for this update: 1.028\n",
      "New Record!\n",
      "Epoch  17/50 Batch  165/485 - Loss:  1.071, Seconds: 7.69\n",
      "Epoch  17/50 Batch  170/485 - Loss:  1.113, Seconds: 8.31\n",
      "Epoch  17/50 Batch  175/485 - Loss:  0.996, Seconds: 9.14\n",
      "Epoch  17/50 Batch  180/485 - Loss:  1.029, Seconds: 9.78\n",
      "Epoch  17/50 Batch  185/485 - Loss:  1.092, Seconds: 7.94\n",
      "Epoch  17/50 Batch  190/485 - Loss:  1.033, Seconds: 9.86\n",
      "Epoch  17/50 Batch  195/485 - Loss:  1.055, Seconds: 7.81\n",
      "Epoch  17/50 Batch  200/485 - Loss:  1.152, Seconds: 7.74\n",
      "Epoch  17/50 Batch  205/485 - Loss:  1.138, Seconds: 7.30\n",
      "Epoch  17/50 Batch  210/485 - Loss:  1.126, Seconds: 9.95\n",
      "Epoch  17/50 Batch  215/485 - Loss:  1.102, Seconds: 8.75\n",
      "Epoch  17/50 Batch  220/485 - Loss:  1.095, Seconds: 9.37\n",
      "Epoch  17/50 Batch  225/485 - Loss:  1.089, Seconds: 7.66\n",
      "Epoch  17/50 Batch  230/485 - Loss:  1.101, Seconds: 9.54\n",
      "Epoch  17/50 Batch  235/485 - Loss:  1.090, Seconds: 9.57\n",
      "Epoch  17/50 Batch  240/485 - Loss:  1.110, Seconds: 7.08\n",
      "Epoch  17/50 Batch  245/485 - Loss:  1.114, Seconds: 9.26\n",
      "Epoch  17/50 Batch  250/485 - Loss:  1.114, Seconds: 10.31\n",
      "Epoch  17/50 Batch  255/485 - Loss:  1.192, Seconds: 10.24\n",
      "Epoch  17/50 Batch  260/485 - Loss:  1.167, Seconds: 9.75\n",
      "Epoch  17/50 Batch  265/485 - Loss:  1.152, Seconds: 7.76\n",
      "Epoch  17/50 Batch  270/485 - Loss:  1.169, Seconds: 9.50\n",
      "Epoch  17/50 Batch  275/485 - Loss:  1.144, Seconds: 9.72\n",
      "Epoch  17/50 Batch  280/485 - Loss:  1.134, Seconds: 9.91\n",
      "Epoch  17/50 Batch  285/485 - Loss:  1.242, Seconds: 9.24\n",
      "Epoch  17/50 Batch  290/485 - Loss:  1.168, Seconds: 9.97\n",
      "Epoch  17/50 Batch  295/485 - Loss:  1.051, Seconds: 10.71\n",
      "Epoch  17/50 Batch  300/485 - Loss:  1.091, Seconds: 10.09\n",
      "Epoch  17/50 Batch  305/485 - Loss:  1.218, Seconds: 10.10\n",
      "Epoch  17/50 Batch  310/485 - Loss:  1.235, Seconds: 8.86\n",
      "Epoch  17/50 Batch  315/485 - Loss:  1.251, Seconds: 9.62\n",
      "Epoch  17/50 Batch  320/485 - Loss:  1.257, Seconds: 10.41\n",
      "Average loss for this update: 1.128\n",
      "No Improvement.\n",
      "Epoch  17/50 Batch  325/485 - Loss:  1.328, Seconds: 8.56\n",
      "Epoch  17/50 Batch  330/485 - Loss:  1.123, Seconds: 11.31\n",
      "Epoch  17/50 Batch  335/485 - Loss:  1.277, Seconds: 10.76\n",
      "Epoch  17/50 Batch  340/485 - Loss:  1.232, Seconds: 10.35\n",
      "Epoch  17/50 Batch  345/485 - Loss:  1.265, Seconds: 10.47\n",
      "Epoch  17/50 Batch  350/485 - Loss:  1.304, Seconds: 10.55\n",
      "Epoch  17/50 Batch  355/485 - Loss:  1.179, Seconds: 10.98\n",
      "Epoch  17/50 Batch  360/485 - Loss:  1.246, Seconds: 11.75\n",
      "Epoch  17/50 Batch  365/485 - Loss:  1.265, Seconds: 12.40\n",
      "Epoch  17/50 Batch  370/485 - Loss:  1.305, Seconds: 11.37\n",
      "Epoch  17/50 Batch  375/485 - Loss:  1.339, Seconds: 10.79\n",
      "Epoch  17/50 Batch  380/485 - Loss:  1.320, Seconds: 11.76\n",
      "Epoch  17/50 Batch  385/485 - Loss:  1.317, Seconds: 11.23\n",
      "Epoch  17/50 Batch  390/485 - Loss:  1.227, Seconds: 11.93\n",
      "Epoch  17/50 Batch  395/485 - Loss:  1.367, Seconds: 11.40\n",
      "Epoch  17/50 Batch  400/485 - Loss:  1.317, Seconds: 12.26\n",
      "Epoch  17/50 Batch  405/485 - Loss:  1.301, Seconds: 12.34\n",
      "Epoch  17/50 Batch  410/485 - Loss:  1.328, Seconds: 12.77\n",
      "Epoch  17/50 Batch  415/485 - Loss:  1.420, Seconds: 14.40\n",
      "Epoch  17/50 Batch  420/485 - Loss:  1.370, Seconds: 14.10\n",
      "Epoch  17/50 Batch  425/485 - Loss:  1.320, Seconds: 12.70\n",
      "Epoch  17/50 Batch  430/485 - Loss:  1.387, Seconds: 13.66\n",
      "Epoch  17/50 Batch  435/485 - Loss:  1.473, Seconds: 14.23\n",
      "Epoch  17/50 Batch  440/485 - Loss:  1.482, Seconds: 14.34\n",
      "Epoch  17/50 Batch  445/485 - Loss:  1.466, Seconds: 14.75\n",
      "Epoch  17/50 Batch  450/485 - Loss:  1.519, Seconds: 14.23\n",
      "Epoch  17/50 Batch  455/485 - Loss:  1.394, Seconds: 15.52\n",
      "Epoch  17/50 Batch  460/485 - Loss:  1.660, Seconds: 15.28\n",
      "Epoch  17/50 Batch  465/485 - Loss:  1.537, Seconds: 17.60\n",
      "Epoch  17/50 Batch  470/485 - Loss:  1.608, Seconds: 18.89\n",
      "Epoch  17/50 Batch  475/485 - Loss:  1.580, Seconds: 20.00\n",
      "Epoch  17/50 Batch  480/485 - Loss:  1.643, Seconds: 20.84\n",
      "Average loss for this update: 1.372\n",
      "No Improvement.\n",
      "Epoch  18/50 Batch    5/485 - Loss:  1.460, Seconds: 7.07\n",
      "Epoch  18/50 Batch   10/485 - Loss:  0.918, Seconds: 7.49\n",
      "Epoch  18/50 Batch   15/485 - Loss:  0.974, Seconds: 6.22\n",
      "Epoch  18/50 Batch   20/485 - Loss:  0.922, Seconds: 8.30\n",
      "Epoch  18/50 Batch   25/485 - Loss:  1.057, Seconds: 5.55\n",
      "Epoch  18/50 Batch   30/485 - Loss:  0.995, Seconds: 8.48\n",
      "Epoch  18/50 Batch   35/485 - Loss:  0.997, Seconds: 7.43\n",
      "Epoch  18/50 Batch   40/485 - Loss:  1.068, Seconds: 5.90\n",
      "Epoch  18/50 Batch   45/485 - Loss:  1.015, Seconds: 7.67\n",
      "Epoch  18/50 Batch   50/485 - Loss:  1.001, Seconds: 8.32\n",
      "Epoch  18/50 Batch   55/485 - Loss:  0.966, Seconds: 5.30\n",
      "Epoch  18/50 Batch   60/485 - Loss:  0.959, Seconds: 5.21\n",
      "Epoch  18/50 Batch   65/485 - Loss:  0.985, Seconds: 6.32\n",
      "Epoch  18/50 Batch   70/485 - Loss:  0.926, Seconds: 7.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18/50 Batch   75/485 - Loss:  1.073, Seconds: 7.81\n",
      "Epoch  18/50 Batch   80/485 - Loss:  0.956, Seconds: 7.53\n",
      "Epoch  18/50 Batch   85/485 - Loss:  0.934, Seconds: 7.11\n",
      "Epoch  18/50 Batch   90/485 - Loss:  0.982, Seconds: 6.73\n",
      "Epoch  18/50 Batch   95/485 - Loss:  0.957, Seconds: 7.09\n",
      "Epoch  18/50 Batch  100/485 - Loss:  0.919, Seconds: 9.15\n",
      "Epoch  18/50 Batch  105/485 - Loss:  0.959, Seconds: 9.04\n",
      "Epoch  18/50 Batch  110/485 - Loss:  0.930, Seconds: 7.91\n",
      "Epoch  18/50 Batch  115/485 - Loss:  1.046, Seconds: 8.78\n",
      "Epoch  18/50 Batch  120/485 - Loss:  0.932, Seconds: 7.95\n",
      "Epoch  18/50 Batch  125/485 - Loss:  1.018, Seconds: 7.43\n",
      "Epoch  18/50 Batch  130/485 - Loss:  0.960, Seconds: 6.58\n",
      "Epoch  18/50 Batch  135/485 - Loss:  1.033, Seconds: 8.87\n",
      "Epoch  18/50 Batch  140/485 - Loss:  1.046, Seconds: 7.41\n",
      "Epoch  18/50 Batch  145/485 - Loss:  1.028, Seconds: 8.07\n",
      "Epoch  18/50 Batch  150/485 - Loss:  0.967, Seconds: 7.67\n",
      "Epoch  18/50 Batch  155/485 - Loss:  1.000, Seconds: 8.27\n",
      "Epoch  18/50 Batch  160/485 - Loss:  0.967, Seconds: 6.99\n",
      "Average loss for this update: 0.998\n",
      "New Record!\n",
      "Epoch  18/50 Batch  165/485 - Loss:  1.039, Seconds: 7.75\n",
      "Epoch  18/50 Batch  170/485 - Loss:  1.063, Seconds: 8.48\n",
      "Epoch  18/50 Batch  175/485 - Loss:  0.997, Seconds: 8.92\n",
      "Epoch  18/50 Batch  180/485 - Loss:  1.019, Seconds: 8.91\n",
      "Epoch  18/50 Batch  185/485 - Loss:  1.115, Seconds: 7.79\n",
      "Epoch  18/50 Batch  190/485 - Loss:  1.035, Seconds: 10.13\n",
      "Epoch  18/50 Batch  195/485 - Loss:  1.072, Seconds: 8.11\n",
      "Epoch  18/50 Batch  200/485 - Loss:  1.130, Seconds: 8.21\n",
      "Epoch  18/50 Batch  205/485 - Loss:  1.129, Seconds: 7.58\n",
      "Epoch  18/50 Batch  210/485 - Loss:  1.115, Seconds: 10.34\n",
      "Epoch  18/50 Batch  215/485 - Loss:  1.112, Seconds: 8.85\n",
      "Epoch  18/50 Batch  220/485 - Loss:  1.074, Seconds: 9.24\n",
      "Epoch  18/50 Batch  225/485 - Loss:  1.092, Seconds: 7.46\n",
      "Epoch  18/50 Batch  230/485 - Loss:  1.106, Seconds: 9.52\n",
      "Epoch  18/50 Batch  235/485 - Loss:  1.067, Seconds: 9.35\n",
      "Epoch  18/50 Batch  240/485 - Loss:  1.136, Seconds: 6.93\n",
      "Epoch  18/50 Batch  245/485 - Loss:  1.100, Seconds: 8.89\n",
      "Epoch  18/50 Batch  250/485 - Loss:  1.094, Seconds: 10.21\n",
      "Epoch  18/50 Batch  255/485 - Loss:  1.155, Seconds: 10.46\n",
      "Epoch  18/50 Batch  260/485 - Loss:  1.163, Seconds: 9.85\n",
      "Epoch  18/50 Batch  265/485 - Loss:  1.111, Seconds: 7.95\n",
      "Epoch  18/50 Batch  270/485 - Loss:  1.191, Seconds: 9.17\n",
      "Epoch  18/50 Batch  275/485 - Loss:  1.185, Seconds: 9.98\n",
      "Epoch  18/50 Batch  280/485 - Loss:  1.110, Seconds: 10.36\n",
      "Epoch  18/50 Batch  285/485 - Loss:  1.231, Seconds: 10.02\n",
      "Epoch  18/50 Batch  290/485 - Loss:  1.155, Seconds: 10.37\n",
      "Epoch  18/50 Batch  295/485 - Loss:  1.046, Seconds: 11.23\n",
      "Epoch  18/50 Batch  300/485 - Loss:  1.110, Seconds: 10.34\n",
      "Epoch  18/50 Batch  305/485 - Loss:  1.243, Seconds: 10.42\n",
      "Epoch  18/50 Batch  310/485 - Loss:  1.219, Seconds: 9.11\n",
      "Epoch  18/50 Batch  315/485 - Loss:  1.263, Seconds: 9.73\n",
      "Epoch  18/50 Batch  320/485 - Loss:  1.219, Seconds: 10.57\n",
      "Average loss for this update: 1.122\n",
      "No Improvement.\n",
      "Epoch  18/50 Batch  325/485 - Loss:  1.330, Seconds: 8.71\n",
      "Epoch  18/50 Batch  330/485 - Loss:  1.142, Seconds: 11.75\n",
      "Epoch  18/50 Batch  335/485 - Loss:  1.289, Seconds: 11.41\n",
      "Epoch  18/50 Batch  340/485 - Loss:  1.233, Seconds: 10.24\n",
      "Epoch  18/50 Batch  345/485 - Loss:  1.297, Seconds: 10.37\n",
      "Epoch  18/50 Batch  350/485 - Loss:  1.324, Seconds: 10.46\n",
      "Epoch  18/50 Batch  355/485 - Loss:  1.210, Seconds: 11.24\n",
      "Epoch  18/50 Batch  360/485 - Loss:  1.246, Seconds: 11.89\n",
      "Epoch  18/50 Batch  365/485 - Loss:  1.210, Seconds: 12.34\n",
      "Epoch  18/50 Batch  370/485 - Loss:  1.286, Seconds: 11.81\n",
      "Epoch  18/50 Batch  375/485 - Loss:  1.292, Seconds: 10.93\n",
      "Epoch  18/50 Batch  380/485 - Loss:  1.314, Seconds: 11.83\n",
      "Epoch  18/50 Batch  385/485 - Loss:  1.274, Seconds: 11.25\n",
      "Epoch  18/50 Batch  390/485 - Loss:  1.205, Seconds: 12.09\n",
      "Epoch  18/50 Batch  395/485 - Loss:  1.343, Seconds: 11.43\n",
      "Epoch  18/50 Batch  400/485 - Loss:  1.283, Seconds: 12.44\n",
      "Epoch  18/50 Batch  405/485 - Loss:  1.296, Seconds: 12.45\n",
      "Epoch  18/50 Batch  410/485 - Loss:  1.331, Seconds: 12.43\n",
      "Epoch  18/50 Batch  415/485 - Loss:  1.407, Seconds: 13.70\n",
      "Epoch  18/50 Batch  420/485 - Loss:  1.338, Seconds: 13.89\n",
      "Epoch  18/50 Batch  425/485 - Loss:  1.332, Seconds: 13.03\n",
      "Epoch  18/50 Batch  430/485 - Loss:  1.363, Seconds: 13.58\n",
      "Epoch  18/50 Batch  435/485 - Loss:  1.484, Seconds: 14.14\n",
      "Epoch  18/50 Batch  440/485 - Loss:  1.455, Seconds: 14.24\n",
      "Epoch  18/50 Batch  445/485 - Loss:  1.473, Seconds: 14.60\n",
      "Epoch  18/50 Batch  450/485 - Loss:  1.535, Seconds: 14.14\n",
      "Epoch  18/50 Batch  455/485 - Loss:  1.370, Seconds: 15.30\n",
      "Epoch  18/50 Batch  460/485 - Loss:  1.647, Seconds: 15.10\n",
      "Epoch  18/50 Batch  465/485 - Loss:  1.525, Seconds: 17.47\n",
      "Epoch  18/50 Batch  470/485 - Loss:  1.589, Seconds: 18.48\n",
      "Epoch  18/50 Batch  475/485 - Loss:  1.580, Seconds: 19.90\n",
      "Epoch  18/50 Batch  480/485 - Loss:  1.629, Seconds: 20.05\n",
      "Average loss for this update: 1.363\n",
      "No Improvement.\n",
      "Epoch  19/50 Batch    5/485 - Loss:  1.536, Seconds: 6.57\n",
      "Epoch  19/50 Batch   10/485 - Loss:  0.912, Seconds: 7.42\n",
      "Epoch  19/50 Batch   15/485 - Loss:  0.957, Seconds: 6.12\n",
      "Epoch  19/50 Batch   20/485 - Loss:  0.923, Seconds: 8.45\n",
      "Epoch  19/50 Batch   25/485 - Loss:  1.018, Seconds: 5.70\n",
      "Epoch  19/50 Batch   30/485 - Loss:  0.966, Seconds: 8.00\n",
      "Epoch  19/50 Batch   35/485 - Loss:  1.026, Seconds: 7.62\n",
      "Epoch  19/50 Batch   40/485 - Loss:  1.053, Seconds: 6.13\n",
      "Epoch  19/50 Batch   45/485 - Loss:  1.013, Seconds: 7.78\n",
      "Epoch  19/50 Batch   50/485 - Loss:  1.023, Seconds: 8.28\n",
      "Epoch  19/50 Batch   55/485 - Loss:  0.937, Seconds: 5.17\n",
      "Epoch  19/50 Batch   60/485 - Loss:  0.967, Seconds: 5.81\n",
      "Epoch  19/50 Batch   65/485 - Loss:  0.998, Seconds: 6.49\n",
      "Epoch  19/50 Batch   70/485 - Loss:  0.904, Seconds: 7.76\n",
      "Epoch  19/50 Batch   75/485 - Loss:  1.062, Seconds: 8.25\n",
      "Epoch  19/50 Batch   80/485 - Loss:  0.937, Seconds: 7.71\n",
      "Epoch  19/50 Batch   85/485 - Loss:  0.947, Seconds: 7.68\n",
      "Epoch  19/50 Batch   90/485 - Loss:  0.977, Seconds: 6.82\n",
      "Epoch  19/50 Batch   95/485 - Loss:  0.939, Seconds: 7.30\n",
      "Epoch  19/50 Batch  100/485 - Loss:  0.940, Seconds: 9.09\n",
      "Epoch  19/50 Batch  105/485 - Loss:  0.961, Seconds: 9.08\n",
      "Epoch  19/50 Batch  110/485 - Loss:  0.914, Seconds: 8.63\n",
      "Epoch  19/50 Batch  115/485 - Loss:  1.010, Seconds: 8.43\n",
      "Epoch  19/50 Batch  120/485 - Loss:  0.913, Seconds: 8.13\n",
      "Epoch  19/50 Batch  125/485 - Loss:  1.011, Seconds: 7.78\n",
      "Epoch  19/50 Batch  130/485 - Loss:  0.930, Seconds: 7.49\n",
      "Epoch  19/50 Batch  135/485 - Loss:  1.004, Seconds: 9.27\n",
      "Epoch  19/50 Batch  140/485 - Loss:  1.040, Seconds: 8.02\n",
      "Epoch  19/50 Batch  145/485 - Loss:  0.971, Seconds: 8.12\n",
      "Epoch  19/50 Batch  150/485 - Loss:  0.948, Seconds: 7.70\n",
      "Epoch  19/50 Batch  155/485 - Loss:  1.015, Seconds: 8.31\n",
      "Epoch  19/50 Batch  160/485 - Loss:  0.982, Seconds: 7.47\n",
      "Average loss for this update: 0.992\n",
      "New Record!\n",
      "Epoch  19/50 Batch  165/485 - Loss:  1.019, Seconds: 7.83\n",
      "Epoch  19/50 Batch  170/485 - Loss:  1.052, Seconds: 8.67\n",
      "Epoch  19/50 Batch  175/485 - Loss:  0.987, Seconds: 9.29\n",
      "Epoch  19/50 Batch  180/485 - Loss:  0.960, Seconds: 9.30\n",
      "Epoch  19/50 Batch  185/485 - Loss:  1.059, Seconds: 8.39\n",
      "Epoch  19/50 Batch  190/485 - Loss:  1.010, Seconds: 10.35\n",
      "Epoch  19/50 Batch  195/485 - Loss:  1.069, Seconds: 8.41\n",
      "Epoch  19/50 Batch  200/485 - Loss:  1.175, Seconds: 8.02\n",
      "Epoch  19/50 Batch  205/485 - Loss:  1.170, Seconds: 7.44\n",
      "Epoch  19/50 Batch  210/485 - Loss:  1.198, Seconds: 10.04\n",
      "Epoch  19/50 Batch  215/485 - Loss:  1.171, Seconds: 9.56\n",
      "Epoch  19/50 Batch  220/485 - Loss:  1.175, Seconds: 9.42\n",
      "Epoch  19/50 Batch  225/485 - Loss:  1.147, Seconds: 7.39\n",
      "Epoch  19/50 Batch  230/485 - Loss:  1.152, Seconds: 9.33\n",
      "Epoch  19/50 Batch  235/485 - Loss:  1.131, Seconds: 9.37\n",
      "Epoch  19/50 Batch  240/485 - Loss:  1.185, Seconds: 6.95\n",
      "Epoch  19/50 Batch  245/485 - Loss:  1.127, Seconds: 8.71\n",
      "Epoch  19/50 Batch  250/485 - Loss:  1.123, Seconds: 10.08\n",
      "Epoch  19/50 Batch  255/485 - Loss:  1.214, Seconds: 10.08\n",
      "Epoch  19/50 Batch  260/485 - Loss:  1.161, Seconds: 9.46\n",
      "Epoch  19/50 Batch  265/485 - Loss:  1.163, Seconds: 7.66\n",
      "Epoch  19/50 Batch  270/485 - Loss:  1.182, Seconds: 9.04\n",
      "Epoch  19/50 Batch  275/485 - Loss:  1.153, Seconds: 9.62\n",
      "Epoch  19/50 Batch  280/485 - Loss:  1.119, Seconds: 10.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19/50 Batch  285/485 - Loss:  1.238, Seconds: 9.77\n",
      "Epoch  19/50 Batch  290/485 - Loss:  1.148, Seconds: 10.49\n",
      "Epoch  19/50 Batch  295/485 - Loss:  1.054, Seconds: 10.68\n",
      "Epoch  19/50 Batch  300/485 - Loss:  1.093, Seconds: 10.11\n",
      "Epoch  19/50 Batch  305/485 - Loss:  1.233, Seconds: 10.15\n",
      "Epoch  19/50 Batch  310/485 - Loss:  1.231, Seconds: 8.88\n",
      "Epoch  19/50 Batch  315/485 - Loss:  1.231, Seconds: 9.84\n",
      "Epoch  19/50 Batch  320/485 - Loss:  1.237, Seconds: 10.57\n",
      "Average loss for this update: 1.136\n",
      "No Improvement.\n",
      "Epoch  19/50 Batch  325/485 - Loss:  1.321, Seconds: 8.48\n",
      "Epoch  19/50 Batch  330/485 - Loss:  1.110, Seconds: 11.36\n",
      "Epoch  19/50 Batch  335/485 - Loss:  1.271, Seconds: 10.88\n",
      "Epoch  19/50 Batch  340/485 - Loss:  1.237, Seconds: 10.19\n",
      "Epoch  19/50 Batch  345/485 - Loss:  1.274, Seconds: 10.28\n",
      "Epoch  19/50 Batch  350/485 - Loss:  1.327, Seconds: 10.48\n",
      "Epoch  19/50 Batch  355/485 - Loss:  1.198, Seconds: 11.51\n",
      "Epoch  19/50 Batch  360/485 - Loss:  1.236, Seconds: 12.17\n",
      "Epoch  19/50 Batch  365/485 - Loss:  1.244, Seconds: 12.29\n",
      "Epoch  19/50 Batch  370/485 - Loss:  1.281, Seconds: 11.33\n",
      "Epoch  19/50 Batch  375/485 - Loss:  1.326, Seconds: 10.86\n",
      "Epoch  19/50 Batch  380/485 - Loss:  1.320, Seconds: 11.94\n",
      "Epoch  19/50 Batch  385/485 - Loss:  1.301, Seconds: 11.26\n",
      "Epoch  19/50 Batch  390/485 - Loss:  1.219, Seconds: 12.11\n",
      "Epoch  19/50 Batch  395/485 - Loss:  1.345, Seconds: 11.30\n",
      "Epoch  19/50 Batch  400/485 - Loss:  1.299, Seconds: 12.16\n",
      "Epoch  19/50 Batch  405/485 - Loss:  1.278, Seconds: 12.46\n",
      "Epoch  19/50 Batch  410/485 - Loss:  1.301, Seconds: 13.30\n",
      "Epoch  19/50 Batch  415/485 - Loss:  1.383, Seconds: 14.12\n",
      "Epoch  19/50 Batch  420/485 - Loss:  1.371, Seconds: 14.47\n",
      "Epoch  19/50 Batch  425/485 - Loss:  1.305, Seconds: 13.40\n",
      "Epoch  19/50 Batch  430/485 - Loss:  1.381, Seconds: 13.78\n",
      "Epoch  19/50 Batch  435/485 - Loss:  1.471, Seconds: 13.94\n",
      "Epoch  19/50 Batch  440/485 - Loss:  1.449, Seconds: 14.50\n",
      "Epoch  19/50 Batch  445/485 - Loss:  1.470, Seconds: 14.41\n",
      "Epoch  19/50 Batch  450/485 - Loss:  1.494, Seconds: 14.10\n",
      "Epoch  19/50 Batch  455/485 - Loss:  1.399, Seconds: 15.31\n",
      "Epoch  19/50 Batch  460/485 - Loss:  1.664, Seconds: 15.06\n",
      "Epoch  19/50 Batch  465/485 - Loss:  1.574, Seconds: 17.88\n",
      "Epoch  19/50 Batch  470/485 - Loss:  1.592, Seconds: 18.70\n",
      "Epoch  19/50 Batch  475/485 - Loss:  1.600, Seconds: 20.63\n",
      "Epoch  19/50 Batch  480/485 - Loss:  1.644, Seconds: 19.97\n",
      "Average loss for this update: 1.365\n",
      "No Improvement.\n",
      "Epoch  20/50 Batch    5/485 - Loss:  1.573, Seconds: 6.71\n",
      "Epoch  20/50 Batch   10/485 - Loss:  0.937, Seconds: 7.41\n",
      "Epoch  20/50 Batch   15/485 - Loss:  1.004, Seconds: 6.14\n",
      "Epoch  20/50 Batch   20/485 - Loss:  0.965, Seconds: 8.00\n",
      "Epoch  20/50 Batch   25/485 - Loss:  1.013, Seconds: 5.69\n",
      "Epoch  20/50 Batch   30/485 - Loss:  0.960, Seconds: 8.01\n",
      "Epoch  20/50 Batch   35/485 - Loss:  1.011, Seconds: 7.54\n",
      "Epoch  20/50 Batch   40/485 - Loss:  1.076, Seconds: 5.72\n",
      "Epoch  20/50 Batch   45/485 - Loss:  1.029, Seconds: 7.38\n",
      "Epoch  20/50 Batch   50/485 - Loss:  1.026, Seconds: 8.13\n",
      "Epoch  20/50 Batch   55/485 - Loss:  0.976, Seconds: 5.79\n",
      "Epoch  20/50 Batch   60/485 - Loss:  0.971, Seconds: 5.15\n",
      "Epoch  20/50 Batch   65/485 - Loss:  0.974, Seconds: 6.37\n",
      "Epoch  20/50 Batch   70/485 - Loss:  0.921, Seconds: 7.12\n",
      "Epoch  20/50 Batch   75/485 - Loss:  1.080, Seconds: 7.53\n",
      "Epoch  20/50 Batch   80/485 - Loss:  0.978, Seconds: 7.57\n",
      "Epoch  20/50 Batch   85/485 - Loss:  0.961, Seconds: 7.09\n",
      "Epoch  20/50 Batch   90/485 - Loss:  0.987, Seconds: 6.67\n",
      "Epoch  20/50 Batch   95/485 - Loss:  0.956, Seconds: 7.16\n",
      "Epoch  20/50 Batch  100/485 - Loss:  0.921, Seconds: 9.17\n",
      "Epoch  20/50 Batch  105/485 - Loss:  0.947, Seconds: 9.66\n",
      "Epoch  20/50 Batch  110/485 - Loss:  0.929, Seconds: 8.53\n",
      "Epoch  20/50 Batch  115/485 - Loss:  1.019, Seconds: 9.14\n",
      "Epoch  20/50 Batch  120/485 - Loss:  0.918, Seconds: 8.39\n",
      "Epoch  20/50 Batch  125/485 - Loss:  1.038, Seconds: 7.92\n",
      "Epoch  20/50 Batch  130/485 - Loss:  0.969, Seconds: 6.86\n",
      "Epoch  20/50 Batch  135/485 - Loss:  0.992, Seconds: 8.53\n",
      "Epoch  20/50 Batch  140/485 - Loss:  1.016, Seconds: 7.55\n",
      "Epoch  20/50 Batch  145/485 - Loss:  0.998, Seconds: 8.28\n",
      "Epoch  20/50 Batch  150/485 - Loss:  0.947, Seconds: 7.65\n",
      "Epoch  20/50 Batch  155/485 - Loss:  1.010, Seconds: 8.14\n",
      "Epoch  20/50 Batch  160/485 - Loss:  0.970, Seconds: 6.96\n",
      "Average loss for this update: 1.002\n",
      "No Improvement.\n",
      "Stopping Training.\n",
      "Model Trained\n"
     ]
    }
   ],
   "source": [
    "print(\"Training will Strat now.\")\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "\n",
    "    #loader = tf.train.import_meta_graph(\"C:/Users/shrey/OneDrive/Study/Bennett/Sem2/HPC/Project/amazon_fine_food_review_summarizer/best_model.ckpt.meta\")\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    #by commenting above 2 lines the code will start retrain the model. \n",
    "            \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries, sorted_texts, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "        saver = tf.train.Saver() \n",
    "        saver.save(sess, checkpoint)\n",
    "\n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n",
    "\n",
    "print(\"Model Trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <span style=\"color:blue\">the above error is because i was tryong to restore the training from a certain point. There is some error that i am unable to find it reite now. It is unable to load old graph for training. I guess making new graph this error might get resolved. I hope to find a solution where i dont have to generate a new graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIIgCIaIQAiCIAiGiEAIgiAIhvz/7SQs2Og315kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c6bed9a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(\"Summary updated loss:\", summary_update_loss[:])\n",
    "#print(\"Updated loss: \", update_loss)\n",
    "\n",
    "plt.plot(summary_update_loss[:])\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing the Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text, remove_stopwords=True)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=35\n",
    "sum_F=12.346860213552002\n",
    "avg_F=0.3527674346729143\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "count=count-1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The problem with the bellow test code is that even if i give my own review, it doesnt take it as a input. It get some review from the data set itself. I am unable to identify why it is doing so. Please check and let me know.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text [328, 7836, 523, 13, 1153, 611, 4044, 61, 3292, 333, 113, 3233, 3, 2487, 812, 468, 113, 13, 2138, 2139, 760, 4888, 1619, 113, 3233, 3, 1411, 1381, 61, 460]\n",
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Index Value:  2805\n",
      "\n",
      "Original Text: This arrived earlier than expected, which was great! However, for some reason, this particular product smelled a little like \"fish food\".... but no worries, still tasted like great cashew butter, just the smell was kinda weird - like fish food. Also, I wish this product was organic.\n",
      "File Written to text.001.txt\n",
      "\n",
      "Orignal Summary:  I love cashew butter!\n",
      "\n",
      "Text\n",
      "  Word Ids:    [328, 7836, 523, 13, 1153, 611, 4044, 61, 3292, 333, 113, 3233, 3, 2487, 812, 468, 113, 13, 2138, 2139, 760, 4888, 1619, 113, 3233, 3, 1411, 1381, 61, 460]\n",
      "  Input Words: arrived earlier expected great however reason particular product smelled little like fish food worries still tasted like great cashew butter smell kinda weird like fish food also wish product organic\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [96, 113, 105, 523, 25386]\n",
      "File Written to text.A.001.txt\n",
      "  Response Words:  tastes like i expected bajillion\n",
      "Time:  45.358612298965454\n"
     ]
    }
   ],
   "source": [
    "count=count+1\n",
    "\n",
    "\n",
    "##Create your own review or use one from the dataset\n",
    "#input_sentence = \"The packaging and everything was intact, but the yeast I got expired on 12/16.. Not sure if this is still good to use?\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "\n",
    "##Comment the following 3 lines if using custom Summary and text\n",
    "random = np.random.randint(0,25389) #len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "print(\"Cleaned Text\", text)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    ## Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "##    Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "## Remove the padding from the summary\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "elapsed_time=time.time()-start_time\n",
    "\n",
    "##for custom summary and text, comment this line.\n",
    "print(\"Index Value: \",random)\n",
    "\n",
    "##for custom summary and text use \"input_sequence\" as parameter insted of \"reviews.Text.loc[random]\"\n",
    "print('\\nOriginal Text:',reviews.Text.loc[random]) #input_sentence) #\n",
    "\n",
    "##for custom orignal summary and text uncomment the next line and comment the next to next line.\n",
    "#ogSum=\"Expired yeast!\"\n",
    "ogSum=reviews.Summary.loc[random]\n",
    "ogFile=open(\"C:/Users/Shreyans/OneDrive/Study/Bennett/Sem2/HPC/Project/amazon_fine_food_review_summarizer/system_summaries/text.001.txt\",'w')\n",
    "ogFile.write(ogSum)\n",
    "print(\"File Written to text.001.txt\")\n",
    "ogFile.close()\n",
    "\n",
    "print('\\nOrignal Summary: ',ogSum)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "\n",
    "predSum=\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
    "predFile=open(\"C:/Users/Shreyans/OneDrive/Study/Bennett/Sem2/HPC/Project/amazon_fine_food_review_summarizer/model_summaries/text.A.001.txt\",'w')\n",
    "predFile.write(predSum)\n",
    "print(\"File Written to text.A.001.txt\")\n",
    "predFile.close()\n",
    "\n",
    "print(\"  Response Words: \",predSum)\n",
    "print(\"Time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Evaluation using ROUGE Scores</h1>\n",
    "Following code is from https://stackoverflow.com/questions/47045436/how-to-install-the-python-package-pyrouge-on-microsoft-windows/47045437#47045437.\n",
    "To use ROUGE, you need to install python wrapper for ROUGE. The processes to install it is given in above link.\n",
    "OR \n",
    "You can try and use this package from this link(code is also given) https://github.com/pcyin/PyRouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pyrouge import Rouge155\n",
    "r = Rouge155()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "r.system_dir = 'C:/Users/shrey/OneDrive/Study/Bennett/Sem2/HPC/Project/amazon_fine_food_review_summarizer/system_summaries'\n",
    "r.model_dir = 'C:/Users/shrey/OneDrive/Study/Bennett/Sem2/HPC/Project/amazon_fine_food_review_summarizer/model_summaries'\n",
    "r.system_filename_pattern = 'text.001.txt'\n",
    "r.model_filename_pattern = 'text.A.001.txt'\n",
    "\n",
    "print(r.system_filename_pattern)\n",
    "\n",
    "output = r.convert_and_evaluate()\n",
    "\n",
    "print(\"checkpoint\")\n",
    "print(output)\n",
    "output_dict = r.output_to_dict(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for calculating ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def get_unigram_count(tokens):\n",
    "    count_dict = dict()\n",
    "    for t in tokens:\n",
    "        if t in count_dict:\n",
    "            count_dict[t] += 1\n",
    "        else:\n",
    "            count_dict[t] = 1\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "class Rouge:\n",
    "    beta = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def my_lcs_grid(x, y):\n",
    "        n = len(x)\n",
    "        m = len(y)\n",
    "\n",
    "        table = [[0 for i in range(m + 1)] for j in range(n + 1)]\n",
    "\n",
    "        for j in range(m + 1):\n",
    "            for i in range(n + 1):\n",
    "                if i == 0 or j == 0:\n",
    "                    cell = (0, 'e')\n",
    "                elif x[i - 1] == y[j - 1]:\n",
    "                    cell = (table[i - 1][j - 1][0] + 1, '\\\\')\n",
    "                else:\n",
    "                    over = table[i - 1][j][0]\n",
    "                    left = table[i][j - 1][0]\n",
    "\n",
    "                    if left < over:\n",
    "                        cell = (over, '^')\n",
    "                    else:\n",
    "                        cell = (left, '<')\n",
    "\n",
    "                table[i][j] = cell\n",
    "\n",
    "        return table\n",
    "\n",
    "    @staticmethod\n",
    "    def my_lcs(x, y, mask_x):\n",
    "        table = Rouge.my_lcs_grid(x, y)\n",
    "        i = len(x)\n",
    "        j = len(y)\n",
    "\n",
    "        while i > 0 and j > 0:\n",
    "            move = table[i][j][1]\n",
    "            if move == '\\\\':\n",
    "                mask_x[i - 1] = 1\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif move == '^':\n",
    "                i -= 1\n",
    "            elif move == '<':\n",
    "                j -= 1\n",
    "\n",
    "        return mask_x\n",
    "\n",
    "    @staticmethod\n",
    "    def rouge_l(cand_sents, ref_sents):\n",
    "        lcs_scores = 0.0\n",
    "        cand_unigrams = get_unigram_count(chain(*cand_sents))\n",
    "        ref_unigrams = get_unigram_count(chain(*ref_sents))\n",
    "        for cand_sent in cand_sents:\n",
    "            cand_token_mask = [0 for t in cand_sent]\n",
    "            cand_len = len(cand_sent)\n",
    "            for ref_sent in ref_sents:\n",
    "                # aligns = []\n",
    "                # Rouge.lcs(ref_sent, cand_sent, aligns)\n",
    "                Rouge.my_lcs(cand_sent, ref_sent, cand_token_mask)\n",
    "\n",
    "                # for i in aligns:\n",
    "                #     ref_token_mask[i] = 1\n",
    "            # lcs = []\n",
    "            cur_lcs_score = 0.0\n",
    "            for i in range(cand_len):\n",
    "                if cand_token_mask[i]:\n",
    "                    token = cand_sent[i]\n",
    "                    if cand_unigrams[token] > 0 and ref_unigrams[token] > 0:\n",
    "                        cand_unigrams[token] -= 1\n",
    "                        ref_unigrams[token] -= 1\n",
    "                        cur_lcs_score += 1\n",
    "\n",
    "                        # lcs.append(token)\n",
    "\n",
    "            # print ' '.join(lcs)\n",
    "\n",
    "            lcs_scores += cur_lcs_score\n",
    "\n",
    "        # print \"lcs_scores: %d\" % lcs_scores\n",
    "        ref_words_count = sum(len(s) for s in ref_sents)\n",
    "        # print \"ref_words_count: %d\" % ref_words_count\n",
    "        cand_words_count = sum(len(s) for s in cand_sents)\n",
    "        # print \"cand_words_count: %d\" % cand_words_count\n",
    "\n",
    "        precision = lcs_scores / cand_words_count\n",
    "        recall = lcs_scores / ref_words_count\n",
    "        f_score = (1 + Rouge.beta ** 2) * precision * recall / (recall +\n",
    "                                                                Rouge.beta ** 2 * precision + 1e-7) + 1e-6  # prevent underflow\n",
    "        return precision, recall, f_score\n",
    "\n",
    "    # @staticmethod\n",
    "    # def rouge_2(cand_sents, ref_sents):\n",
    "    #     cand_bigram_counts = get_bigram_counts(cand_sents)\n",
    "    #     ref_bigram_counts = get_bigram_counts(ref_sents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    r = Rouge()\n",
    "    #A simple eample of how rouge can be calculated\n",
    "    print(r.rouge_l([[1, 7, 6, 7, 5], [0, 2, 8, 3, 5]],\n",
    "                    [[1, 2, 3, 4, 5], [3, 9, 5]]))\n",
    "\n",
    "    #A more practical example of how it can be used for summary evaluation\n",
    "    system_generated_summary = \" The Kyrgyz President pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections In an effort to live up to its reputation in the 1990s as an island of democracy. The use of ink is one part of a general effort to show commitment towards more open elections. improper use of this type of ink can cause additional problems as the elections in Afghanistan showed. The use of ink and readers by itself is not a panacea for election ills.\"\n",
    "    manual_summmary = \" The use of invisible ink and ultraviolet readers in the elections of the Kyrgyz Republic which is a small, mountainous state of the former Soviet republic, causing both worries and guarded optimism among different sectors of the population. Though the actual technology behind the ink is not complicated, the presence of ultraviolet light (of the kind used to verify money) causes the ink to glow with a neon yellow light. But, this use of the new technology has caused a lot of problems. \"\n",
    "\n",
    "    print(r.rouge_l([system_generated_summary], [manual_summmary]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is :0.38095238095238093\n",
      "Recall is :0.25\n",
      "F Score is :0.30188774460662915\n",
      "Sum of ROUGE Score:  14.068020212266791\n",
      "Average ROUGE Score =  0.3271632607503905\n",
      "Count:  43\n"
     ]
    }
   ],
   "source": [
    "r = Rouge()\n",
    "\n",
    "system_generated_summary = ogSum\n",
    "manual_summmary = predSum\n",
    "\n",
    "[precision, recall, f_score] = r.rouge_l([system_generated_summary], [manual_summmary])\n",
    "\n",
    "print(\"Precision is :\"+str(precision)+\"\\nRecall is :\"+str(recall)+\"\\nF Score is :\"+str(f_score))\n",
    "\n",
    "sum_F=sum_F+f_score\n",
    "avg_F=sum_F/count\n",
    "\n",
    "print(\"Sum of ROUGE Score: \",sum_F)\n",
    "print(\"Average ROUGE Score = \", avg_F)\n",
    "\n",
    "print(\"Count: \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
